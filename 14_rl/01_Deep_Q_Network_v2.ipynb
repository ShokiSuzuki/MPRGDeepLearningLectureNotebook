{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01.Deep Q-Network_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/14_rl/01_Deep_Q_Network_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9WNOydibxSw"
      },
      "source": [
        "# 目的\n",
        "Deep Q-networkの仕組みを理解し，ゲームタスクを用いて強化学習をおこなう．\\\n",
        "学習後のエージェントの可視化を行い，学習がうまくできているか確認を行う．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_x6vE50N8dJ"
      },
      "source": [
        "# Deep Q-Network(DQN)\n",
        "Deep Q-Network(DQN)は，GoogleのDeep mindチームが2016年に発表した手法で，Q学習におけるQテーブルを用いた行動価値の導出をDCNNを用いた近似関数で代用するのが主な手法の内容である．それまでのQ学習ではすべての状態と行動に組み合わせについての行動価値をQテーブルに記録していたために状態数と行動数の組み合わせが膨大な環境に対して膨大なメモリが必要となる問題を抱えていました．これに対してDQNはQテーブルそのものをDCNNで代用することにより膨大な行動数と状態数の組み合わせが存在する環境に対しても学習を行うことが可能となり，atari2600のゲーム環境において人間を超えるスコアを出すことに成功している．また，DQNにはその他にも強化学習における学習の安定性獲得のためにExperience replay，Target Q-Network，reward_clippingなどの工夫がなされている．行動価値関数(Q値)を用いて行動を決定し，最適行動価値関数になるよう更新を行っていく手法は，価値ベースの手法と呼ばれている．派生手法には，DDQN，Ape-X，R2D2などの手法がある．\\\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1J7YPwb1iMd4eKo3YKLc7Rk4XG-vT7hoz\" width = 50%>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rQGfxWYK_4O"
      },
      "source": [
        "## 準備\n",
        "\n",
        "### Google Colaboratoryの設定確認・変更\n",
        "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n",
        "**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n",
        "\n",
        "\n",
        "### モジュールの追加インストール\n",
        "下記のプログラムを実行して，実験結果の表示に必要な追加ライブラリやモジュールをインストールする．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0AFod2df5j5"
      },
      "source": [
        "!apt-get -qq -y install libcusparse9.1 libnvrtc9.1 libnvtoolsext1 > /dev/null\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.9.1 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "\n",
        "!pip -q install gym[atari,accept-rom-license]==0.23.1\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKpJ1mtDFoDo"
      },
      "source": [
        "## モジュールのインポート\n",
        "はじめに必要なモジュールをインポートする．\n",
        "\n",
        "今回はPyTorchに加えて，Pongを実行するためのシミュレータであるOpenAI Gym（gym）をインポートする．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import gym.spaces\n",
        "\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# GPUの確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ],
      "metadata": {
        "id": "QNYHBpJ5cR6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo5u_p4SFoMa"
      },
      "source": [
        "## OpenAI GymによるPongの環境の定義\n",
        " [OpenAI Gym](https://github.com/openai/gym) は，様々な種類の環境を提供しているモジュールです．\n",
        " \n",
        " 今回はgymで利用できるatariゲームであるPongを実行します．\n",
        " まず，gym.make関数で実行したい環境を指定します．\n",
        " その後，reset関数を実行することで，環境を初期化します．\n",
        " \n",
        "Pong環境は，パドルを操作してボールが自分の陣地に入らないように打ち返すゲームです．Pong環境において現在の状態を確認するためにゲームの画面情報が与えられており，`observation_space`という変数で確認することができます．\n",
        "また，`action_space`という変数で，エージェントが取ることのできる行動の数を確認することができます．\n",
        "Pongの場合は，パドルを上下どちらかに移動させるという行動を取るため，行動の数は2となっています．\n",
        "Pongのゲーム概要は，相手の陣地にボールを入れることで得点を獲得し，ボールを自分の陣地に入れられることで得点を取られます．どちらかのプレイヤーが21点取った時点で終了となります．\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 環境の指定\n",
        "env = gym.make('ALE/Pong-v5', frameskip=1, repeat_action_probability=0.0, full_action_space=False)\n",
        "\n",
        "# 環境の初期化\n",
        "obs = env.reset(seed=0)\n",
        "print('observation space:', env.observation_space) # 状態空間\n",
        "print('action space:', env.action_space) # 行動空間\n",
        "print('initial observation:', obs.shape)\n",
        "\n",
        "# 行動の決定と決定した行動の入力\n",
        "action = env.action_space.sample()\n",
        "obs, r, done, info = env.step(action)\n",
        "print('next observation:', obs.shape) # 環境から返ってくる次状態\n",
        "print('reward:', r)\n",
        "print('done:', done)\n",
        "print('info:', info)"
      ],
      "metadata": {
        "id": "hfJX5ooWcTvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOgbuO2I1bfq"
      },
      "source": [
        "## ネットワーク構造\n",
        "ネットワークモデルを定義します． ここでは，環境からのゲーム画面情報を入力し，行動に対するQ値を出力するようなネットワークを定義するために，畳み込み層3層と全結合層2層から構成されるネットワークとします．\n",
        "\n",
        "入力データのサイズをinput_shape，出力する行動の数をn_actionsとし，ネットワークの作成時に変更できるようにしておきます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh12648qOHXC"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsX8hfGurmBt"
      },
      "source": [
        "## Deep Q-Networkにおける学習工夫の定義\n",
        "\n",
        "Deep Q-Networkでは学習の促進と安定化の為に，いくつ工夫を施して学習を行っています．代表的な工夫としてExperience Replay, Target Q-Network, Reward Clippingと呼ばれる3つの工夫があります．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXY4bzSf3fRA"
      },
      "source": [
        "### Experience Replay\n",
        "DQNは環境の状態を直接観測するのではなく一度Replay Bufferと呼ばれるBufferに経験を格納しておき，学習する際に格納したランダムな経験を取ってくることによって学習を行っています．これをExperience Replayと呼びデータの再利用をおこなうことで，データ効率を高め効率的な学習をおこないます．ここでは，replay memoryの定義を行います．\n",
        "\n",
        "memoryへは，現在の状態，その時に選択された行動，行動によって遷移した状態（次状態），その時の報酬の4種類の情報を1つの経験として蓄積します． まず，Experienceという変数を定義します． ここでは，state, action, reward, done, next_stateが1セットとなるようなデータ構造（辞書オブジェクト）を定義します．\n",
        "\n",
        "その後Experience Bufferクラスを定義します． Experience Bufferクラスでは，memoryへ格納する経験の数（capacity），経験を蓄積するbuffer（buffer）を定義します． append関数では，メモリへ経験を格納します． また，sample関数では，指定したバッチサイズ (batch_size) 分の経験をランダムに選択し，返す関数を定義します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL0Oolhp47OJ"
      },
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYQi9_LQ5UiF"
      },
      "source": [
        "### Target Q-Network\n",
        "強化学習で誤差を計算する際，目標値として使用される行動価値関数と現在の行動価値関数をそれぞれ別のネットワークの出力を用いて誤差を計算します．目標値の行動価値関数を出力するネットワークは一定周期経過するまで固定したネットワークとし，一定周期で現在のネットワークと同期しながら学習を行う工夫がTarget Q-Networkです．この工夫により，学習の安定化を図ります．\n",
        "\n",
        "target_netを現在のnetと同期するsync_networkを定義します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iod_iK4Yt7tB"
      },
      "source": [
        "def sync_network():\n",
        "    tgt_net.load_state_dict(net.state_dict())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUL1qa38uol6"
      },
      "source": [
        "### reward clipping\n",
        "学習における報酬値を −1 から 1 の範囲に指定します．これにより,学習における外れ値に対する過剰反応を防ぐことができます.\n",
        "\n",
        "reward clippingは環境に直接ラップするためwrapperクラスを定義します．reward関数では，環境から受け取った報酬を-1から1の範囲にクリップします．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL7Kkp-svXyu"
      },
      "source": [
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk9EDXRAt-1I"
      },
      "source": [
        "## その他の学習に必要な処理\n",
        "gymのatari環境で学習の安定化と効率的な学習を行うための処理をいくつか行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53nSLW5xwH70"
      },
      "source": [
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
        "                                                dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7C3y7rvo9HP"
      },
      "source": [
        "### 学習に必要な処理の適用\n",
        "環境に対して必要となるそれぞれの処理を作成した環境に対して適用します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1G4H7WxpSKA"
      },
      "source": [
        "env = gym.make(\"ALE/Pong-v5\")\n",
        "env = ClipRewardEnv(env)\n",
        "env = MaxAndSkipEnv(env)\n",
        "env = FireResetEnv(env)\n",
        "env = ProcessFrame84(env)\n",
        "env = ImageToPyTorch(env)\n",
        "env = BufferWrapper(env, 4)\n",
        "env = ScaledFloatFrame(env)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23ixNGvKdAB4"
      },
      "source": [
        "## エージェントの定義\n",
        "エージェントが環境に対して行動価値に沿った行動を行い，環境から経験を取得し，Experience Buffer記録にするようにします．\n",
        "\n",
        "エージェントの環境に対する動きのクラスを定義します．play_step関数は，環境にたいして行動を決定する関数です．epsilon-greedy法を用いて一定の割合でランダムに行動選択を行います．それ以外の場合は，使用しているネットワークへ環境情報を入力し，行動を決定します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaGTP_HhdB6m"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def play_step(self, net=None, epsilon=0.0):\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a)\n",
        "            if use_cuda:\n",
        "              state_v = state_v.cuda()\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJrH9M2xiPey"
      },
      "source": [
        "## TD誤差の計算\n",
        "強化学習は，TD誤差と呼ばれる次状態の推定の価値と実際に起こした行動から得られる価値の差を用いて学習を行います．次状態の推定の価値を教師あり学習の教師と同じ役割として計算します．DQNはQ学習をもとにしているため，現在の行動価値関数を最適行動価値関数になるように更新を行っていきます．\n",
        "\n",
        "TD誤差の計算を行う関数を定義します．calc_loss関数では，replay_bufferからランダムに取得したbatch分の経験をもとに下記のLoss計算を行います．\n",
        "\n",
        "\\\n",
        "$$\n",
        "L_{\\theta}=\\frac{1}{2}(r+\\gamma \\max_{a'}Q_{\\theta_{i}}(s',a')-Q_{\\theta_{i}}(s,a))^{2}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjm67ns-iUYR"
      },
      "source": [
        "def calc_loss(batch, net, tgt_net):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    states_v = torch.tensor(states)\n",
        "    next_states_v = torch.tensor(next_states)\n",
        "    actions_v = torch.tensor(actions)\n",
        "    rewards_v = torch.tensor(rewards)\n",
        "    done_mask = torch.ByteTensor(dones)\n",
        "    if use_cuda:\n",
        "      states_v = states_v.cuda()\n",
        "      next_states_v = next_states_v.cuda()\n",
        "      actions_v = actions_v.cuda()\n",
        "      rewards_v = rewards_v.cuda()\n",
        "      done_mask = done_mask.cuda()\n",
        "\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
        "    next_state_values[done_mask] = 0.0\n",
        "    next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMkQ1vGVxTUQ"
      },
      "source": [
        "## 学習\n",
        "DQNを用いて学習を行います．学習環境はatari環境のPongゲーム環境を用います．各パラメータを定義します．Experience Replayで利用するReplay Bufferのサイズは1万とし，Replay Bufferが埋まるまでエージェントがランダムで動き，経験を貯めてから学習を行います．\n",
        "\n",
        "学習回数を100万frame(num_frame)分とし，環境の終了条件はどちらかが21点とったら終了としています．また，最適化手法にはRMSprop利用します．\n",
        "\n",
        "学習を開始します． まず，環境を初期化し，経験をReplayBufferへ蓄積します． 十分に蓄積された後，パラメータの更新を行います． また，SNC_TARGET_FRAMESで指定した回数ごとに，target_netのパラメータをnetのパラメータと同じになるようにコピーを行います．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MEAN_REWARD_BOUND = 19.5\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 10**5\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02\n",
        "\n",
        "num_frame = 1000000"
      ],
      "metadata": {
        "id": "hsfDJsuB3XE4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "\n",
        "if use_cuda:\n",
        "  net = net.cuda()\n",
        "  tgt_net = tgt_net.cuda()\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = optim.RMSprop(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts = time.time()\n",
        "ts_frame = 0\n",
        "best_mean_reward = None\n",
        "\n",
        "while frame_idx < num_frame:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        mean_reward = np.mean(total_rewards[-100:])\n",
        "        print(\"Frame {0}: done {1} games, mean reward {2:.3f}, eps {3:.2f}, speed {4:.2f} f/s\".format(frame_idx, len(total_rewards), mean_reward, epsilon, speed))\n",
        "        \n",
        "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "            torch.save(net.state_dict(), \"Pong\" + \"-best.dat\")\n",
        "            if best_mean_reward is not None:\n",
        "                print(\"Best mean reward updated {0:.3f} -> {1:.3f}, model saved\".format(best_mean_reward, mean_reward))\n",
        "            best_mean_reward = mean_reward\n",
        "        if len(total_rewards) == 1000000:\n",
        "            print(\"Solved in {0} frames!\".format(frame_idx))\n",
        "            break\n",
        "\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        sync_network()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t = calc_loss(batch, net, tgt_net)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "VskHWwcGcauJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksjONOyoxstC"
      },
      "source": [
        "## 評価\n",
        "学習したネットワーク（エージェント）を確認してみます．\n",
        "\n",
        "ここでは，framesに描画したフレームを順次格納します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14MSs0oPeXl2"
      },
      "source": [
        "# 結果を描画するための設定\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display._obj._screen)\n",
        "\n",
        "\n",
        "frames = []\n",
        "for i in range(3):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    t = 0\n",
        "    \n",
        "    while not done and t < 200:\n",
        "        frames.append(env.render(mode='rgb_array'))\n",
        "        state_a = np.array([state], copy=False)\n",
        "        state_v = torch.tensor(state_a)\n",
        "        if use_cuda:\n",
        "          state_v = state_v.cuda()\n",
        "        q_vals_v = net(state_v)\n",
        "        _, act_v = torch.max(q_vals_v, dim=1)\n",
        "        action = int(act_v.item())\n",
        "        new_state, reward, is_done, _ = env.step(action)\n",
        "        state = new_state\n",
        "        done = is_done\n",
        "        t += 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5tJoQekxstF"
      },
      "source": [
        "## 描画\n",
        "\n",
        "maptlotlibを用いて，保存した動画フレームをアニメーションとして作成し，表示しています．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 実行結果の表示\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ],
      "metadata": {
        "id": "OoiuPWi1cdtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZDh4S6m2hK0"
      },
      "source": [
        "## 課題"
      ]
    }
  ]
}