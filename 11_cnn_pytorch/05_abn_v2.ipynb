{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "05_abn_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/11_cnn_pytorch/05_abn_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQfhYcVF8KPt"
      },
      "source": [
        "# Attention Branch Network (ABN)\n",
        "\n",
        "---\n",
        "\n",
        "## 目的\n",
        "ABNの構造を理解する.\n",
        "\n",
        "ABNを用いてCIFAR-10データセットに対する物体認識を行う．\n",
        "\n",
        "## Attention Branch Network (ABN)\n",
        "Attention Branch Network (ABN)[1]とは，Attention機構により推論時における注視領域の可視化と，画像認識の高精度化ができるネットワークです．\n",
        "02_CAM.ipynbで使用したClass Activation Mapping (CAM)[2] は，CNNが推論時に着目した領域をAttention mapとして可視化できる一方で，最後の畳み込み層の後のGlobal Average Pooling (GAP)[3] によって特徴マップの空間的な情報が失われることによる認識精度の低下が問題となっていました．\n",
        "ABNでは，ネットワークの判断根拠であるAttention mapを認識処理に用いることで説明性の高いAttention mapの獲得と認識精度の高精度化を実現しました．\n",
        "ABNは，Feature extractor，Attention branch，Perception branchの3つのモジュールから構成されています．\n",
        "Feature extractorは，複数の畳み込み層が含まれており，入力から特徴マップを獲得します．\n",
        "Attention branchは，特徴マップを受け取り，クラス識別及びAttention mapを生成します．\n",
        "Attention branchにより生成されたAttention mapは，Attention機構によりFeature extractorから得られた特徴マップ全体に対して重み付けします．\n",
        "この重み付けした特徴マップを用いてPerception branchにより最終的な各クラスにおける確率を出力します．\n",
        "これにより，画像認識に有効な領域に着目して学習することができます．\n",
        "学習時は，以下の式よりAttention branchとPerception branchの学習誤差を用いてネットワークを学習します．\n",
        "\n",
        "$$L_{all}({\\bf x}_i) = L_{att}({\\bf x}_i) + L_{per}({\\bf x}_i)$$\n",
        "\n",
        "ここで，${\\rm x}_i$は$i$番目の入力画像，$L_{att}({\\bf x}_i)$はAttention branch の学習誤差，$L_{per}({\\bf x}_i)$ はPerception branchの学習誤差を示します．\n",
        "ABNは，Attention branchをベースラインのネットワークに追加することで，様々な画像認識タスクに応用することができます．\n",
        "\n",
        "### Attention branch\n",
        "Attention branchは，畳み込み層とGAPから構成されており，Attention mapを出力するモジュールです．Attention branchでは，GAPの2層前の特徴マップに対し，$1×1$の重みフィルタを畳み込みます．\n",
        "その後，Sigmoid関数により0から1の範囲に正規化することでAttention mapを獲得します．\n",
        "\n",
        "### Attention機構\n",
        "Attention機構とは，認識に有益な特徴量に対して重み付けして学習することで，ネットワークの汎化性能を向上させる方法です．\n",
        "ABNでは，Attention branchにより生成されたAttention mapをAttention機構によりFeature extractorから得られた特徴マップ全体に対して重み付けします．\n",
        "また，重み付け後の特徴マップに重み付け前の特徴マップを加算することで，特徴マップの消失を抑制し，Attention mapを認識に効率的に反映させることができます．\n",
        "ABNでは，以下の式よりFeature extractorから得られた特徴マップに対してAttention mapを重み付けします．\n",
        "\n",
        "$$f'({\\rm x}_i)=(1+M({\\rm x}_i))\\cdot f({\\rm x}_i)$$\n",
        "\n",
        "ここで，$M({\\rm x}_i)$は$i$番目の入力画像に対するAttention mapを示しています．\n",
        "\n",
        "### Perception branch\n",
        "Perception branchは，Attention機構により重み付けされた特徴マップを入力し，最終的な認識結果を出力します．\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/11_cnn_pytorch/05_abn/ABN.png?raw=true\" width = 100%>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8ehSX6U8KPv"
      },
      "source": [
        "## 準備\n",
        "\n",
        "### Google Colaboratoryの設定確認・変更\n",
        "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n",
        "**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n",
        "\n",
        "## モジュールのインポート\n",
        "はじめに必要なモジュールをインポートする．\n",
        "\n",
        "### GPUの確認\n",
        "GPUを使用した計算が可能かどうかを確認します．\n",
        "\n",
        "`GPU availability: True`と表示されれば，GPUを使用した計算をChainerで行うことが可能です．\n",
        "Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaNXqInW8KPv",
        "outputId": "79cd987b-401c-491c-ba3e-9a1fad63fb57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# モジュールのインポート\n",
        "from time import time\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torchsummary\n",
        "\n",
        "# GPUの確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEgbYa9EloXV"
      },
      "source": [
        "## 使用するデータセット\n",
        "\n",
        "### データセット\n",
        "今回の物体認識では，CIFAR-10データセットを使用します．CIFAR-10データセットは，飛行機や犬などの10クラスの物体が表示されている画像から構成されたデータセットです．\n",
        "\n",
        "![CIFAR10_sample.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/176458/b6b43478-c85f-9211-7bc6-227d9b387af5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNzvYQL58KP4"
      },
      "source": [
        "## データセットの読み込みとData Augmentation\n",
        "\n",
        "学習データ（CIFAR10データセット）を読み込みます．\n",
        "学習には，大量のデータを利用しますが，それでも十分ではありません． そこで，データ拡張 (data augmentation) により，データのバリエーションを増やします． 一般的な方法は，画像の左右反転，明るさ変換などです．\n",
        "\n",
        "今回はImageNetデータセットで事前に学習したモデルを利用するため，入力サイズをサイズを224ピクセルにします．また，各チャンネルの平均値を基準に平均が0，分散が分散が1になるようにNormalizeを用いて正規化します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leWJTOIL8KP4"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    transforms.RandomErasing(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(root='./', train=True, download=True, transform=transform_train)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./', train=False, download=True, transform=transform_test)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DduOi-s18KP_"
      },
      "source": [
        "## ネットワークモデルの定義\n",
        "### Residual Block (Basic BlockとBottleneck) の定義\n",
        "まずはじめに，2種類のResidual Block（BasicBlockとBottleneck）を定義します．\n",
        "ここでは，`BasicBlock(nn.Module)`および`Bottleneck(nn.Module)`で，任意の形の構造（チャンネル数など）を定義できるクラスを作成します．\n",
        "`__init__`関数の引数である，`inplanes`は入力される特徴マップのチャンネル数，`planes`はBottleNeck内の特徴マップのチャンネル数を指定します．\n",
        "また，`stride`はResidual Block内の1つ目の3x3の畳み込み層のstrideの値です．\n",
        "`downsample`は，Residual Blockに入力された特徴マップサイズと畳み込み演算後の特徴マップのサイズが異なる場合に元の特徴マップ (resudual) のサイズを調整するための演算を定義するための引数です（詳細は後述）"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(planes),\n",
        "        )\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.convs(x)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, planes, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(self.expansion * planes),\n",
        "        )\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.convs(x)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "JGp3ClWorlW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に，ABNを定義します．\n",
        "今回は，ResNetにABNを導入したモデル (ResNet-ABN) を使用します．ResNet-ABNの層の深さ (depth) は，20層に設定します．"
      ],
      "metadata": {
        "id": "ZQjlvgRqsNCQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0cpWP_w8KP_"
      },
      "source": [
        "class ResNet_ABN(nn.Module):\n",
        "    def __init__(self, depth, num_classes=10):\n",
        "        super(ResNet_ABN, self).__init__()\n",
        "        # 指定した深さ（畳み込みの層数）でネットワークを構築できるかを確認\n",
        "        block_name = 'Bottleneck' if depth >=44 else 'BasicBlock'\n",
        "        if block_name.lower() == 'basicblock':\n",
        "            assert (depth - 2) % 6 == 0, 'When use basicblock, depth should be 6n+2, e.g. 20, 32, 44'\n",
        "            n = (depth - 2) // 6\n",
        "            block = BasicBlock\n",
        "        elif block_name.lower() == 'bottleneck':\n",
        "            assert (depth - 2) % 9 == 0, 'When use bottleneck, depth should be 9n+2, e.g. 47, 56, 110, 1199'\n",
        "            n = (depth - 2) // 9\n",
        "            block = Bottleneck\n",
        "        else:\n",
        "            raise ValueError('block_name shoule be Basicblock or Bottleneck')\n",
        "\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 16, n, down_size=True)\n",
        "        self.layer2 = self._make_layer(block, 32, n, stride=2, down_size=True)\n",
        "\n",
        "        self.att_layer3 = self._make_layer(block, 64, n, stride=1, down_size=False)\n",
        "        self.bn_att = nn.BatchNorm2d(64 * block.expansion)\n",
        "        self.att_conv   = nn.Conv2d(64 * block.expansion, num_classes, kernel_size=1, padding=0,\n",
        "                               bias=False)\n",
        "        self.bn_att2 = nn.BatchNorm2d(num_classes)\n",
        "        self.att_conv2  = nn.Conv2d(num_classes, num_classes, kernel_size=1, padding=0,\n",
        "                               bias=False)\n",
        "        self.att_conv3  = nn.Conv2d(num_classes, 1, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn_att3 = nn.BatchNorm2d(1)\n",
        "        self.att_gap = nn.AvgPool2d(16)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.layer3 = self._make_layer(block, 64, n, stride=2, down_size=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, n_blocks, stride=1, down_size=True):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        if down_size:\n",
        "            self.inplanes = planes * block.expansion\n",
        "            for i in range(0, n_blocks-1):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "        else:\n",
        "            inplanes = planes * block.expansion\n",
        "            for i in range(0, n_blocks-1):\n",
        "                layers.append(block(inplanes, planes))\n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # feature extractor\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)    # 32x32\n",
        "\n",
        "        x = self.layer1(x)  # 32x32\n",
        "        x = self.layer2(x)  # 16x16\n",
        "\n",
        "        # attention branch\n",
        "        ax = self.bn_att(self.att_layer3(x))\n",
        "        ax = self.relu(self.bn_att2(self.att_conv(ax)))\n",
        "        bs, cs, ys, xs = ax.shape\n",
        "        self.att = self.sigmoid(self.bn_att3(self.att_conv3(ax)))\n",
        "        ax = self.att_conv2(ax)\n",
        "        ax = self.att_gap(ax)\n",
        "        ax = ax.view(ax.size(0), -1)\n",
        "\n",
        "        # perception branch\n",
        "        rx = x * self.att\n",
        "        rx = rx + x\n",
        "        rx = self.layer3(rx)  # 8x8\n",
        "        rx = self.avgpool(rx)\n",
        "        rx = rx.view(rx.size(0), -1)\n",
        "        rx = self.fc(rx)\n",
        "\n",
        "        return ax, rx, self.att\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFCekfs78KQC"
      },
      "source": [
        "## ネットワークの作成\n",
        "上のプログラムで定義したネットワークを作成します．\n",
        "使用したいResdual Block構造の種類に応じて，層数を指定します．\n",
        "\n",
        "CNNクラスを呼び出して，ネットワークモデルを定義します． \n",
        "また，GPUを使う場合（`use_cuda == True`）には，ネットワークモデルをGPUメモリ上に配置します． \n",
        "これにより，GPUを用いた演算が可能となります．\n",
        "\n",
        "学習を行う際の最適化方法としてモーメンタムSGD (モーメンタム付き確率的勾配降下法) を利用します． \n",
        "また，学習率を0.01，モーメンタムを0.9として引数に与えます．\n",
        "\n",
        "最後に，定義したネットワークの詳細情報を`torchsummary.summary()`関数を用いて表示します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rntVJhx98KQC"
      },
      "source": [
        "# ResNetの層数を指定 (e.g. 20, 32, 44, 47, 56, 110, 1199)\n",
        "depth = 20\n",
        "\n",
        "# ResNetを構築\n",
        "model = ResNet_ABN(depth)\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# モデルの情報を表示\n",
        "torchsummary.summary(model, (3, 32, 32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4NRFd6i8KQF"
      },
      "source": [
        "## 学習\n",
        "読み込んだCIFAR-10データセットと作成したネットワークを用いて，学習を行います．\n",
        "今回は，学習エポック数を10とします．\n",
        "1エポック学習するごとに学習したモデルを評価し，最も認識精度の高いモデルが保存されます．\n",
        "各更新において，学習用データと教師データをそれぞれ`image`と`label`とします．\n",
        "学習モデルに`image`を与えて，Attention branchの出力，Perception branchの出力及びAttention mapを取得します．\n",
        "Attention branchの出力及びPerception branchの出力と教師ラベル`label`との誤差を`criterion`で算出します．\n",
        "また，認識精度も算出します．\n",
        "そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．\n",
        "認識精度も同時に計算して，`print`関数で学習経過における誤差や認識精度を表示します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnMVTad98KQG"
      },
      "source": [
        "# ミニバッチサイズ・エポック数の設定\n",
        "batch_size = 64\n",
        "epoch_num = 10\n",
        "n_iter = len(train_data) / batch_size\n",
        "\n",
        "# データローダーの設定\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion.cuda()\n",
        "\n",
        "# ネットワークを学習モードへ変更\n",
        "model.train()\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "    \n",
        "    for image, label in train_loader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        att_outputs, per_outputs, _  = model(image)\n",
        "        att_loss = criterion(att_outputs, label)\n",
        "        per_loss = criterion(per_outputs, label)\n",
        "        loss = att_loss + per_loss\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        sum_loss += loss.item()\n",
        "        \n",
        "        pred = torch.argmax(per_outputs, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed_time :{}\".format(epoch,\n",
        "                                                                                 sum_loss / n_iter,\n",
        "                                                                                 count.item() / len(train_data),\n",
        "                                                                                 time() - start))\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwWfJO358KQS"
      },
      "source": [
        "## テスト\n",
        "学習したネットワークモデルを用いて評価を行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij38kQBP8KQS"
      },
      "source": [
        "# データローダーの準備\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=False)\n",
        "\n",
        "# ネットワークを評価モードへ変更\n",
        "model.eval()\n",
        "\n",
        "# 評価の実行\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for image, label in test_loader:\n",
        "\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "            \n",
        "        _, outputs, attention = model(image)\n",
        "\n",
        "        pred = torch.argmax(outputs, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "print(\"test accuracy: {}\".format(count.item() / len(test_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiY_kiXb8KQV"
      },
      "source": [
        "## Attention mapの可視化\n",
        "Attention mapを可視化して，ネットワークの判断根拠を確認してみます．\n",
        "再度，実行することで他のテストサンプルに対するAttention mapを可視化することができます．\n",
        "predictedは認識結果，conf_dataは認識結果に対する信頼度を示しています．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB0SvioEwlm4"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from ipykernel import kernelapp as app\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=True, num_workers=2)\n",
        "\n",
        "classes_list = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "def min_max(x, axis=None):\n",
        "    x_min = x.min(axis=axis, keepdims=True)\n",
        "    x_max = x.max(axis=axis, keepdims=True)\n",
        "    return (x - x_min) / (x_max - x_min)\n",
        "\n",
        "with torch.no_grad():\n",
        "    v_list = []\n",
        "    att_list = []\n",
        "    for image, label in test_loader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "        _, outputs, attention = model(image)\n",
        "        outputs = softmax(outputs)\n",
        "        conf_data = outputs.data.topk(k=1, dim=1, largest=True, sorted=True)\n",
        "        _, predicted = outputs.max(1)\n",
        "        c_att = attention.data.cpu()\n",
        "        c_att = c_att.numpy()\n",
        "        d_inputs = image.data.cpu().numpy()\n",
        "\n",
        "        in_b, in_c, in_y, in_x = image.shape\n",
        "        for item_img, item_att in zip(d_inputs, c_att):\n",
        "            v_img = ((item_img.transpose((1, 2, 0)) * [0.2023, 0.1994, 0.2010]) + [0.4914, 0.4822, 0.4465]) * 255\n",
        "            #v_img = v_img[:, :, ::-1]\n",
        "            resize_att = cv2.resize(item_att[0], (in_x, in_y))\n",
        "            #resize_att = min_max(resize_att)\n",
        "            resize_att *= 255.\n",
        "\n",
        "            v_img = np.uint8(v_img)\n",
        "            resize_att = np.uint8(resize_att)\n",
        "            jet_map = cv2.applyColorMap(resize_att, cv2.COLORMAP_JET)\n",
        "            jet_map = cv2.addWeighted(v_img, 0.6, jet_map, 0.4, 0)\n",
        "            v_list.append(v_img)\n",
        "            att_list.append(jet_map)\n",
        "        break\n",
        "\n",
        "# Show attention map\n",
        "cols = 8\n",
        "rows = 1\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3.0))\n",
        "plt.title('Input image')\n",
        "plt.axis(\"off\")\n",
        "for r in range(rows):\n",
        "    for c in range(cols):\n",
        "        cls = label[c].item()\n",
        "        ax = fig.add_subplot(r+1, cols, c+1)\n",
        "        plt.title('{}'.format(classes_list[cls]))\n",
        "        ax.imshow(v_list[cols * r + c])\n",
        "        ax.set_axis_off()\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3.5))\n",
        "plt.title('Attention map')\n",
        "plt.axis(\"off\")\n",
        "for r in range(rows):\n",
        "    for c in range(cols):\n",
        "        pred = predicted[c].item()\n",
        "        conf = conf_data[0][c].item()\n",
        "        ax = fig.add_subplot(r+1, cols, c+1)\n",
        "        ax.imshow(att_list[cols * r + c])\n",
        "        plt.title('pred: {}\\nconf: {:.2f}'.format(classes_list[pred], conf))\n",
        "        ax.set_axis_off()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mgak3JiLga-P"
      },
      "source": [
        "## 課題\n",
        "1. エポック数やミニバッチサイズを変えて実験し，認識精度とAttention mapの変化を確認しましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9vIEyDvg187"
      },
      "source": [
        "#ここにコードを書く"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCghkSTztILp"
      },
      "source": [
        "# 参考文献\n",
        "- [1] H. Fukui, T. Hirakawa, T. Yamashita, and\n",
        "H. Fujiyoshi, \"Attention branch network: Learning of\n",
        "attention mechanism for visual explanation\". In 2019 IEEE\n",
        "Conference on Computer Vision and Pattern Recognition,\n",
        "pp. 10705–10714, 2019.\n",
        "\n",
        "- [2] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva,\n",
        "and A. Torralba, \"Learning deep features for discriminative\n",
        "localization\". In 2016 IEEE Conference on Computer\n",
        "Vision and Pattern Recognition, pp. 2921–2929, 2016.\n",
        "\n",
        "- [3] M. Lin, Q. Chen, and S. Yan, \"Network in network\".\n",
        "In 2nd International Conference on Learning Representations,\n",
        "Banff, AB, Canada, April 14-16, 2014, Conference\n",
        "Track Proceedings, 2014."
      ]
    }
  ]
}