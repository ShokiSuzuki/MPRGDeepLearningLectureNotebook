{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "05_abn_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/11_cnn_pytorch/05_abn_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQfhYcVF8KPt"
      },
      "source": [
        "# Attention Branch Network (ABN)\n",
        "\n",
        "---\n",
        "\n",
        "## 目的\n",
        "ABNの構造を理解する.\n",
        "\n",
        "ABNを用いてCIFAR-10データセットに対する物体認識を行う．\n",
        "\n",
        "## Attention Branch Network (ABN)\n",
        "Attention Branch Network (ABN)[1]とは，Attention機構により推論時における注視領域の可視化と，画像認識の高精度化ができるネットワークです．\n",
        "02_CAM.ipynbで使用したClass Activation Mapping (CAM)[2] は，CNNが推論時に着目した領域をAttention mapとして可視化できる一方で，最後の畳み込み層の後のGlobal Average Pooling (GAP)[3] によって特徴マップの空間的な情報が失われることによる認識精度の低下が問題となっていました．\n",
        "ABNでは，ネットワークの判断根拠であるAttention mapを認識処理に用いることで説明性の高いAttention mapの獲得と認識精度の高精度化を実現しました．\n",
        "ABNは，Feature extractor，Attention branch，Perception branchの3つのモジュールから構成されています．\n",
        "Feature extractorは，複数の畳み込み層が含まれており，入力から特徴マップを獲得します．\n",
        "Attention branchは，特徴マップを受け取り，クラス識別及びAttention mapを生成します．\n",
        "Attention branchにより生成されたAttention mapは，Attention機構によりFeature extractorから得られた特徴マップ全体に対して重み付けします．\n",
        "この重み付けした特徴マップを用いてPerception branchにより最終的な各クラスにおける確率を出力します．\n",
        "これにより，画像認識に有効な領域に着目して学習することができます．\n",
        "学習時は，以下の式よりAttention branchとPerception branchの学習誤差を用いてネットワークを学習します．\n",
        "\n",
        "$$L_{all}({\\bf x}_i) = L_{att}({\\bf x}_i) + L_{per}({\\bf x}_i)$$\n",
        "\n",
        "ここで，${\\rm x}_i$は$i$番目の入力画像，$L_{att}({\\bf x}_i)$はAttention branch の学習誤差，$L_{per}({\\bf x}_i)$ はPerception branchの学習誤差を示します．\n",
        "ABNは，Attention branchをベースラインのネットワークに追加することで，様々な画像認識タスクに応用することができます．\n",
        "\n",
        "### Attention branch\n",
        "Attention branchは，畳み込み層とGAPから構成されており，Attention mapを出力するモジュールです．Attention branchでは，GAPの2層前の特徴マップに対し，$1×1$の重みフィルタを畳み込みます．\n",
        "その後，Sigmoid関数により0から1の範囲に正規化することでAttention mapを獲得します．\n",
        "\n",
        "### Attention機構\n",
        "Attention機構とは，認識に有益な特徴量に対して重み付けして学習することで，ネットワークの汎化性能を向上させる方法です．\n",
        "ABNでは，Attention branchにより生成されたAttention mapをAttention機構によりFeature extractorから得られた特徴マップ全体に対して重み付けします．\n",
        "また，重み付け後の特徴マップに重み付け前の特徴マップを加算することで，特徴マップの消失を抑制し，Attention mapを認識に効率的に反映させることができます．\n",
        "ABNでは，以下の式よりFeature extractorから得られた特徴マップに対してAttention mapを重み付けします．\n",
        "\n",
        "$$f'({\\rm x}_i)=(1+M({\\rm x}_i))\\cdot f({\\rm x}_i)$$\n",
        "\n",
        "ここで，$M({\\rm x}_i)$は$i$番目の入力画像に対するAttention mapを示しています．\n",
        "\n",
        "### Perception branch\n",
        "Perception branchは，Attention機構により重み付けされた特徴マップを入力し，最終的な認識結果を出力します．\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1AWFGV9j9I4VVlaus79Iwr2OzX60gR-PR\" width=100%>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8ehSX6U8KPv"
      },
      "source": [
        "## モジュールのインポート\n",
        "プログラムの実行に必要なモジュールをインポートします．\n",
        "今回は，機械学習ライブラリであるPytorchを使用します．\n",
        "PyTorchとは，Python向けのオープンソース機械学習ライブラリで，Facebookに開発されました．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaNXqInW8KPv"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchsummary"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4gEy7cGCfz8"
      },
      "source": [
        "## GPUの確認\n",
        "GPUを使用した計算が可能かどうかを確認します．\n",
        "下記のコードを実行してGPU情報を確認します． GPUの確認を行うためには，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．\n",
        "\n",
        "`Use CUDA: True`と表示されれば，GPUを使用した計算をPytorchで行うことが可能です． CPUとなっている場合は，上記に記載している手順にしたがって，設定を変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9fjeG_U8KP1"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "use_cuda = torch.cuda.is_available()\n",
        "cudnn.benchmark = True\n",
        "print('Use CUDA:', use_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uanw74k0F9iw"
      },
      "source": [
        "下記のコードを実行してGPU情報を確認します．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeUIUazLGGPu"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEgbYa9EloXV"
      },
      "source": [
        "## 使用するデータセット\n",
        "\n",
        "### データセット\n",
        "今回の物体認識では，CIFAR-10データセットを使用します．CIFAR-10データセットは，飛行機や犬などの10クラスの物体が表示されている画像から構成されたデータセットです．\n",
        "\n",
        "![CIFAR10_sample.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/176458/b6b43478-c85f-9211-7bc6-227d9b387af5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNzvYQL58KP4"
      },
      "source": [
        "### データセットのダウンロードと読み込み\n",
        "実験に使用するCIFAR-10データセットを読み込みます．\n",
        "１回の誤差を算出するデータ数 (ミニバッチサイズ) は，64とします．\n",
        "まず，CIFAR-10データセットをダウンロードします．\n",
        "次に，ダウンロードしたデータセットを読み込みます．\n",
        "学習には，大量のデータを利用しますが，それでも十分ではありません． そこで，データ拡張 (data augmentation) により，データのバリエーションを増やします． 一般的な方法は，画像の左右反転，明るさ変換などです． また，画像の一部にマスク処理をかけるRandom Erasingも行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leWJTOIL8KP4"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    transforms.RandomErasing(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=20)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=20)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHQfAwb_8KP7"
      },
      "source": [
        "### CIFAR-10データセットの表示\n",
        "CIFAR-10データセットに含まれる画像を表示してみます．\n",
        "ここでは，matplotlibを用いて複数の画像を表示させるプログラムを利用します．\n",
        "学習データは5万枚，1つのデータサイズは3x32x32の画像のような形式となっています． これは32x32ピクセルのカラー画像という意味になります．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fod_SRFR8KP8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    npimg = ((npimg.transpose((1,2,0))  * [0.2023, 0.1994, 0.2010]) + [0.4914, 0.4822, 0.4465])  # unnormalize\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images[0:4]))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DduOi-s18KP_"
      },
      "source": [
        "## ネットワークモデルの定義\n",
        "次に，ABNを定義します．\n",
        "今回は，ResNetにABNを導入したモデル (ResNet-ABN) を使用します．ResNet-ABNの層の深さ (depth) は，20層に設定します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0cpWP_w8KP_"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "    \n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(planes),\n",
        "        )\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.convs(x)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, planes, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(self.expansion * planes),\n",
        "        )\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.convs(x)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet_ABN(nn.Module):\n",
        "    def __init__(self, depth, num_classes=10):\n",
        "        super(ResNet_ABN, self).__init__()\n",
        "        # Model type specifies number of layers for CIFAR-10 model\n",
        "        block_name = 'Bottleneck' if depth >=44 else 'BasicBlock'\n",
        "        if block_name.lower() == 'basicblock':\n",
        "            assert (depth - 2) % 6 == 0, 'When use basicblock, depth should be 6n+2, e.g. 20, 32, 44'\n",
        "            n = (depth - 2) // 6\n",
        "            block = BasicBlock\n",
        "        elif block_name.lower() == 'bottleneck':\n",
        "            assert (depth - 2) % 9 == 0, 'When use bottleneck, depth should be 9n+2, e.g. 47, 56, 110, 1199'\n",
        "            n = (depth - 2) // 9\n",
        "            block = Bottleneck\n",
        "        else:\n",
        "            raise ValueError('block_name shoule be Basicblock or Bottleneck')\n",
        "\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, n, down_size=True)\n",
        "        self.layer2 = self._make_layer(block, 32, n, stride=2, down_size=True)\n",
        "\n",
        "        self.att_layer3 = self._make_layer(block, 64, n, stride=1, down_size=False)\n",
        "        self.bn_att = nn.BatchNorm2d(64 * block.expansion)\n",
        "        self.att_conv   = nn.Conv2d(64 * block.expansion, num_classes, kernel_size=1, padding=0,\n",
        "                               bias=False)\n",
        "        self.bn_att2 = nn.BatchNorm2d(num_classes)\n",
        "        self.att_conv2  = nn.Conv2d(num_classes, num_classes, kernel_size=1, padding=0,\n",
        "                               bias=False)\n",
        "        self.att_conv3  = nn.Conv2d(num_classes, 1, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn_att3 = nn.BatchNorm2d(1)\n",
        "        self.att_gap = nn.AvgPool2d(16)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.layer3 = self._make_layer(block, 64, n, stride=2, down_size=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, down_size=True):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        if down_size:\n",
        "            self.inplanes = planes * block.expansion\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "        else:\n",
        "            inplanes = planes * block.expansion\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(inplanes, planes))\n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # feature extractor\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)    # 32x32\n",
        "\n",
        "        x = self.layer1(x)  # 32x32\n",
        "        x = self.layer2(x)  # 16x16\n",
        "\n",
        "        # attention branch\n",
        "        ax = self.bn_att(self.att_layer3(x))\n",
        "        ax = self.relu(self.bn_att2(self.att_conv(ax)))\n",
        "        bs, cs, ys, xs = ax.shape\n",
        "        self.att = self.sigmoid(self.bn_att3(self.att_conv3(ax)))\n",
        "        ax = self.att_conv2(ax)\n",
        "        ax = self.att_gap(ax)\n",
        "        ax = ax.view(ax.size(0), -1)\n",
        "\n",
        "        # perception branch\n",
        "        rx = x * self.att\n",
        "        rx = rx + x\n",
        "        rx = self.layer3(rx)  # 8x8\n",
        "        rx = self.avgpool(rx)\n",
        "        rx = rx.view(rx.size(0), -1)\n",
        "        rx = self.fc(rx)\n",
        "\n",
        "        return ax, rx, self.att\n",
        "\n",
        "depth = 20 # e.g. 20, 32, 44, 47, 56, 110, 1199\n",
        "model = ResNet_ABN(depth)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFCekfs78KQC"
      },
      "source": [
        "## 損失関数と最適化手法の定義\n",
        "学習に使用する損失関数と最適化手法を定義します．\n",
        "今回は，分類問題を扱うため，クロスエントロピー誤差を計算するための`CrossEntropyLoss`を`criterion`として定義します．\n",
        "最適化手法には，確率的勾配降下法 (stochastic gradient descent: SGD) を用いて学習します．\n",
        "最後に，定義したネットワークの詳細情報を`torchsummary.summary()`関数を用いて表示します．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rntVJhx98KQC"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim import SGD, lr_scheduler\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion.cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9,  weight_decay=5e-4)\n",
        "#scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[150, 225], gamma=0.1) # 50 < 75\n",
        "\n",
        "# モデルの情報を表示\n",
        "torchsummary.summary(model, (3, 32, 32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4NRFd6i8KQF"
      },
      "source": [
        "## 学習\n",
        "読み込んだCIFAR-10データセットと作成したネットワークを用いて，学習を行います．\n",
        "今回は，学習エポック数を15とします．\n",
        "1エポック学習するごとに学習したモデルを評価し，最も認識精度の高いモデルが保存されます．\n",
        "各更新において，学習用データと教師データをそれぞれ`inputs`と`targets`とします．\n",
        "学習モデルに`inputs`を与えて，Attention branchの出力，Perception branchの出力及びAttention mapを取得します．\n",
        "Attention branchの出力及びPerception branchの出力と教師ラベル`targets`との誤差を`criterion`で算出します．\n",
        "また，認識精度も算出します．\n",
        "そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．\n",
        "認識精度も同時に計算して，`print`関数で学習経過における誤差や認識精度を表示します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnMVTad98KQG"
      },
      "source": [
        "epoch_num = 15\n",
        "best_acc = 0  # best test accuracy\n",
        "\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    #scheduler.step()\n",
        "    train_running_loss = 0.0\n",
        "    train_running_acc = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    count = 0\n",
        "    for image, label in trainloader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        att_outputs, per_outputs, _ = model(image)\n",
        "        att_loss = criterion(att_outputs, label)\n",
        "        per_loss = criterion(per_outputs, label)\n",
        "        loss = att_loss + per_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = per_outputs.max(1)\n",
        "        total += label.size(0)\n",
        "        correct += predicted.eq(label).sum().item()\n",
        "        # print statistics\n",
        "        train_running_loss += loss\n",
        "        train_running_acc += 100.*correct/total\n",
        "        count += 1\n",
        "\n",
        "    print('[Epoch %d] Train Loss: %.5f | Train Acc: %.3f%%'\n",
        "                  % (epoch, train_loss/count, train_running_acc/count))\n",
        "\n",
        "    # testing\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        test_running_loss = 0.0\n",
        "        test_running_acc = 0.0\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        count = 0\n",
        "        for image, label in testloader:\n",
        "          if use_cuda:\n",
        "              image = image.cuda()\n",
        "              label = label.cuda()\n",
        "          _, outputs, attention = model(image)\n",
        "          loss = criterion(outputs, label)\n",
        "\n",
        "          test_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total += label.size(0)\n",
        "          correct += predicted.eq(label).sum().item()\n",
        "\n",
        "          # print statistics\n",
        "          test_running_loss += loss\n",
        "          test_running_acc += 100.*correct/total\n",
        "          count += 1\n",
        "\n",
        "        print('Test Loss: %.5f | Test Acc: %.3f%%'\n",
        "                      % (test_loss/count, test_running_acc/count))\n",
        "        \n",
        "    # save model\n",
        "    if test_running_acc/count > best_acc:\n",
        "        best_acc = max(test_running_acc/count, best_acc)\n",
        "        PATH = './cifar_net.pth'\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "    \n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlOo-rzY8KQL"
      },
      "source": [
        "##テスト\n",
        "学習したネットワークのテストデータに対する認識精度の確認を行います．\n",
        "まず，学習したネットワークを評価するために保存したモデルをロードします．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSa0ATFj8KQP"
      },
      "source": [
        "depth = 20 #e.g. 20, 32, 44, 47, 56, 110, 1199\n",
        "model = ResNet_ABN(depth)\n",
        "model.to(device)\n",
        "PATH = './cifar_net.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwWfJO358KQS"
      },
      "source": [
        "次に，学習したネットワークを用いて，テストデータに対する認識精度の確認を行います．\n",
        "`model.eval()`を適用することで，ネットワーク演算を評価モードへ変更します． これにより，学習時と評価時で挙動が異なる演算（dropout等）を変更することが可能です． また，`torch.no_grad()`を適用することで，学習時には必要になる勾配情報を保持することなく演算を行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij38kQBP8KQS"
      },
      "source": [
        "model.eval()\n",
        "running_loss = 0.0\n",
        "running_acc = 0.0\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for image, label in testloader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "        _, outputs, attention = model(image)\n",
        "        loss = criterion(outputs, label)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += label.size(0)\n",
        "        correct += predicted.eq(label).sum().item()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss\n",
        "        running_acc += 100.*correct/total\n",
        "        count += 1\n",
        "\n",
        "    print('Test Loss: %.5f | Test Acc: %.3f%%'\n",
        "                  % (test_loss/count, running_acc/count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiY_kiXb8KQV"
      },
      "source": [
        "## Attention mapの可視化\n",
        "Attention mapを可視化して，ネットワークの判断根拠を確認してみます．\n",
        "再度，実行することで他のテストサンプルに対するAttention mapを可視化することができます．\n",
        "pred (prediction) は認識結果，conf (confidence) は認識結果に対する信頼度を示しています．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB0SvioEwlm4"
      },
      "source": [
        "import cv2\n",
        "from ipykernel import kernelapp as app\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
        "                                         shuffle=True, num_workers=20)\n",
        "\n",
        "classes_list = ['plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "def min_max(x, axis=None):\n",
        "    x_min = x.min(axis=axis, keepdims=True)\n",
        "    x_max = x.max(axis=axis, keepdims=True)\n",
        "    return (x - x_min) / (x_max - x_min)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for image, label in testloader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "        _, outputs, attention = model(image)\n",
        "        outputs = softmax(outputs)\n",
        "        conf_data = outputs.data.topk(1, 1, True, True)\n",
        "        _, predicted = outputs.max(1)\n",
        "        c_att = attention.data.cpu()\n",
        "        c_att = c_att.numpy()\n",
        "        d_inputs = image.data.cpu()\n",
        "        d_inputs = d_inputs.numpy()\n",
        "\n",
        "        in_b, in_c, in_y, in_x = image.shape\n",
        "        v_list = []\n",
        "        att_list = []\n",
        "        for item_img, item_att in zip(d_inputs, c_att):\n",
        "            v_img = ((item_img.transpose((1, 2, 0)) * [0.2023, 0.1994, 0.2010]) + [0.4914, 0.4822, 0.4465]) * 255\n",
        "            #v_img = v_img[:, :, ::-1]\n",
        "            resize_att = cv2.resize(item_att[0], (in_x, in_y))\n",
        "            #resize_att = min_max(resize_att)\n",
        "            resize_att *= 255.\n",
        "\n",
        "            v_img = np.uint8(v_img)\n",
        "            resize_att = np.uint8(resize_att)\n",
        "            jet_map = cv2.applyColorMap(resize_att, cv2.COLORMAP_JET)\n",
        "            jet_map = cv2.addWeighted(v_img, 0.6, jet_map, 0.4, 0)\n",
        "            v_list.append(v_img)\n",
        "            att_list.append(jet_map)\n",
        "        break\n",
        "\n",
        "# Show attention map\n",
        "cols = 8\n",
        "rows = 1\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3.0))\n",
        "plt.title('Input image')\n",
        "plt.axis(\"off\")\n",
        "for r in range(rows):\n",
        "    for c in range(cols):\n",
        "        cls = label[c].item()\n",
        "        ax = fig.add_subplot(r+1, cols, c+1)\n",
        "        plt.title('{}'.format(classes_list[cls]))\n",
        "        ax.imshow(v_list[cols * r + c])\n",
        "        ax.set_axis_off()\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3.5))\n",
        "plt.title('Attention map')\n",
        "plt.axis(\"off\")\n",
        "for r in range(rows):\n",
        "    for c in range(cols):\n",
        "        pred = predicted[c].item()\n",
        "        conf = conf_data[0][c].item()\n",
        "        ax = fig.add_subplot(r+1, cols, c+1)\n",
        "        ax.imshow(att_list[cols * r + c])\n",
        "        plt.title('pred: {}\\nconf: {:.2f}'.format(classes_list[pred], conf))\n",
        "        ax.set_axis_off()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mgak3JiLga-P"
      },
      "source": [
        "## 課題\n",
        "1. エポック数やミニバッチサイズを変えて実験し，認識精度とAttention mapの変化を確認しましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9vIEyDvg187"
      },
      "source": [
        "#ここにコードを書く"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCghkSTztILp"
      },
      "source": [
        "# 参考文献\n",
        "- [1] H. Fukui, T. Hirakawa, T. Yamashita, and\n",
        "H. Fujiyoshi, \"Attention branch network: Learning of\n",
        "attention mechanism for visual explanation\". In 2019 IEEE\n",
        "Conference on Computer Vision and Pattern Recognition,\n",
        "pp. 10705–10714, 2019.\n",
        "\n",
        "- [2] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva,\n",
        "and A. Torralba, \"Learning deep features for discriminative\n",
        "localization\". In 2016 IEEE Conference on Computer\n",
        "Vision and Pattern Recognition, pp. 2921–2929, 2016.\n",
        "\n",
        "- [3] M. Lin, Q. Chen, and S. Yan, \"Network in network\".\n",
        "In 2nd International Conference on Learning Representations,\n",
        "Banff, AB, Canada, April 14-16, 2014, Conference\n",
        "Track Proceedings, 2014."
      ]
    }
  ]
}