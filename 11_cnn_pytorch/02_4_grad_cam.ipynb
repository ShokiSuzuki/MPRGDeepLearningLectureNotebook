{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/11_cnn_pytorch/02_4_grad_cam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQfhYcVF8KPt"
      },
      "source": [
        "# CNNの可視化 (Gradient-weighted Class Activation Mapping; Grad-CAM)\n",
        "\n",
        "---\n",
        "\n",
        "## 目的\n",
        "Gradient-weighted Class Activation Mapping (Grad-CAM)の仕組みを理解する.\n",
        "\n",
        "Grad-CAMを用いてCIFAR-10データセットに対するネットワークの判断根拠の可視化を行う．\n",
        "\n",
        "## Gradient-weighted Class Activation Mapping (Grad-CAM)\n",
        "Grad-CAM[1]は，逆伝播時の勾配を用いることでCNNを可視化する手法です．\n",
        "Grad-CAMは，逆伝播時の特定のクラスにおける勾配をGlobal Average Pooling (GAP)[2]により空間方向に対する平均値を求め，各特徴マップに対する重みとします．\n",
        "その後，獲得した重みを各特徴マップに重み付けして，重み付けした特徴マップを全て加算します．\n",
        "最後に加算した特徴マップにReLUを適用することでAttention mapを獲得します．\n",
        "02_CAM.ipynbで使用したClass Activation Mapping (CAM)[3]は，ネットワークの一部をGAPに置き換える必要があるため，Attention mapを獲得するためにネットワークを学習させる必要があります．一方で，Grad-CAMはネットワークの順伝播時の特徴マップと逆伝播時の勾配を用いてAttention mapを獲得します．そのため，学習済みの様々なネットワークからAttention map を獲得することができます．\n",
        "逆伝播時のクラス$c$に対する$k$番目の勾配に対する重み $\\alpha^{c}_{k}$は，以下の式のように表すことができます．\n",
        "\n",
        "$$\\alpha^{c}_{k}=\\frac{1}{Z}\\sum_{x,y}\\frac{\\partial y^c}{\\partial f^{k}_{x,y}}$$\n",
        "ここで，$Z$は勾配の空間方向に対する値の数，$y^c$はクラス$c$に対する出力，$f^{k}_{x,y}$は特徴マップを示します．\n",
        "その後，以下の式よりクラス$c$に対するAttention map $M^{c}_{\\mathrm{Grad-CAM}}$を獲得します．\n",
        "\n",
        "$$M^{c}_{\\mathrm{Grad-CAM}}={\\rm ReLU}\\left( \\sum_{k}\\alpha^{c}_{k}f^{k} \\right)$$\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/11_cnn_pytorch/02_5_grad_cam/grad-cam.png?raw=true\" width = 100%>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8ehSX6U8KPv"
      },
      "source": [
        "## 準備\n",
        "\n",
        "### Google Colaboratoryの設定確認・変更\n",
        "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n",
        "**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n",
        "\n",
        "## モジュールのインポート\n",
        "はじめに必要なモジュールをインポートする．\n",
        "\n",
        "### GPUの確認\n",
        "GPUを使用した計算が可能かどうかを確認します．\n",
        "\n",
        "`GPU availability: True`と表示されれば，GPUを使用した計算をPyTorchで行うことが可能です．\n",
        "Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaNXqInW8KPv"
      },
      "outputs": [],
      "source": [
        "# モジュールのインポート\n",
        "from time import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "\n",
        "import torchsummary\n",
        "\n",
        "# GPUの確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNzvYQL58KP4"
      },
      "source": [
        "## データセットの読み込みとData Augmentation\n",
        "\n",
        "学習データ（CIFAR10データセット）を読み込みます．\n",
        "学習には，大量のデータを利用しますが，それでも十分ではありません． そこで，データ拡張 (data augmentation) により，データのバリエーションを増やします． 一般的な方法は，画像の左右反転，明るさ変換などです．\n",
        "\n",
        "今回はImageNetデータセットで事前に学習したモデルを利用するため，入力サイズをサイズを224ピクセルにします．また，各チャンネルの平均値を基準に平均が0，分散が分散が1になるようにNormalizeを用いて正規化します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leWJTOIL8KP4"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(root=\"./\", train=True, transform=transform_train, download=True)\n",
        "test_data = torchvision.datasets.CIFAR10(root=\"./\", train=False, transform=transform_test, download=True)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DduOi-s18KP_"
      },
      "source": [
        "## ネットワークモデルの定義\n",
        "今回は，ImageNetで事前に学習された学習済みモデルを用いて，転移学習を行います．転移学習とは，ある領域における学習済みモデル（あるデータセットで学習したニューラルネットワーク）を別の領域（別のデータセットでの学習）に活用し，効率的に学習を行う方法です．学習済みモデルには，ResNet-18を利用して学習します．`pretrained = True`にすると，ImageNetで学習したモデルを利用できます．ここで，ImageNetは1,000クラスのデータセットです．すなわち，ImageNetで学習したResNet-18の出力層のユニット数は1,000になっています．転移学習に利用するCIFAR-10データセットは10クラスなので，出力層のユニット数を10に変更します．\n",
        "\n",
        "また，GPUを使う場合（`use_cuda == True`）には，ネットワークモデルをGPUメモリ上に配置します． \n",
        "これにより，GPUを用いた演算が可能となります．\n",
        "学習を行う際の最適化方法としてモーメンタムSGD (モーメンタム付き確率的勾配降下法) を利用します． \n",
        "また，学習率を0.001，モーメンタムを0.9として引数に与えます．\n",
        "\n",
        "最後に，定義したネットワークの詳細情報を`torchsummary.summary()`関数を用いて表示します．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0cpWP_w8KP_"
      },
      "outputs": [],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "print(\"======== Original netowrk architecutre ========\\n\")\n",
        "print(model)\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 10)\n",
        "print(\"======== Fine-funing netowrk architecutre ========\\n\")\n",
        "print(model)\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# モデルの情報を表示\n",
        "torchsummary.summary(model, (3, 32, 32))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4NRFd6i8KQF"
      },
      "source": [
        "## 学習\n",
        "１回の誤差を算出するデータ数（ミニバッチサイズ）を64，学習エポック数を10とします．\n",
        "CIFAR10の学習データサイズを取得し，１エポック内における更新回数を求めます．\n",
        "学習モデルに`image`を与えて各クラスの確率yを取得します．各クラスの確率yと教師ラベル`label`との誤差をsoftmax coross entropy誤差関数で算出します．\n",
        "また，認識精度も算出します．そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnMVTad98KQG"
      },
      "outputs": [],
      "source": [
        "# ミニバッチサイズ・エポック数の設定\n",
        "batch_size = 64\n",
        "epoch_num = 10\n",
        "n_iter = len(train_data) / batch_size\n",
        "\n",
        "# データローダーの設定\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion.cuda()\n",
        "\n",
        "# ネットワークを学習モードへ変更\n",
        "model.train()\n",
        "\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "    \n",
        "    for image, label in train_loader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model(image)\n",
        "        loss = criterion(y, label)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        sum_loss += loss.item()\n",
        "        \n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed_time :{}\".format(epoch,\n",
        "                                                                                 sum_loss / n_iter,\n",
        "                                                                                 count.item() / len(train_data),\n",
        "                                                                                 time() - start))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwWfJO358KQS"
      },
      "source": [
        "## テスト\n",
        "学習したネットワークモデルを用いて評価を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij38kQBP8KQS"
      },
      "outputs": [],
      "source": [
        "# データローダーの準備\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=False)\n",
        "\n",
        "# ネットワークを評価モードへ変更\n",
        "model.eval()\n",
        "\n",
        "# 評価の実行\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for image, label in test_loader:\n",
        "\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "            \n",
        "        y = model(image)\n",
        "\n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "print(\"test accuracy: {}\".format(count.item() / len(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMFN4AkzRM5Y"
      },
      "source": [
        "##Grad-CAMによるAttention mapの可視化\n",
        "Grad-CAMを利用するために必要なツールをインストールします．\n",
        "Grad-CAMは，`pytorch-gradcam`というツールをインストールすることで簡単に利用することができます．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veTKG-vmQsdV"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-gradcam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWkM1AaK829Q"
      },
      "source": [
        "Grad-CAMによりAttention mapを可視化して，ネットワークの判断根拠を確認してみます． 再度，実行することで他のテストサンプルに対するAttention mapを可視化することができます． pred (prediction) は認識結果，conf (confidence) は認識結果に対する信頼度を示しています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6cIbDMZQWaF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from gradcam.utils import visualize_cam\n",
        "from gradcam import GradCAM\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "])\n",
        "\n",
        "test_data = torchvision.datasets.CIFAR10(root=\"./\", train=False, transform=transform_test, download=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=False)\n",
        "\n",
        "classes_list = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Grad-CAM\n",
        "target_layer = model.layer4 # ex., layer1, layer2, layer3, layer3[1], layer4[0]\n",
        "gradcam = GradCAM(model, target_layer)\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "def save_gradcam(gcam, raw_image):\n",
        "    h, w, _ = raw_image.shape\n",
        "    gcam = gcam * 255.0\n",
        "    gcam = np.uint8(gcam)\n",
        "    gcam = gcam.transpose((1, 2, 0))\n",
        "    v_list.append(raw_image)\n",
        "    att_list.append(gcam)\n",
        "\n",
        "v_list = []\n",
        "att_list = []\n",
        "for image, label in test_loader:\n",
        "    if use_cuda:\n",
        "        image = image.cuda()\n",
        "        label = label.cuda()\n",
        "    normed_input_img = transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])(image)\n",
        "    outputs = model(normed_input_img)\n",
        "    outputs = softmax(outputs)\n",
        "    conf_data = outputs.data.topk(k=1, dim=1, largest=True, sorted=True)\n",
        "    _, predicted = outputs.max(1)\n",
        "    d_inputs = image.data.cpu().numpy()\n",
        "    in_b, in_c, in_y, in_x = image.shape\n",
        "\n",
        "    for i in range(in_b):\n",
        "        input = image[i,:,:,:]\n",
        "        normed_torch_img = transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])(input)[None]\n",
        "\n",
        "        v_img = d_inputs[i,:,:,:]\n",
        "        v_img = v_img.transpose(1, 2, 0) * 255\n",
        "        v_img = np.uint8(v_img)\n",
        "\n",
        "        mask, _ = gradcam(normed_torch_img)\n",
        "        heatmap, result = visualize_cam(mask, input)\n",
        "\n",
        "        save_gradcam(result, v_img)\n",
        "    break\n",
        "\n",
        "# Show attention map\n",
        "cols = 8\n",
        "rows = 1\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3.0))\n",
        "plt.title('Input image')\n",
        "plt.axis(\"off\")\n",
        "for r in range(rows):\n",
        "    for c in range(cols):\n",
        "        cls = label[c].item()\n",
        "        ax = fig.add_subplot(r+1, cols, c+1)\n",
        "        plt.title('{}'.format(classes_list[cls]))\n",
        "        ax.imshow(v_list[cols * r + c])\n",
        "        ax.set_axis_off()\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3.5))\n",
        "plt.title('Attention map')\n",
        "plt.axis(\"off\")\n",
        "for r in range(rows):\n",
        "    for c in range(cols):\n",
        "        pred = predicted[c].item()\n",
        "        conf = conf_data[0][c].item()\n",
        "        ax = fig.add_subplot(r+1, cols, c+1)\n",
        "        ax.imshow(att_list[cols * r + c])\n",
        "        plt.title('pred: {}\\nconf: {:.2f}'.format(classes_list[pred], conf))\n",
        "        ax.set_axis_off()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjJ48SKsd3dp"
      },
      "source": [
        "#課題\n",
        "1. Attention mapを可視化する層を変更して，Attention mapの変化を確認してみましょう．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bapCu45reINz"
      },
      "outputs": [],
      "source": [
        "#ここにコードを書く"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2B4F1QjBpa9"
      },
      "source": [
        "# 参考文献\n",
        "- [1] S. Ramprasaath, R., C. Michael, D. Abhishek,\n",
        "V. Ramakrishna, P. Devi, and B.\n",
        "Dhruv, \"Grad-CAM: Visual explanations from deep networks\n",
        "via gradient-based localization\". In International Conference\n",
        "on Computer Vision, pp. 618–626, 2017.\n",
        "\n",
        "- [2] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva,\n",
        "and A. Torralba, \"Learning deep features for discriminative\n",
        "localization\". In 2016 IEEE Conference on Computer\n",
        "Vision and Pattern Recognition, pp. 2921–2929, 2016.\n",
        "\n",
        "- [2] M. Lin, Q. Chen, and S. Yan, \"Network in network\".\n",
        "In 2nd International Conference on Learning Representations,\n",
        "Banff, AB, Canada, April 14-16, 2014, Conference\n",
        "Track Proceedings, 2014."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "02_4_grad_cam.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}