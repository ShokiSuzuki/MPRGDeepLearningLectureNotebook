{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "08-segnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/11_cnn_pytorch/08_segnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSp9ycyYGKjA"
      },
      "source": [
        "# SegNet\n",
        "\n",
        "---\n",
        "\n",
        "## 目的\n",
        "セマンティックセグメンテーションとは何か理解する．\\\n",
        "SegNetの構造を理解する．\\\n",
        "SegNetを用いてARCDatsetでセグメンテーションを行う．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nTBxarRFuO2"
      },
      "source": [
        "## セマンティックセグメンテーション\n",
        "\n",
        "セマンティックセグメンテーションは,\n",
        "画像内のオブジェクトをピクセル単位でクラス分類を行うタスクです．\\\n",
        "複数の物体を認識することができ，物体位置や形状も認識することができます．\n",
        "自動運転や医療画像などの分野で使用されています．\n",
        "\n",
        "<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/377968/d63667de-412a-3b38-afb4-6af4e6dae645.png\" width = 30%>\n",
        "\n",
        "\n",
        "<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/377968/3ffffea2-2a68-4fe4-1c81-b19748ecc2ff.png\" width = 30%>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0cbwkZLH_uz"
      },
      "source": [
        "## SegNet\n",
        "\n",
        "セマンティックセグメンテーションのネットワークは全結合層が無くなり．すべての層が畳み込み層となっています．\\\n",
        "最後の畳み込み層では,元画像と同じサイズ確率マップをクラス数分出力します．\\\n",
        "その確率マップにソフトマックス関数を使い最終的な出力結果とします．\n",
        "<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/377968/94b5a80e-b1aa-87e4-4229-270c4fd440d7.png\" width = 70%>\n",
        "\n",
        "### エンコーダデコーダ構造\n",
        "エンコーダでは，入力画像に対して畳み込みとpoolingを繰り返すことで，圧縮していき特徴マップを抽出する役割をしています．\\\n",
        "デコーダでは，unpoolingと畳み込みを繰り返して，圧縮した特徴マップを元のサイズに戻していきます． エンコーダデコーダ構造にすることで，省メモリ化といった効果があります．\n",
        "\n",
        "\n",
        "## Pooling indices\n",
        "プーリング処理は，繰り返しおこなうことで局所的な特徴が欠落してしまいます．\n",
        "これでは，オブジェクトの境界部分などが曖昧になってしまいます．\n",
        " そこでSegNetでは，エンコーダでMaxPoolingを行ったときに最大値の位置情報を記録します，そして，unpoolingするときにその位置情報を使ってピクセルを戻していきます．この時，記録されていない位置には0が入ります．\\\n",
        "<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/377968/c1c96d67-7a1a-8978-f279-b5706e867dce.png\" width = 70%>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf2xacX0lS98"
      },
      "source": [
        "必要なモジュールのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puNTcqNClSwW"
      },
      "source": [
        "import os\n",
        "from PIL import Image, ImageOps, ImageFilter\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import normalize\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import sys\n",
        "import glob\n",
        "import numbers\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torchsummary\n",
        "import cv2\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6hMkL4PljJB"
      },
      "source": [
        "GPUの確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snuSd9IfPhi3"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EwQYfDKMR0g"
      },
      "source": [
        "GPUの確認です．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTcKUbyjlo96"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFRKOczNFsr1"
      },
      "source": [
        "# データセットの用意\n",
        "今回は，画像サイズは半分，枚数を削減したARCDatsetを使用します．\\\n",
        "40のオブジェクトと背景の全41クラスがラベル付けされています．\\\n",
        "各クラスのラベル付けは以下のように定義されています．\n",
        "<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/377968/5c634cce-1c98-48e7-ae16-c2ea3475d668.png\" width = 70%>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf80Mt8G3Ck4"
      },
      "source": [
        "!wget http://www.mprg.cs.chubu.ac.jp/~masaki/share/ARCdataset_png2.zip\n",
        "!unzip ./ARCdataset_png2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTOgiZw9x0BQ"
      },
      "source": [
        "## データセットクラス\n",
        "\n",
        "セマンティックセグメンテーションは入力画像とラベル画像を用意する必要があります\\\n",
        "そのため，専用のDatasetクラスやデータ拡張クラスを用意しなければなりません．\\\n",
        "専用のDatasetクラスを用意します．\\\n",
        "Datsetクラスを作成するときは，torch.utils.data.Datasetを継承してオーバーライドします．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlH74rQmahX3"
      },
      "source": [
        "def is_image(filename):\n",
        "    return any(filename.endswith(ext) for ext in '.png')\n",
        "\n",
        "def is_label(filename):\n",
        "    return filename.endswith(\"_s.png\")\n",
        "\n",
        "def image_basename(filename):\n",
        "    return os.path.basename(os.path.splitext(filename)[0])\n",
        "\n",
        "class MYDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, split, transform):\n",
        "        self._base_dir = './ARCdataset_png/'\n",
        "        \n",
        "        self.split = split\n",
        "        self.images_root = os.path.join(self._base_dir, split, 'rgb/')\n",
        "        self.labels_root = os.path.join(self._base_dir, split, 'label/')\n",
        "        \n",
        "        self.filenames = [image_basename(f)\n",
        "            for f in os.listdir(self.images_root) if is_image(f)]\n",
        "        self.filenames.sort()\n",
        "\n",
        "        self.filenamesGt = [image_basename(f)\n",
        "            for f in os.listdir(self.labels_root) if is_label(f)]\n",
        "        self.filenamesGt.sort()\n",
        "\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "         # 1. 画像読み込み\n",
        "        image_file_path = self.filenames[index]+ '.png'\n",
        "        image_file_path = os.path.join(self._base_dir, self.split, 'rgb/', image_file_path)\n",
        "        img = Image.open(image_file_path).convert('RGB')\n",
        "\n",
        "        # 2. アノテーション画像読み込み\n",
        "        label_file_path = self.filenamesGt[index]+ '.png'\n",
        "        label_file_path = os.path.join(self._base_dir, self.split, 'label/', label_file_path)\n",
        "        label_class_img = Image.open(label_file_path).convert('L')      \n",
        "        sample = {'image': img, 'label': label_class_img}\n",
        "\n",
        "        # 3. データ拡張を実施\n",
        "        return self.transform(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfBQcwaLqNGB"
      },
      "source": [
        "## データ拡張\n",
        "セマンティックセグメンテーションでデータ拡張する場合は，入力画像とラベル画像に同じ処理を行なう必要があります．\\\n",
        "Pytorchのデータ拡張は，入力画像とラベル画像を同時に行えないため専用のものを用意します．\\\n",
        "今回は切り抜き，正規化の処理をPILという画像処理ライブラリを使い実装します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fKBjvqhq8Sf"
      },
      "source": [
        "class Normalize(object):\n",
        "    def __init__(self, mean=(0., 0., 0.), std=(1., 1., 1.)):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        img = sample['image']\n",
        "        mask = sample['label']\n",
        "        img = np.array(img).astype(np.float32)\n",
        "        mask = np.array(mask).astype(np.float32)\n",
        "        img /= 255.0\n",
        "        img -= self.mean\n",
        "        img /= self.std\n",
        "       \n",
        "        return {'image': img,\n",
        "                'label': mask}\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, sample):\n",
        "       \n",
        "        img = sample['image']\n",
        "        mask = sample['label']\n",
        "        img = np.array(img).astype(np.float32).transpose((2, 0, 1))\n",
        "        mask = np.array(mask).astype(np.float32)\n",
        "\n",
        "        img = torch.from_numpy(img).float()\n",
        "        mask = torch.from_numpy(mask).float()\n",
        "\n",
        "        return {'image': img,\n",
        "                'label': mask}\n",
        "\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    def __init__(self, size, padding=0):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size # h, w\n",
        "        self.padding = padding\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        img, mask = sample['image'], sample['label']\n",
        "\n",
        "        if self.padding > 0:\n",
        "            img = ImageOps.expand(img, border=self.padding, fill=0)\n",
        "            mask = ImageOps.expand(mask, border=self.padding, fill=255)\n",
        "\n",
        "        assert img.size == mask.size\n",
        "        w, h = img.size\n",
        "        th, tw = self.size # target size\n",
        "        x1 = random.randint(0, w - tw)\n",
        "        y1 = random.randint(0, h - th)\n",
        "        img = img.crop((x1, y1, x1 + tw, y1 + th))\n",
        "        mask = mask.crop((x1, y1, x1 + tw, y1 + th))\n",
        "\n",
        "        return {'image': img,\n",
        "                'label': mask}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9it_51ML9NI"
      },
      "source": [
        "## データローダの作成\n",
        "`transforms.Compose`を使い，使用するデータ拡張を設定します．先ほど作成したクロップ，画像の正規化のクラスを使用します．\\\n",
        "次にMYDatasetクラスには学習データか検証データどちらを使用するかとデータ拡張の設定を与えます．\\\n",
        "作成したMYDatasetクラスは`DataLoader`に与えます．\n",
        "ミニバッチは6とします．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lyjjqOXdXZk"
      },
      "source": [
        "# データ拡張を設定\n",
        "transform = transforms.Compose([                          \n",
        "    RandomCrop((320,320)), \n",
        "    Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "# データセットの作成\n",
        "train_dataset = MYDataset(split='train', transform=transform)\n",
        "val_dataset = MYDataset(split='val', transform=test_transform)\n",
        "\n",
        "# データローダーの作成\n",
        "batch_size = 5\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=1, shuffle=True,pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, num_workers=0, shuffle=False,pin_memory=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvURcjTolyLg"
      },
      "source": [
        "# ネットワークモデルの定義\n",
        "SegNetを定義します．\n",
        "\n",
        "エンコーダでは，畳み込み層，BatchNorm，ReLUを2，3回繰り返した後MaxPoolingを行います．Pooling後はチャンネル数を増やして最終的に512チャンネルになります．\n",
        "Pooling処理は，`F.max_pool2d()`の引数を`return_indices=True`にすることで最大値を取った場所の位置情報を獲得することができます．\n",
        "\n",
        "また，各Pooling前の特徴マップのサイズ情報も獲得しておきます\n",
        "\n",
        "\n",
        "デコーダでは，まずアンプ―リング処理を行います．\n",
        "その後，逆畳み込み処理，BatchNorm，ReLUを2，3回繰り返していきます．\n",
        "エンコーダでは，チャンネル数を徐々に増やしていきましたが，デコーダではチャンネル数を徐々に減らしていきます．\n",
        "unpooling処理では，位置情報と出力する得著マップのサイズを渡します.\\\n",
        "`F.max_unpool2d()`の二つ目の引数に`F.max_pool2d()`で獲得した位置情報を与え,\n",
        "`output_siz`に出力する特徴マップサイズを与えます．\n",
        "\n",
        "デコーダの最終層はクラス数のチャンネルを出力します．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ0Ybi0TstZW"
      },
      "source": [
        "class SegNet(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels):\n",
        "        super(SegNet, self).__init__()\n",
        "\n",
        "        # Encoder layers\n",
        "\n",
        "        self.encoder_0 = nn.Sequential(nn.Conv2d(in_channels=input_channels, out_channels=64, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(64),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(64),\n",
        "                                            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.encoder_1= nn.Sequential(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(128),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(128),\n",
        "                                            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.encoder_2 = nn.Sequential(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(256),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(256),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(256),\n",
        "                                            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.encoder_3 = nn.Sequential(nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(512),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(512),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=512,out_channels=512, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(512),\n",
        "                                            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.encoder_4 = nn.Sequential(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(512),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(512),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(512),\n",
        "                                            nn.ReLU(inplace=True))\n",
        "\n",
        "        # Decoder layers\n",
        "\n",
        "        self.decoder_4 = nn.Sequential(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(512),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(512),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(512),\n",
        "                                            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.decoder_3 = nn.Sequential(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(512),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(512),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(256),\n",
        "                                            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.decoder_2 = nn.Sequential(nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(256),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(256),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(128),\n",
        "                                            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.decoder_1 = nn.Sequential(nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(128),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(64),\n",
        "                                            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.decoder_0 = nn.Sequential(nn.Conv2d(in_channels=64,out_channels=64, kernel_size=3, padding=1),\n",
        "                                            nn.BatchNorm2d(64),\n",
        "                                            nn.ReLU(inplace=True),\n",
        "                                            nn.Conv2d(in_channels=64, out_channels=output_channels, kernel_size=1))\n",
        "\n",
        "        self._init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass `input_img` through the network\n",
        "        \"\"\"\n",
        "\n",
        "        # Encoder\n",
        "\n",
        "        # Encoder Stage - 1\n",
        "        dim_0 = x.size()\n",
        "        x = self.encoder_0(x)\n",
        "        x, indices_0 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Encoder Stage - 2\n",
        "        dim_1 = x.size()\n",
        "        x = self.encoder_1(x)\n",
        "        x, indices_1 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Encoder Stage - 3\n",
        "        dim_2 = x.size()\n",
        "        x = self.encoder_2(x)\n",
        "        x, indices_2 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Encoder Stage - 4\n",
        "        dim_3 = x.size()\n",
        "        x = self.encoder_3(x)\n",
        "        x, indices_3 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Encoder Stage - 5\n",
        "        dim_4 = x.size()\n",
        "        x = self.encoder_4(x)\n",
        "        x, indices_4 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Decoder\n",
        "\n",
        "        #dim_d = x.size()\n",
        "\n",
        "        # Decoder Stage - 5\n",
        "        x = F.max_unpool2d(x, indices_4, kernel_size=2, stride=2, output_size=dim_4)\n",
        "        x = self.decoder_4(x)\n",
        "        #dim_4d = x.size()\n",
        "\n",
        "        # Decoder Stage - 4\n",
        "        x = F.max_unpool2d(x, indices_3, kernel_size=2, stride=2, output_size=dim_3)\n",
        "        x = self.decoder_3(x)\n",
        "        #dim_3d = x.size()\n",
        "\n",
        "        # Decoder Stage - 3\n",
        "        x = F.max_unpool2d(x, indices_2, kernel_size=2, stride=2, output_size=dim_2)\n",
        "        x = self.decoder_2(x)\n",
        "        #dim_2d = x.size()\n",
        "\n",
        "        # Decoder Stage - 2\n",
        "        x = F.max_unpool2d(x, indices_1, kernel_size=2, stride=2, output_size=dim_1)\n",
        "        x = self.decoder_1(x)\n",
        "\n",
        "        # Decoder Stage - 1\n",
        "        x = F.max_unpool2d(x, indices_0, kernel_size=2, stride=2, output_size=dim_0)\n",
        "        x = self.decoder_0(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def _init_weight(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkeuMX5zCLMc"
      },
      "source": [
        "# 学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13G3IFnwGLPe"
      },
      "source": [
        "## ネットワークの設定\n",
        "\n",
        "定義したネットワークを作成します．\n",
        "`SegNet`クラスを呼び出して，ネットワークモデルを定義します．\n",
        "また，GPUを使う場合（`use_cuda == True`）には，ネットワークモデルをGPUメモリ上に配置します．\n",
        "これにより，GPUを用いた演算が可能となります．\n",
        "\n",
        "学習を行う際の最適化方法としてモーメンタムSGD(モーメンタム付き確率的勾配降下法）を利用します．\n",
        "また，学習率を0.01，モーメンタムを0.9として引数に与えます．\n",
        "\n",
        "定義したネットワーク情報を`torchsummary.summary()`関数を用いて表示ます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O1dE1FN4Dcj"
      },
      "source": [
        "import time\n",
        "num_class = 41\n",
        "model = SegNet(input_channels=3, output_channels=num_class)\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "#エポック数の設定\n",
        "epoch_num = 10\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "if use_cuda:\n",
        "    criterion.cuda()\n",
        "\n",
        "#モデルの情報を表示\n",
        "torchsummary.summary(model,(3,128,128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpYGTvgtEKXQ"
      },
      "source": [
        "## 評価関数の設定\n",
        "mIoUとAccuracyの設定を行います．\n",
        "\n",
        "IoUはIntersection(領域の共通部分) over Union(領域の和集合)の略で，セマンティックセグメンテーションでよく使用される評価指標です．\n",
        "正解の領域と予測した領域がどれくらい重なっているかを表す指標になっています．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udXH8jVMtGqU"
      },
      "source": [
        "class Evaluator(object):\n",
        "    def __init__(self, num_class):\n",
        "        \n",
        "        self.num_class = num_class\n",
        "        self.confusion_matrix = np.zeros((self.num_class,)*2)\n",
        "\n",
        "    def Pixel_Accuracy(self):\n",
        "        Acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n",
        "        return Acc\n",
        "\n",
        "    def Mean_Intersection_over_Union(self):\n",
        "        MIoU = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "        MIoU = np.nanmean(MIoU)\n",
        "        return MIoU\n",
        "\n",
        "    def _generate_matrix(self, gt_image, pre_image):\n",
        "        mask = (gt_image >= 0) & (gt_image < self.num_class)\n",
        "        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n",
        "        count = np.bincount(label, minlength=self.num_class**2)\n",
        "        confusion_matrix = count.reshape(self.num_class, self.num_class)\n",
        "        return confusion_matrix\n",
        "\n",
        "    def add_batch(self, gt_image, pre_image):\n",
        "        assert gt_image.shape == pre_image.shape\n",
        "        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.num_class,) * 2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om2rxBKKPpt4"
      },
      "source": [
        "## モデルの学習\n",
        "\n",
        "\n",
        "誤差関数を設定します． 使用する誤差関数はクロスエントロピー誤差です．CrossEntropyLossをcriterionとして定義します．\n",
        "\n",
        "学習を開始します．\n",
        "\n",
        "各更新において，学習用データと教師データをそれぞれimageとlabelとします． 学習モデルにimageを与えて画素レベルでクラスの確率を出力するyを取得します． 各クラスの確率yと教師ラベルlabelとの誤差をcriterionで算出します． また，認識精度も算出します． そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．\n",
        "\n",
        "segmentationの学習は時間がかかるため今回は学習済みのモデルを使用して学習します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB8jS1J24ES0"
      },
      "source": [
        "# 学習済みモデルを呼び出す\n",
        "\n",
        "# load_path = \"./ARCdataset_png/checkpoint.pth.tar\"\n",
        "# checkpoint = torch.load(load_path)\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "#評価関数\n",
        "evaluator = Evaluator(num_class)\n",
        "# 学習の実行\n",
        "loss_history=[]\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "    evaluator.reset()\n",
        "    # ネットワークを学習モードへ変更\n",
        "    model.train()\n",
        "\n",
        "    for sample in train_loader:\n",
        "\n",
        "        image, label = sample['image'], sample['label']\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "        y = model(image)\n",
        "        loss = criterion(y, label.long())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #sum_loss += loss.item()\n",
        "\n",
        "    # ネットワークを評価モードへ変更\n",
        "    model.eval()\n",
        "    # 評価の実行\n",
        "    for sample in val_loader:\n",
        "        image, label = sample['image'], sample['label']\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "        with torch.no_grad():\n",
        "            y = model(image)\n",
        "\n",
        "        loss = criterion(y, label.long())\n",
        "        sum_loss += loss.item()\n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        pred = pred.data.cpu().numpy()\n",
        "        label = label.cpu().numpy()\n",
        "        evaluator.add_batch(label, pred) \n",
        "       \n",
        "    #img_size = image.size()\n",
        "    #loss_history.append(sum_loss)\n",
        "    mIoU = evaluator.Mean_Intersection_over_Union()\n",
        "    Acc = evaluator.Pixel_Accuracy()\n",
        "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}，　mean IoU: {}\".format(epoch, sum_loss/(len(train_loader)*batch_size), Acc, mIoU))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKo4Hlf9BTgs"
      },
      "source": [
        "## 学習結果\n",
        "出力結果を確認しやすくするため，各クラスの色を定義する．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyM-hvi8Ti1K"
      },
      "source": [
        "#ラベル画像がRGB画像となっているので0～41の値に変換します\n",
        "class_color = np.array([\n",
        "           [  0,   0,   0],\n",
        "           [ 85,   0,   0],\n",
        "           [170,   0,   0],\n",
        "           [255,   0,   0],\n",
        "           [  0,  85,   0],\n",
        "           [ 85,  85,   0],\n",
        "           [170,  85,   0],\n",
        "           [255,  85,   0],\n",
        "           [  0, 170,   0],\n",
        "           [ 85, 170,   0],\n",
        "           [170, 170,   0],\n",
        "           [255, 170,   0],\n",
        "           [  0, 255,   0],\n",
        "           [ 85, 255,   0],\n",
        "           [170, 255,   0],\n",
        "           [255, 255,   0],\n",
        "           [  0,   0,  85],\n",
        "           [ 85,   0,  85],\n",
        "           [170,   0,  85],\n",
        "           [255,   0,  85],\n",
        "           [  0,  85,  85],\n",
        "           [ 85,  85,  85],\n",
        "           [170,  85,  85],\n",
        "           [255,  85,  85],\n",
        "           [  0, 170,  85],\n",
        "           [ 85, 170,  85],\n",
        "           [170, 170,  85],\n",
        "           [255, 170,  85],\n",
        "           [  0, 255,  85],\n",
        "           [ 85, 255,  85],\n",
        "           [170, 255,  85],\n",
        "           [255, 255,  85],\n",
        "           [  0,   0, 170],\n",
        "           [ 85,   0, 170],\n",
        "           [170,   0, 170],\n",
        "           [255,   0, 170],\n",
        "           [  0,  85, 170],\n",
        "           [ 85,  85, 170],\n",
        "           [170,  85, 170],\n",
        "           [255,  85, 170],\n",
        "           [  0, 170, 170]])\n",
        "\n",
        "\n",
        "\n",
        "class_color = class_color[:, ::-1]\n",
        "print(class_color.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgahLxiVTiF_"
      },
      "source": [
        "学習した結果を確認します．\n",
        "入力画像，教師画像，出力画像を表示させます．\\\n",
        "また，クラスごとの確率マップを表示させて，各クラスの認識結果がどのようになっているか確認します．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMK767PlgGo9"
      },
      "source": [
        "# ネットワークを評価モードへ変更\n",
        "model.eval()\n",
        "classes_list=['input','GT_label','output','background']\n",
        "# 評価の実行\n",
        "count = 0\n",
        "evaluator = Evaluator(num_class)\n",
        "evaluator.reset()\n",
        "for sample in val_loader:\n",
        "    image, label = sample['image'], sample['label']\n",
        "    if use_cuda:\n",
        "        image = image.cuda()\n",
        "        label = label.cuda()\n",
        "    with torch.no_grad():\n",
        "        y = model(image)\n",
        "\n",
        "    pred = torch.argmax(y, dim=1)\n",
        "    pred = pred.data.cpu().numpy()\n",
        "    label = label.cpu().numpy()\n",
        "    yy = F.softmax(y,dim=1) \n",
        "    image = image.data.cpu().numpy()\n",
        "    yy = yy.cpu().numpy()\n",
        "\n",
        "    evaluator.add_batch(label, pred)        \n",
        "    img_list = []\n",
        "\n",
        "    image=image[0]\n",
        "    v_img = ((image.transpose((1, 2, 0)) * [0.2023, 0.1994, 0.2010]) + [0.4914, 0.4822, 0.4465]) * 255\n",
        "    v_img = np.uint8(v_img)\n",
        "    img_list.append(v_img)\n",
        "\n",
        "    result_img = np.transpose(pred, axes=[1, 2, 0])\n",
        "    result_img = np.array(result_img).astype(np.uint8)\n",
        "    label = np.transpose(label, axes=[1, 2, 0])\n",
        "    label = np.array(label).astype(np.uint8)\n",
        "    \n",
        "    result_img = cv2.cvtColor(result_img, cv2.COLOR_GRAY2BGR)\n",
        "    label = cv2.cvtColor(label, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    for i in range(0, class_color.shape[0]):\n",
        "        result_img[np.where((result_img ==  [i, i, i]).all(axis=2))] = class_color[i]\n",
        "        label[np.where((label ==  [i, i, i]).all(axis=2))] = class_color[i]\n",
        "\n",
        "    label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)\n",
        "    img_list.append(label)\n",
        "    result_img = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
        "    img_list.append(result_img)\n",
        "\n",
        "    for i in range(yy.shape[1]):\n",
        "        map = yy[:,i,:,:]\n",
        "        map = np.transpose(map, axes=[1, 2, 0])\n",
        "        map = np.uint8(map*255)\n",
        "        map = cv2.applyColorMap(map, cv2.COLORMAP_JET)\n",
        "        map = cv2.cvtColor(map, cv2.COLOR_BGR2RGB)\n",
        "        img_list.append(map)\n",
        "\n",
        "    row = 6\n",
        "    col = 8\n",
        "    plt.figure(figsize=(18,10))\n",
        "    num = 0\n",
        "    while num < len(img_list):\n",
        "        num += 1\n",
        "        plt.subplot(row, col, num)\n",
        "        plt.imshow(img_list[num-1])\n",
        "        if num-1 < 4: \n",
        "            plt.title('{}'.format(classes_list[num-1]))\n",
        "        else:\n",
        "            plt.title('item{}'.format(num-4))  \n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "mIoU = evaluator.Mean_Intersection_over_Union()\n",
        "Acc = evaluator.Pixel_Accuracy()\n",
        "print(\"mean accuracy: {}, mean IoU: {}\".format(Acc, mIoU))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEb0pAp8f_8h"
      },
      "source": [
        "## 学習済みモデル\n",
        "\n",
        "数epochの学習では，まったく認識ができていないことが確認できました．\\\n",
        "実際．数百epochは学習しないと良い結果が出てくれません．\\\n",
        "今回は，400epoch学習したモデル (学習時間約4日) を読み込んで結果画像を確認します．\n",
        "\n",
        "モデルの読み込みは`torch.load`で行います．\n",
        "`model.load_state_dict`でネットワークに重みを渡します．\n",
        "出力結果画像と正解画像を表示して比べます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5LMHV7BtGOa"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "classes_list=['input','GT_label','output','background']\n",
        "load_path = \"./ARCdataset_png/checkpoint.pth.tar\"\n",
        "checkpoint = torch.load(load_path)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "\n",
        "# ネットワークを評価モードへ変更\n",
        "model.eval()\n",
        "evaluator = Evaluator(num_class)\n",
        "evaluator.reset()\n",
        "# 評価の実行\n",
        "count = 0\n",
        "img_list = []\n",
        "with torch.no_grad():\n",
        "    for sample in val_loader:\n",
        "        image, label = sample['image'], sample['label']\n",
        "\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "           \n",
        "        y = model(image)\n",
        "        yy = F.softmax(y,dim=1) \n",
        "        image = image.data.cpu().numpy()\n",
        "        pred = y.data.cpu().numpy()\n",
        "        yy = yy.data.cpu().numpy()\n",
        "        label = label.cpu().numpy()\n",
        "        img_list = []   \n",
        "\n",
        "        image=image[0]\n",
        "        v_img = ((image.transpose((1, 2, 0)) * [0.2023, 0.1994, 0.2010]) + [0.4914, 0.4822, 0.4465]) * 255\n",
        "        v_img = np.uint8(v_img)\n",
        "        img_list.append(v_img)\n",
        "\n",
        "        pred = np.argmax(pred, axis=1)\n",
        "        evaluator.add_batch(label, pred)    \n",
        "        result_img = np.transpose(pred, axes=[1, 2, 0])\n",
        "        result_img = np.array(result_img).astype(np.uint8)\n",
        "\n",
        "        label = np.transpose(label, axes=[1, 2, 0])\n",
        "        label = np.array(label).astype(np.uint8)\n",
        "       \n",
        "        result_img = cv2.cvtColor(result_img, cv2.COLOR_GRAY2BGR)\n",
        "        label = cv2.cvtColor(label, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "        for i in range(0, class_color.shape[0]):\n",
        "            result_img[np.where((result_img ==  [i, i, i]).all(axis=2))] = class_color[i]\n",
        "            label[np.where((label ==  [i, i, i]).all(axis=2))] = class_color[i]\n",
        "\n",
        "        label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)\n",
        "        img_list.append(label)\n",
        "        result_img = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
        "        img_list.append(result_img)\n",
        "\n",
        "        for i in range(yy.shape[1]):\n",
        "            map = yy[:,i,:,:]\n",
        "            map = np.transpose(map, axes=[1, 2, 0])\n",
        "            map = np.uint8(map*255)\n",
        "            map = cv2.applyColorMap(map, cv2.COLORMAP_JET)\n",
        "            map = cv2.cvtColor(map, cv2.COLOR_BGR2RGB)\n",
        "            img_list.append(map)\n",
        "\n",
        "        row = 6\n",
        "        col = 8\n",
        "        plt.figure(figsize=(18,10))\n",
        "\n",
        "        num = 0\n",
        "\n",
        "        while num < len(img_list):\n",
        "            num += 1\n",
        "            plt.subplot(row, col, num)\n",
        "            plt.imshow(img_list[num-1])\n",
        "            if num-1 < 4: \n",
        "                plt.title('{}'.format(classes_list[num-1]))\n",
        "            else:\n",
        "                plt.title('item{}'.format(num-4))  \n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        " \n",
        "\n",
        "mIoU = evaluator.Mean_Intersection_over_Union()\n",
        "Acc = evaluator.Pixel_Accuracy()\n",
        "print(\"mean accuracy: {}，mean IoU: {}\".format(Acc, mIoU))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoJIIeMmmZZ3"
      },
      "source": [
        "# 課題\n",
        "学習済みモデルを使って学習してみましょう"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdScpIkUkHEN"
      },
      "source": [
        "# 参考文献\n",
        "\n",
        "V. Badrinarayanan, A. Kendall and R. Cipolla, \"SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 12, pp. 2481-2495, 1 Dec. 2017, doi: 10.1109/TPAMI.2016.2644615.\n"
      ]
    }
  ]
}