{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 既存のデータセットの活用\n",
    "\n",
    "---\n",
    "## 目的\n",
    "PyTorch（`torchvision`）における既存のデータセットクラスとその使用方法について理解する．\n",
    "\n",
    "## 準備\n",
    "\n",
    "### Google Colaboratoryの設定確認・変更\n",
    "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n",
    "**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n",
    "\n",
    "## モジュールのインポート\n",
    "はじめに，必要なモジュールをインポートしたのち，GPUを使用した計算が可能かどうかを確認します．\n",
    "\n",
    "### GPUの確認\n",
    "GPUを使用した計算が可能かどうかを確認します．\n",
    "\n",
    "`GPU availability: True`と表示されれば，GPUを使用した計算をPyTorchで行うことが可能です．\n",
    "Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モジュールのインポート\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# GPUの確認\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Use CUDA:', use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 既存のデータセット\n",
    "\n",
    "PyTorchのコンピュータビジョン用ライブラリであるtorchvisionには，様々な既存のデータセットを簡単に利用できるよう，データセットクラスが定義されています．\n",
    "以下では，画像分類タスクの代表的なデータセットである\n",
    "\n",
    "* MNIST\n",
    "* CIFAR10\n",
    "* CIFAR100\n",
    "* ImageNet\n",
    "\n",
    "データセットについて紹介します．\n",
    "\n",
    "その他のデータセットクラスについては，[torchvision.datasetsのリファレンスページ](https://pytorch.org/vision/stable/datasets.html)をご確認ください．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## MNISTデータセット\n",
    "\n",
    "[MNISTデータセット](http://yann.lecun.com/exdb/mnist/)は，0~9までの手書き数字を分類するためのデータセットであり，Deep Learningを学ぶ際のHello Worldのような位置づけのデータセットです．\n",
    "これまでにほかのノートブックを使用したことのある方であれば，ご存知だと思いますので，詳細については割愛します．\n",
    "\n",
    "### 使い方\n",
    "MNISTデータセットは`torchvision.datasets.MNIST`クラスを呼び出すことで，データのダウンロードから読み込みまでを自動的に行い，すぐに使用することが可能です．\n",
    "\n",
    "`root`はMNISTデータセットのあるディレクトリまでのパス，`train`は学習用データを使用するかどうか（`False`の場合はテストデータを使用），`download`は`root`にデータセットがない場合に自動的にダウンロードするかどうかを示す引数です．\n",
    "\n",
    "\n",
    "`transform`は画像データに対して行う前処理を定義して受け渡す引数です．\n",
    "これ以降で紹介するデータセットについても`transform`は引数として存在しており，各データに対する前処理を簡単に定義することができます．\n",
    "`transform`の定義としては，`torchvision.transforms`を使用します．\n",
    "`torchvision.transforms`には様々な前処理のためのクラスが用意されています．\n",
    "代表的なものとしては，\n",
    "\n",
    "* `ToTensor()`: 呼び出した画像データ（Numpy arrayまたはPILフォーマットの画像データ）を`torch.Tensor`（PyTorchの配列型式）に変換するクラス\n",
    "* `Normalize()`: 画像データを正規化するクラス\n",
    "* `Grayscale()`: 画像をグレースケール変換するクラス\n",
    "* `RandomCrop()`: 画像をランダムクロップするクラス（Data Augmentation）\n",
    "\n",
    "などです．\n",
    "\n",
    "今回は，読み込んだデータを`Tensor`型の配列に変換する`ToTensor()`を前処理として使用します．\n",
    "\n",
    "そのほかにも様々な前処理がありますので，興味のある方は[torchvision.transformsのリファレンスページ](https://pytorch.org/vision/stable/transforms.html)より確認してください．\n",
    "\n",
    "また，`torchvision.transforms`のより詳細な使い方に関しては，[データ拡張（Data Augmentation）](02_dnn_simple_pytorch/augmentation.ipynb)にて説明を行っていますので，そちらを参照してください． [![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/02_dnn_simple_pytorch/augmentation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST(root=\"./\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = torchvision.datasets.MNIST(root=\"./\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "print(type(train_data.data), type(train_data.targets))\n",
    "print(type(test_data.data), type(test_data.targets))\n",
    "print(train_data.data.size(), train_data.targets.size())\n",
    "print(test_data.data.size(), test_data.targets.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットクラスの情報とデータの呼び出し\n",
    "\n",
    "`MNIST()`などのtorchvisionのデータセットクラスは，呼び出し後，クラスインスタンスのインデックスを指定することで，画像データとラベルを呼び出すことができます．\n",
    "\n",
    "**データセットのサンプル数**\n",
    "\n",
    "`len()`を用いることで，データセット内のサンプル数を確認することができます．\n",
    "\n",
    "\n",
    "**インデックスを用いたデータの呼び出し**\n",
    "\n",
    "`train_data[0]`のようにインデックスを指定することで，対応する番号の画像データとそのラベルを呼び出すことが可能です．\n",
    "インデックスの上限は`len()`で表示した，サンプル数に対応しています．\n",
    "\n",
    "呼び出した画像データの配列の形状を`size()`を用いて確認してみます．\n",
    "その場合．配列のサイズが`[1, 28, 28]`となっていることがわかります．\n",
    "これは，前処理の`ToTensor()`を使用するようにしているためであり，`ToTensor()`を用いることで，`Tensor`型の配列に変換すると同時に配列の型式を`[チャンネル, 縦, 横]`の順番に入れ替える配列操作も同時に行っているためです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ数の確認\n",
    "print(\"The number of training data:\", len(train_data))\n",
    "print(\"The number of test data:\", len(test_data))\n",
    "\n",
    "# インデックスを用いたデータの呼び出し\n",
    "image_data, label_data = train_data[0]\n",
    "print(type(image_data), image_data.size())\n",
    "print(type(label_data), label_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaderの定義\n",
    "\n",
    "上記のデータセットクラスをそのまま活用することで，ネットワークの学習や評価を行うことも可能ですが，\n",
    "データの呼び出しを便利にするために，`torch.utils.data.DataLoader`を活用する方法を紹介します．\n",
    "\n",
    "`DataLoader`には次のような引数があります．\n",
    "\n",
    "* `dataset`に使用したいデータセットクラスのインスタンスを指定します．\n",
    "* `batch_size`で，一度の呼び出したいサンプル数（ミニバッチサイズ）を指定します．\n",
    "* `shuffle`で，ランダムにサンプルを抽出するかどうかを選択します．\n",
    "\n",
    "そのほかにも様々な引数がありますが，興味のある方は[DataLoaderのリファレンスページ](https://pytorch.org/docs/stable/data.html)をご確認ください．\n",
    "\n",
    "以下のプログラムでは，MNISTデータセットをDataLoaderへと受け渡して，実際にデータを呼び出してみます．\n",
    "定義したDataLoaderは，データセットのサンプル数と指定した`batch_size`から，全てのデータが一通り呼び出される回数を自動的に計算します．\n",
    "そして，for文に用いることで，データとラベルをループで呼び出すことが可能です．\n",
    "\n",
    "呼び出されたデータを確認すると，画像データの配列サイズは，`[10, 1, 28, 28]`となっていることがわかります．\n",
    "これは，`[ミニバッチサイズ，チャンネル，縦，横]`という配列サイズになっています．\n",
    "DataLoaderでは，1次元目がバッチサイズとなり，その後ろはDatasetクラスでインデックスを用いたデータの呼び出しの際の配列の形状が維持されます．\n",
    "\n",
    "以前のノートブックから，PyTorchでのネットワーク（畳み込み層）へのデータの入力の形式は`[ミニバッチサイズ，チャンネル，縦，横]`でした．\n",
    "そのため，データセットクラスの`transform`に`ToTensor()`を指定して，DataLoaderへ受け渡して使用すると．自動的にネットワークへ入力することのできる配列になった状態でデータを受け取ることが可能です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderの定義\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=10, shuffle=False)\n",
    "\n",
    "for image, label in train_loader:\n",
    "    print(image.size())\n",
    "    print(label.size(), label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CIFAR10データセット\n",
    "\n",
    "[CIFAR10/100データセット](https://www.cs.toronto.edu/~kriz/cifar.html)は，飛行機や犬などの物体が表示されている画像から構成されたデータセットである．\n",
    "\n",
    "CIFAR10では，10クラスの画像データから構成されています．\n",
    "\n",
    "![CIFAR10_sample.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/176458/b6b43478-c85f-9211-7bc6-227d9b387af5.png)\n",
    "\n",
    "呼び出しには`CIFAR10()`を使用します．\n",
    "引数については，前述のMNISTと同様ですので，説明は割愛します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root=\"./\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(root=\"./\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "# DataLoaderの定義\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=10, shuffle=False)\n",
    "\n",
    "for image, label in train_loader:\n",
    "    print(image.size())\n",
    "    print(label.size(), label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CIFAR100データセット\n",
    "\n",
    "CIFAR100は100クラスの画像データから構成されるデータセットである．\n",
    "\n",
    "![1fTQtXyApxWPoW2vzSEk_Pw_t41Ubes.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/143078/484c6678-85f6-341e-1e63-d686e3ba9c47.png)\n",
    "\n",
    "CIFAR100には，各物体のクラスだけではなく，その上位クラスであるsuperclassが用意されている．\n",
    "superclassの数は20であり，それぞれのsuperclassは5種類のクラスから構成されている．\n",
    "\n",
    "| Superclass | Classes |\n",
    "|-------------------|----------|\n",
    "| aquatic mammals | beaver, dolphin, otter, seal, whale |\n",
    "| fish | aquarium fish, flatfish, ray, shark, trout |\n",
    "| flowers | orchids, poppies, roses, sunflowers, tulips |\n",
    "| food containers | bottles, bowls, cans, cups, plates |\n",
    "| fruit and vegetables | apples, mushrooms, oranges, pears, sweet peppers |\n",
    "| household electrical devices | clock, computer keyboard, lamp, telephone, television |\n",
    "| household furniture | bed, chair, couch, table, wardrobe |\n",
    "| insects | bee, beetle, butterfly, caterpillar, cockroach |\n",
    "| large carnivores | bear, leopard, lion, tiger, wolf |\n",
    "| large man-made outdoor things | bridge, castle, house, road, skyscraper |\n",
    "| large natural outdoor scenes | cloud, forest, mountain, plain, sea |\n",
    "| large omnivores and herbivores | camel, cattle, chimpanzee, elephant, kangaroo |\n",
    "| medium-sized mammals | fox, porcupine, possum, raccoon, skunk |\n",
    "| non-insect invertebrates | crab, lobster, snail, spider, worm |\n",
    "| people | baby, boy, girl, man, woman |\n",
    "| reptiles | crocodile, dinosaur, lizard, snake, turtle |\n",
    "| small mammals | hamster, mouse, rabbit, shrew, squirrel |\n",
    "| trees | maple, oak, palm, pine, willow |\n",
    "| vehicles 1 | bicycle, bus, motorcycle, pickup truck, train |\n",
    "| vehicles 2 | lawn-mower, rocket, streetcar, tank, tractor |\n",
    "\n",
    "呼び出しには`CIFAR100()`を使用します．\n",
    "引数については，前述のMNISTと同様ですので，説明は割愛します．\n",
    "\n",
    "※ superclassの情報は`torchvision.datasets.CIFAR100`には含まれていません．\n",
    "必要な場合は，こちらの[GitHub](https://github.com/ryanchankh/cifar100coarse)などを参考にご活用ください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.CIFAR100(root=\"./\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = torchvision.datasets.CIFAR100(root=\"./\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "# DataLoaderの定義\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=10, shuffle=False)\n",
    "\n",
    "for image, label in train_loader:\n",
    "    print(image.size())\n",
    "    print(label.size(), label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ImageNet & ILSVRC\n",
    "\n",
    "### ImageNet\n",
    "[ImageNet](https://image-net.org/index.php)は，Stanford大学のFei-Fei Liらのを中心としたグループが作成・管理している画像データベースです．\n",
    "ImageNetは1400万枚を超える画像から構成されています．\n",
    "各画像には物体のクラスが付与されており，ImageNet内のクラス数は2万種類以上となっています．\n",
    "\n",
    "これらの物体クラスは，[WordNet](https://wordnet.princeton.edu/)と呼ばれる英語の語彙のデータベースを基に作成されています．\n",
    "WordNetは**Synset**と呼ばれる同義語 (synonyms) セットがツリー階層構造でグループを形成しながら定義されている．\n",
    "ImageNetではその2万以上のsynsetが採用されています．\n",
    "\n",
    "![ImageNet](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/143078/223f3a75-75ed-8601-a017-02cf84d5d31e.jpeg)\n",
    "\n",
    "\n",
    "### ImageNet Large Scale Visual Recognition Challenge (ILSVRC)\n",
    "\n",
    "[ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://www.image-net.org/challenges/LSVRC/) は，2010年から2017年まで開催された大規模画像認識コンペです．\n",
    "ILSVRCでは，上で紹介したImageNetの1400万枚の画像全てを使用するわけではなく，一部の画像データを使用して学習や評価を行なっています．\n",
    "\n",
    "また，ILSVRCにはいくつかのタスクが設けられています．\n",
    "\n",
    "1. Classification (2010 ~)\n",
    "2. Classification with Localizaiton (2011 ~)\n",
    "3. Fine-grained Classification (2012 ~)\n",
    "4. Detection (2013 ~)\n",
    "5. Object Detection from Video (2015 ~)\n",
    "6. Scene classification (2015, 2016)\n",
    "7. Scene Parsing (2016)\n",
    "\n",
    "このうち，1.のClassificationタスクが最も有名なタスクです．\n",
    "Classificationタスクは，1000カテゴリの物体クラスを分類するタスクであり，学習用データ120万枚，検証用データ5万枚，テストデータ10万枚のデータから構成されています．\n",
    "\n",
    "2010年からスタートしたコンペタスクですが．2012年に開催されたILSVRC 2012にて，CNNモデル (AlexNet) が高い認識精度（低いエラー率）で優勝したことをキッカケに，ディープラーニングが注目を浴びました．\n",
    "それ以降，様々なネットワークが提案され，このILSVRC2012で使用されたデータセットを用いて認識性能向上を図っています．\n",
    "このILSVRC2012で用いられたデータセットを通称 **ImageNet** と呼び，広く用いられています．\n",
    "\n",
    "最後のILSVRCとなった2017年以降は，画像認識コンペの役割を[Kaggle](https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description)に譲渡しています．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNetのPyTorchでの使用方法\n",
    "\n",
    "`torchvision.datasets`にImageNetのデータセットクラスは用意されていますが，\n",
    "\n",
    "* ImageNetは非常に大規模なデータセットであること\n",
    "* ダウンロードにはユーザー登録が必要なこと\n",
    "\n",
    "から前述のMNISTやCIFARのように自動でダウンロードを行なってはくれません．\n",
    "データの準備に非常に時間がかかるため，ここではImageNetを使用するための手順について説明し，最後にデータセットを呼び出すためのスクリプトを紹介するのみとします．\n",
    "\n",
    "#### 1. ImageNet公式サイトへのユーザー登録\n",
    "\n",
    "まず，[ImageNetの公式サイト](https://image-net.org/index.php)へ移動し，ユーザー登録を行います．\n",
    "画面右上あたりに`signup`のリンクがありますので，そこからユーザー登録を行ってください．\n",
    "\n",
    "※ すでにユーザー登録を行っている人は，ログインを行ってください．\n",
    "\n",
    "#### 2. ImageNetデータセットのダウンロードと展開\n",
    "\n",
    "次に．[ILSVRC2012のデータセットダウンロードページ](https://image-net.org/challenges/LSVRC/2012/2012-downloads.php)へ移動し，\n",
    "* Development kit (Task 1 & 2). 2.5MB.\n",
    "* Training images (Task 1 & 2). 138GB. MD5: 1d675b47d978889d74fa0da5fadfb00e\n",
    "* Validation images (all tasks). 6.3GB. MD5: 29b22e2961454d5413ddabcf34fc5622\n",
    "\n",
    "の3つをダウンロードします．\n",
    "\n",
    "* Test images (all tasks). 13GB. MD5: e1b8681fff3d63731c599df9b4b6fc02\n",
    "\n",
    "はtorchvisionでは使用できませんが，必要な場合はダウンロードしてください．\n",
    "\n",
    "**※ ダウンロードには1日以上かかると思われますので，ご注意ください．**\n",
    "\n",
    "ダウンロードが完了したら，`*.tar.gz`ファイルを展開します．\n",
    "\n",
    "展開した学習データのファイル（フォルダ）の中身を確認すると，`n01440764`や`n01443537`などの名前のフォルダが存在しており，そのフォルダの中に様々な画像データが格納されていることがわかります．\n",
    "学習用画像では，画像データがクラス毎にフォルダ分けされています．\n",
    "このフォルダの番号は，WordNet IDと呼ばれるもので，クラス識別のためのIDとなっています．\n",
    "\n",
    "#### 3. ImageNetクラスの呼び出し\n",
    "\n",
    "`torchvision.datasets.ImageNet`を用いて，ImageNetデータセットを使用できるよう呼び出します．\n",
    "\n",
    "`root`は先ほどダウンロード，展開したデータセットがあるディレクトリまでのパスを指定します．\n",
    "`split`は学習用 (`\"train\"`) または検証用 (`\"val\"`) を指定します．\n",
    "`transform`はデータを呼び出す際に行う前処理やaugmentation指定する部分です．\n",
    "`download`はデータセットをダウンロードするかどうかの引数ですが，`True`を指定した場合でもエラーメッセージを出力して終了しますので使用しないよう注意してください．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_train_data = torchvision.datasets.ImageNet(root='path/to/imagenet_root/', split=\"train\", transform=transforms.ToTensor())\n",
    "imagenet_val_data = torchvision.datasets.ImageNet(root='path/to/imagenet_root/', split=\"val\", transform=transforms.ToTensor())\n",
    "\n",
    "print(len(imagenet_train_data))\n",
    "print(len(imagenet_val_data))\n",
    "\n",
    "# DataLoaderの定義\n",
    "imagenet_train_loader = torch.utils.data.DataLoader(dataset=imagenet_train_data, batch_size=256, shuffle=True)\n",
    "imagenet_val_loader = torch.utils.data.DataLoader(dataset=imagenet_test_data, batch_size=256, shuffle=False)\n",
    "\n",
    "for image, label in imagenet_train_loader:\n",
    "    print(image.size())\n",
    "    print(label.size(), label)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8485f4de6932f238666d4709aa126f6b75be12ba2cdaa4516de10ecce3e09fd4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('anaconda3-5.3.1': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
