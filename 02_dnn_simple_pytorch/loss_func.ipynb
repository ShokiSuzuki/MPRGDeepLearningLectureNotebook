{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差関数の変更による学習効果\n",
    "\n",
    "---\n",
    "\n",
    "## 目的\n",
    "\n",
    "ネットワークの学習に使用する誤差関数（Loss関数）を変更した際に獲得される特徴表現（空間）を可視化し，その効果を確認する．\n",
    "\n",
    "## 準備\n",
    "\n",
    "### Google Colaboratoryの設定確認・変更\n",
    "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n",
    "**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モジュールのインポート\n",
    "はじめに必要なモジュールをインポートする．\n",
    "\n",
    "### GPUの確認\n",
    "GPUを使用した計算が可能かどうかを確認します．\n",
    "\n",
    "`Use CUDA: True`と表示されれば，GPUを使用した計算をPyTorchで行うことが可能です．\n",
    "Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モジュールのインポート\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# GPUの確認\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Use CUDA:', use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G418kZOgToXR"
   },
   "source": [
    "## ネットワークモデルの定義\n",
    "\n",
    "畳み込みニューラルネットワークを定義します．\n",
    "\n",
    "ここでは，畳み込み層２層，全結合層３層から構成されるネットワークとします．\n",
    "\n",
    "1層目の畳み込み層は入力チャンネル数が1，出力する特徴マップ数が16，畳み込むフィルタサイズが3x3です．\n",
    "2層目の畳み込み層は入力チャネル数が16．出力する特徴マップ数が32，畳み込むフィルタサイズは同じく3x3です．\n",
    "１つ目の全結合層は入力ユニット数は`7*7*32`とし，出力は1024としています．\n",
    "次の全結合層入力，出力共に1024，出力層は入力が1024，出力が10です．\n",
    "また，活性化関数として`self.act`にシグモイド関数を定義します．\n",
    "さらに，プーリング処理を行うための`self.pool`を定義します．\n",
    "ここでは，maxpoolingを使用します．\n",
    "これらの各層の構成を`__init__`関数で定義します．\n",
    "\n",
    "次に，`forward`関数では，定義した層を接続して処理するように記述します．\n",
    "`forward`関数の引数`x`は入力データです．\n",
    "それを`__init__`関数で定義した`conv1`に入力し，その出力を活性化関数である`self.act`に与えます．\n",
    "そして，その出力を`self.pool`に与えて，プーリング処理結果を`h`として出力します．\n",
    "2層目の畳み込み層でも同様の手順で処理を行います．\n",
    "\n",
    "畳み込みを適用した後の特徴マップを全結合層へと入力して，識別結果を出力します．\n",
    "まず．畳み込みによって得られた特徴マップの形状（チャンネルx縦x横）を1次元の配列へと変換します．\n",
    "ここで，`view()`を用いることで，`h`の配列を操作します．引数として，変換したい配列のサイズを入力します．\n",
    "まず一つ目の引数の`h.size()[0]`で，`h`の1次元目のサイズを取得し，変換後の配列の1次元目のサイズとして指定します．\n",
    "二つ目の引数の`-1`で任意のサイズを指定します．\n",
    "これにより，`h`を（バッチ数x任意の長さのデータ）の形状へ変換します．\n",
    "変換した`h`を全結合層および活性化関数へと順次入力することで，最終的にクラススコアを返します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FJhkBJnTuPd"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, in_channels=1, hidden_dim=2, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "    self.conv2 = nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "    self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "    self.conv4 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "    self.conv5 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "    self.conv6 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "\n",
    "    self.act = nn.PReLU()\n",
    "\n",
    "    self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "    self.fc1 = nn.Linear(128, hidden_dim)\n",
    "    self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "  \n",
    "  def forward(self, x):\n",
    "\n",
    "    h = self.act(self.conv1(x))\n",
    "    h = self.act(self.conv2(h))\n",
    "    h = F.max_pool2d(h, 2, 2)\n",
    "    h = self.act(self.conv3(h))\n",
    "    h = self.act(self.conv4(h))\n",
    "    h = F.max_pool2d(h, 2, 2)\n",
    "    h = self.act(self.conv5(h))\n",
    "    h = self.act(self.conv6(h))\n",
    "    h = F.max_pool2d(h, 2, 2)\n",
    "\n",
    "    h = self.gap(h).flatten(start_dim=1)\n",
    "    h_out = self.fc1(h)\n",
    "    out = self.fc2(h_out)\n",
    "    return out, h_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss関数の作成\n",
    "\n",
    "次に，Cross Entropy Loss以外のLoss関数を定義します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "  def __init__(self, num_classes=10, num_features=2, use_gpu=True):\n",
    "    super().__init__()\n",
    "    self.num_classes = num_classes\n",
    "    self.feat_dim = num_features\n",
    "    self.use_gpu = use_gpu\n",
    "\n",
    "    if self.use_gpu:\n",
    "      self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "    else:\n",
    "      self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
    "\n",
    "  def forward(self, x, labels):\n",
    "    batch_size = x.size(0)\n",
    "    distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "              torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "    distmat.addmm_(x, self.centers.t(), beta=1, alpha=-2)\n",
    "\n",
    "    classes = torch.arange(self.num_classes).long()\n",
    "    if self.use_gpu: classes = classes.cuda()\n",
    "    labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "    mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "    dist = distmat * mask.float()\n",
    "    loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
    "    return loss\n",
    "\n",
    "\n",
    "class PCLoss(nn.Module):\n",
    "  def __init__(self, num_features=2, num_classes=10, use_gpu=True):\n",
    "    super().__init__()\n",
    "    self.num_classes = num_classes\n",
    "    self.feat_dim = num_features\n",
    "    self.use_gpu = use_gpu\n",
    "\n",
    "    if self.use_gpu:\n",
    "      self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "    else:\n",
    "      self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
    "  \n",
    "  def forward(self, x, labels):\n",
    "    batch_size = x.size(0)\n",
    "    distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "              torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "    distmat.addmm_(x, self.centers.t(), beta=1, alpha=-2)\n",
    "\n",
    "    classes = torch.arange(self.num_classes).long()\n",
    "    if self.use_gpu: classes = classes.cuda()\n",
    "    labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "    mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "    inverse_mask = 1 - mask.float()\n",
    "    dist = (distmat * inverse_mask).clamp(min=1e-12, max=1e+12)\n",
    "\n",
    "    pos_c = mask.float().unsqueeze(2).repeat(1,1,self.feat_dim) * self.centers.unsqueeze(0).repeat(batch_size,1,1)\n",
    "    neg_c = inverse_mask.unsqueeze(2).repeat(1,1,self.feat_dim) * self.centers.unsqueeze(0).repeat(batch_size,1,1)\n",
    "    dist_c = torch.pow(pos_c - neg_c, 2).clamp(min=1e-12, max=1e+12)\n",
    "    loss = (torch.sum(dist) + torch.sum(dist_c)) / (batch_size * (self.num_classes - 1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・評価の関数の定義\n",
    "\n",
    "本ノートブックでは，用いる誤差関数を変更し，それぞれで獲得された特徴の分布を可視化します．\n",
    "そのため，複数回学習と評価を行う必要があります．\n",
    "そこで，ここではネットワークやデータセット，パラメータなどの必要な情報を引数として渡すことでネットワークの学習や評価を実行する関数を定義します．\n",
    "\n",
    "\n",
    "### 学習用関数の定義\n",
    "\n",
    "学習用の関数`training()`では，次のような引数を定義します．\n",
    "\n",
    "* `epoch`: 現在の学習回数（表示用）\n",
    "* `model`: 学習するネットワークモデル\n",
    "* `dataloader`: 学習に使用するデータのDataLoader\n",
    "* `xent`: CrossEntropyLossを計算するクラスインスタンス\n",
    "* `model_optimizer`: ネットワークモデルを学習するための最適化関数（optimizer）\n",
    "* `center`: Center Lossを計算するためのクラスインスタンス（Noneの場合はCenter Lossは計算されずに学習を行う）\n",
    "* `pc`: PC Lossを計算するためのクラスインスタンス（Noneの場合はPC Lossは計算されずに学習を行う）\n",
    "* `center_optimzer`: Center Lossの中心座標を更新するためのoptimizer（Noneの場合はCenter Lossは計算されずに学習を行う）\n",
    "* `pc_optimizer`: PC Lossの中心座標を更新するためのoptimizer（Noneの場合はPC Lossは計算されずに学習を行う）\n",
    "* `lambda_center`: Center Lossを計算する際のスケールパラメータ（重み）\n",
    "* `lambda_pc`: PC Lossを計算する際のスケールパラメータ（重み）\n",
    "\n",
    "このうち，Center LossとPC Lossを計算するための誤差関数とそのoptimizerについては，デフォルトの値を`None`としておき，\n",
    "入力された場合のみ，学習に使用するように定義を行います．\n",
    "\n",
    "\n",
    "### 評価用関数の定義\n",
    "\n",
    "評価用の関数`evaluation()`では，引数として`model`, `dataloader`, `xent`を用意します．\n",
    "各引数は，上の`training()`の場合と同様です．\n",
    "こちらでは，学習済みのネットワークモデルと評価に使用するデータのDataLoader, クロスエントロピー誤差を計算するクラスインスタンスを入力して，学習済みのネットワークの精度を確認します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, model, dataloader, xent, model_optimizer, center=None, pc=None, center_optimizer=None, pc_optimizer=None, lambda_center=1, lambda_pc=0.0001):\n",
    "  model.train()\n",
    "  total = 0\n",
    "  total_acc = 0\n",
    "  for index, (x, y) in enumerate(dataloader):\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "\n",
    "    logits, features = model(x)\n",
    "\n",
    "    # Cross Entropy Lossの計算\n",
    "    xent_loss = xent(logits, y)\n",
    "    # Center Lossの計算（引数にCenter LossとOptimizerが設定されていれば）\n",
    "    if center is not None and center_optimizer is not None:\n",
    "        center_loss = center(features, y)\n",
    "    else:\n",
    "        center_loss = 0\n",
    "    # PC Lossの計算（引数にPC LossとOptimizerが設定されていれば）\n",
    "    if pc is not None and pc_optimizer is not None:\n",
    "        pc_loss = pc(features, y)\n",
    "    else:\n",
    "        pc_loss = 0\n",
    "\n",
    "    loss = xent_loss + lambda_center * center_loss - lambda_pc * pc_loss\n",
    "\n",
    "    model_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "\n",
    "    if center is not None and center_optimizer is not None:\n",
    "        center_optimizer.zero_grad()\n",
    "        center_optimizer.step()\n",
    "\n",
    "    if pc is not None and pc_optimizer is not None:\n",
    "        pc_optimizer.zero_grad()\n",
    "        for param in pc.parameters():\n",
    "            param.grad.data *= (1/lambda_pc)\n",
    "        pc_optimizer.step()\n",
    "\n",
    "    if index % 100 == 0:\n",
    "      num_correct = torch.argmax(torch.softmax(logits, dim=1), dim=1).eq(y).sum().item()\n",
    "      total += x.size(0)\n",
    "      total_acc += num_correct\n",
    "      print('{} epoch [{}/{}] | '\n",
    "           'Loss: {:.3f} | xent: {:.3f} | center: {:.3f} | pc: {:.3f} | '\n",
    "           'Acc: {:.3f} (avg: {:.3f})'.format(epoch, index, len(dataloader), loss, xent_loss, center_loss, pc_loss, num_correct / x.size(0), total_acc / total))\n",
    "\n",
    "\n",
    "def evaluation(model, dataloader, xent):\n",
    "  model.eval()\n",
    "  total = 0\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "  for index, (x, y) in enumerate(dataloader):\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      logits, _ = model(x)\n",
    "      loss = xent(logits, y)\n",
    "\n",
    "    total += x.size(1)\n",
    "    num_correct = torch.argmax(torch.softmax(logits, dim=1), dim=1).eq(y).sum().item()\n",
    "    total_correct += num_correct\n",
    "    total_loss += loss.item()\n",
    "    # if index % 10 == 0:\n",
    "    #   print('Test [{}/{}] | Loss: {:.4f} (avg: {:.4f}) | Acc: {:.4f} (avg: {:.4f})'.format(index, len(dataloader.dataset), loss.item(), total_loss/total, 100*(num_correct/x.size(0)), 100*(total_correct/total)))\n",
    "  return total_loss/total, 100*(total_correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共通するパラメータの定義\n",
    "\n",
    "実際の学習を行う前に，用いる誤差関数にかかわらず，同じ設定のパラメータを事前に設定します．\n",
    "\n",
    "また，学習と評価に使用するデータ（DataLoader）を準備します．\n",
    "今回はMNISTデータセットを使用して実験を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "hidden_dim = 2\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "train_dataset = datasets.MNIST('./root', download=True, train=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=True, drop_last=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijVjOGVhb6vs"
   },
   "source": [
    "## 1. CrossEntropy誤差のみで学習した場合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "model_xent = CNN(in_channels=1, hidden_dim=hidden_dim, num_classes=10).cuda()\n",
    "xent = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "model_optimizer = optim.SGD(model_xent.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "# 学習\n",
    "for epoch in range(epochs):\n",
    "  training(epoch, model_xent, train_dataloader, xent, model_optimizer)\n",
    "\n",
    "# 評価\n",
    "test_loss, test_acc = evaluation(model_xent, test_dataloader, xent)\n",
    "print('Evaluation result: Loss {:.4f}, Acc {:.4f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴の分布の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SyfYfpXvb62g"
   },
   "outputs": [],
   "source": [
    "coords = np.zeros((len(test_dataloader.dataset), 2))\n",
    "labels = np.zeros((len(test_dataloader.dataset)))\n",
    "cnt = 0\n",
    "\n",
    "model_xent.eval()\n",
    "for (x, y) in test_dataloader:\n",
    "  x = x.cuda()\n",
    "  batch = x.size(0)\n",
    "  _, features = model_xent(x)\n",
    "  coords[cnt:cnt+batch] = features.squeeze().data.cpu().numpy()\n",
    "  labels[cnt:cnt+batch] = y.data.numpy()\n",
    "  cnt += batch\n",
    "\n",
    "mpl_colorlist = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "colorlist = [mpl_colorlist[int(idx)] for idx in labels]\n",
    "xcoords, ycoords = coords[:, 0], coords[:, 1]\n",
    "xmax, xmin = xcoords.max(), xcoords.min()\n",
    "ymax, ymin = ycoords.max(), ycoords.min()\n",
    "xcoords = (xcoords - xmin) / (xmax - xmin)\n",
    "ycoords = (ycoords - ymin) / (ymax - ymin)\n",
    "\n",
    "plt.scatter(xcoords, ycoords, color=colorlist, s=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross Entropy Loss + Center Lossを用いて学習した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "center_lr = 0.5\n",
    "lambda_center = 1\n",
    "\n",
    "model_xent_center = CNN(in_channels=1, hidden_dim=hidden_dim, num_classes=10).cuda()\n",
    "xent = nn.CrossEntropyLoss().cuda()\n",
    "center = CenterLoss(num_classes=num_classes, num_features=hidden_dim, use_gpu=True)\n",
    "\n",
    "model_optimizer = optim.SGD(model_xent_center.parameters(), lr=lr, momentum=momentum)\n",
    "center_optimizer = optim.SGD(center.parameters(), lr=center_lr)\n",
    "\n",
    "# 学習\n",
    "for epoch in range(epochs):\n",
    "  training(epoch, model_xent_center, train_dataloader, xent, model_optimizer,\n",
    "           center=center, center_optimizer=center_optimizer, lambda_center=lambda_center)\n",
    "\n",
    "# 評価\n",
    "test_loss, test_acc = evaluation(model_xent_center, test_dataloader, xent)\n",
    "print('Evaluation result: Loss {:.4f}, Acc {:.4f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可視化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.zeros((len(test_dataloader.dataset), 2))\n",
    "labels = np.zeros((len(test_dataloader.dataset)))\n",
    "cnt = 0\n",
    "\n",
    "model_xent_center.eval()\n",
    "for (x, y) in test_dataloader:\n",
    "  x = x.cuda()\n",
    "  batch = x.size(0)\n",
    "  _, features = model_xent_center(x)\n",
    "  coords[cnt:cnt+batch] = features.squeeze().data.cpu().numpy()\n",
    "  labels[cnt:cnt+batch] = y.data.numpy()\n",
    "  cnt += batch\n",
    "\n",
    "mpl_colorlist = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "colorlist = [mpl_colorlist[int(idx)] for idx in labels]\n",
    "xcoords, ycoords = coords[:, 0], coords[:, 1]\n",
    "xmax, xmin = xcoords.max(), xcoords.min()\n",
    "ymax, ymin = ycoords.max(), ycoords.min()\n",
    "xcoords = (xcoords - xmin) / (xmax - xmin)\n",
    "ycoords = (ycoords - ymin) / (ymax - ymin)\n",
    "\n",
    "plt.scatter(xcoords, ycoords, color=colorlist, s=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss + Center Loss + PC Lossを用いて学習した場合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "# Center Lossのパラメータ\n",
    "center_lr = 0.5\n",
    "lambda_center = 1\n",
    "# PC Lossのパラメータ\n",
    "pc_lr = 0.01\n",
    "lambda_pc = 0.0001\n",
    "\n",
    "model_xent_center_pc = CNN(in_channels=1, hidden_dim=hidden_dim, num_classes=10).cuda()\n",
    "xent = nn.CrossEntropyLoss().cuda()\n",
    "center = CenterLoss(num_classes=num_classes, num_features=hidden_dim, use_gpu=True)\n",
    "pc = PCLoss(num_classes=num_classes, num_features=hidden_dim, use_gpu=True)\n",
    "\n",
    "model_optimizer = optim.SGD(model_xent_center_pc.parameters(), lr=lr, momentum=momentum)\n",
    "center_optimizer = optim.SGD(center.parameters(), lr=center_lr)\n",
    "pc_optimizer = optim.SGD(pc.parameters(), lr=pc_lr)\n",
    "\n",
    "# 学習\n",
    "for epoch in range(epochs):\n",
    "  training(epoch, model_xent_center_pc, train_dataloader, xent, model_optimizer,\n",
    "           center=center, pc=pc, center_optimizer=center_optimizer, pc_optimizer=pc_optimizer,\n",
    "           lambda_center=lambda_center, lambda_pc=lambda_pc)\n",
    "\n",
    "# 評価\n",
    "test_loss, test_acc = evaluation(model_xent_center_pc, test_dataloader, xent)\n",
    "print('Evaluation result: Loss {:.4f}, Acc {:.4f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.zeros((len(test_dataloader.dataset), 2))\n",
    "labels = np.zeros((len(test_dataloader.dataset)))\n",
    "cnt = 0\n",
    "\n",
    "model_xent_center_pc.eval()\n",
    "for (x, y) in test_dataloader:\n",
    "  x = x.cuda()\n",
    "  batch = x.size(0)\n",
    "  _, features = model_xent_center_pc(x)\n",
    "  coords[cnt:cnt+batch] = features.squeeze().data.cpu().numpy()\n",
    "  labels[cnt:cnt+batch] = y.data.numpy()\n",
    "  cnt += batch\n",
    "\n",
    "mpl_colorlist = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "colorlist = [mpl_colorlist[int(idx)] for idx in labels]\n",
    "xcoords, ycoords = coords[:, 0], coords[:, 1]\n",
    "xmax, xmin = xcoords.max(), xcoords.min()\n",
    "ymax, ymin = ycoords.max(), ycoords.min()\n",
    "xcoords = (xcoords - xmin) / (xmax - xmin)\n",
    "ycoords = (ycoords - ymin) / (ymax - ymin)\n",
    "\n",
    "plt.scatter(xcoords, ycoords, color=colorlist, s=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題\n",
    "\n",
    "1. Lossのパラメータを変更して，学習後の特徴の分布がどのように変化するかを確認しましょう．\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
