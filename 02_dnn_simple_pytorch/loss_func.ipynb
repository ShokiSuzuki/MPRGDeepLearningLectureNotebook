{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差関数の変更による学習効果\n",
    "\n",
    "---\n",
    "\n",
    "## 目的\n",
    "\n",
    "ネットワークの学習に使用する誤差関数（Loss関数）を変更した際に獲得される特徴表現（空間）を可視化し，その効果を確認する．\n",
    "\n",
    "## 準備\n",
    "\n",
    "### Google Colaboratoryの設定確認・変更\n",
    "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n",
    "**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モジュールのインポート\n",
    "はじめに必要なモジュールをインポートする．\n",
    "\n",
    "### GPUの確認\n",
    "GPUを使用した計算が可能かどうかを確認します．\n",
    "\n",
    "`Use CUDA: True`と表示されれば，GPUを使用した計算をPyTorchで行うことが可能です．\n",
    "Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モジュールのインポート\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# GPUの確認\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Use CUDA:', use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G418kZOgToXR"
   },
   "source": [
    "## ネットワークモデルの定義\n",
    "\n",
    "畳み込みニューラルネットワークを定義します．\n",
    "\n",
    "ここでは，畳み込み層3層，全結合層2層から構成されるネットワークとします．\n",
    "\n",
    "このとき，入力される画像のチャンネル数を`in_channels`，畳み込み直後の全結合槽の出力ユニット数を`hidden_dim`，出力層のユニット数（クラス数）を`num_classes`として指定します．\n",
    "\n",
    "`forward`関数では，後の実験でネットワークから出力されるクラススコアだけでなく，1層目の全結合層の出力を出力するように定義します．\n",
    "ここでは，`self.fc1`の出力結果を`h_out`として別の変数に格納しておき，最後の`return`でクラススコアの`out`と一緒に返すように定義します．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FJhkBJnTuPd"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_dim=2, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(128, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.act(self.conv1(x))\n",
    "        h = F.max_pool2d(h, 2, 2)\n",
    "        h = self.act(self.conv2(h))\n",
    "        h = F.max_pool2d(h, 2, 2)\n",
    "        h = self.act(self.conv3(h))\n",
    "        h = F.max_pool2d(h, 2, 2)\n",
    "\n",
    "        h = self.gap(h).flatten(start_dim=1)\n",
    "        h_out = self.fc1(h)\n",
    "        out = self.fc2(h_out)\n",
    "        return out, h_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss関数の作成\n",
    "\n",
    "次に，Cross Entropy Loss以外のLoss関数を定義します．\n",
    "\n",
    "### a. Center Loss\n",
    "\n",
    "Center Lossは **作成途中**．\n",
    "\n",
    "\n",
    "### b. PC Loss\n",
    "\n",
    "PC Lossは **作成途中**．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    def __init__(self, num_classes=10, num_features=2, use_gpu=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = num_features\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        if self.use_gpu:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "        else:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(x, self.centers.t(), beta=1, alpha=-2)\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long()\n",
    "        if self.use_gpu: classes = classes.cuda()\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = distmat * mask.float()\n",
    "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
    "        return loss\n",
    "\n",
    "\n",
    "class PCLoss(nn.Module):\n",
    "    def __init__(self, num_features=2, num_classes=10, use_gpu=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = num_features\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        if self.use_gpu:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "        else:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(x, self.centers.t(), beta=1, alpha=-2)\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long()\n",
    "        if self.use_gpu: classes = classes.cuda()\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        inverse_mask = 1 - mask.float()\n",
    "        dist = (distmat * inverse_mask).clamp(min=1e-12, max=1e+12)\n",
    "\n",
    "        pos_c = mask.float().unsqueeze(2).repeat(1,1,self.feat_dim) * self.centers.unsqueeze(0).repeat(batch_size,1,1)\n",
    "        neg_c = inverse_mask.unsqueeze(2).repeat(1,1,self.feat_dim) * self.centers.unsqueeze(0).repeat(batch_size,1,1)\n",
    "        dist_c = torch.pow(pos_c - neg_c, 2).clamp(min=1e-12, max=1e+12)\n",
    "        loss = (torch.sum(dist) + torch.sum(dist_c)) / (batch_size * (self.num_classes - 1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・評価の関数の定義\n",
    "\n",
    "本ノートブックでは，学習する誤差関数を変更し，それぞれで獲得された特徴の分布を可視化しその効果を確認します．\n",
    "そのため，複数回の学習と評価を行う必要があります．\n",
    "そこで，ネットワークやデータセット，パラメータなどの必要な情報を引数として渡すことでネットワークの学習や評価を実行する関数を定義します．\n",
    "\n",
    "このような学習や評価の関数は，一度汎用的に作成しておくことで，様々なネットワークやパラメータの学習を行うことが可能となります．\n",
    "実際の研究や実験を行う際には，様々なパラメータやネットワークを用いて実験を行いますが，\n",
    "それぞれの設定の学習・評価プログラムを個別に作成するよりもコンパクトにプログラムを作成することが可能なため，頻繁に用いられる（見かける）書き方です．\n",
    "\n",
    "### 学習用関数の定義\n",
    "\n",
    "今回の学習用の関数`training()`では，次のような引数を定義します．\n",
    "\n",
    "* `epoch`: 現在の学習回数（表示用）\n",
    "* `model`: 学習するネットワークモデル\n",
    "* `dataloader`: 学習に使用するデータのDataLoader\n",
    "* `xent`: CrossEntropyLossを計算するクラスインスタンス\n",
    "* `model_optimizer`: ネットワークモデルを学習するための最適化関数（optimizer）\n",
    "* `center`: Center Lossを計算するためのクラスインスタンス（Noneの場合はCenter Lossは計算されずに学習を行う）\n",
    "* `pc`: PC Lossを計算するためのクラスインスタンス（Noneの場合はPC Lossは計算されずに学習を行う）\n",
    "* `center_optimzer`: Center Lossの中心座標を更新するためのoptimizer（Noneの場合はCenter Lossは計算されずに学習を行う）\n",
    "* `pc_optimizer`: PC Lossの中心座標を更新するためのoptimizer（Noneの場合はPC Lossは計算されずに学習を行う）\n",
    "* `lambda_center`: Center Lossを計算する際のスケールパラメータ（重み）\n",
    "* `lambda_pc`: PC Lossを計算する際のスケールパラメータ（重み）\n",
    "\n",
    "このうち，Center LossとPC Lossを計算するための誤差関数とそのoptimizerについては，デフォルトの値を`None`としておき，入力された場合のみ，学習に使用するように定義を行います．\n",
    "このようにすることで，一つの`training()`関数で，複数の誤差関数の組み合わせの学習を実行することが可能となります．\n",
    "\n",
    "\n",
    "### 評価用関数の定義\n",
    "\n",
    "評価用の関数`evaluation()`では，引数として`model`, `dataloader`, `xent`を用意します．\n",
    "各引数は，上の`training()`の場合と同様です．\n",
    "こちらでは，学習済みのネットワークモデルと評価に使用するデータのDataLoader, クロスエントロピー誤差を計算するクラスインスタンスを入力して，学習済みのネットワークの精度を確認します．\n",
    "評価の関数はパラメータや誤差関数の組み合わせに関係なく，テストデータに対する精度を評価するため，共通して用いることが可能となります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, model, dataloader, xent, model_optimizer, center=None, pc=None, center_optimizer=None, pc_optimizer=None, lambda_center=1, lambda_pc=0.0001):\n",
    "  model.train()\n",
    "  total = 0\n",
    "  total_acc = 0\n",
    "  for index, (x, y) in enumerate(dataloader):\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "\n",
    "    logits, features = model(x)\n",
    "\n",
    "    # Cross Entropy Lossの計算\n",
    "    xent_loss = xent(logits, y)\n",
    "    # Center Lossの計算（引数にCenter LossとOptimizerが設定されていれば）\n",
    "    if center is not None and center_optimizer is not None:\n",
    "        center_loss = center(features, y)\n",
    "    else:\n",
    "        center_loss = 0\n",
    "    # PC Lossの計算（引数にPC LossとOptimizerが設定されていれば）\n",
    "    if pc is not None and pc_optimizer is not None:\n",
    "        pc_loss = pc(features, y)\n",
    "    else:\n",
    "        pc_loss = 0\n",
    "\n",
    "    loss = xent_loss + lambda_center * center_loss - lambda_pc * pc_loss\n",
    "\n",
    "    model_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "\n",
    "    if center is not None and center_optimizer is not None:\n",
    "        center_optimizer.zero_grad()\n",
    "        center_optimizer.step()\n",
    "\n",
    "    if pc is not None and pc_optimizer is not None:\n",
    "        pc_optimizer.zero_grad()\n",
    "        for param in pc.parameters():\n",
    "            param.grad.data *= (1/lambda_pc)\n",
    "        pc_optimizer.step()\n",
    "\n",
    "    if index % 100 == 0:\n",
    "      num_correct = torch.argmax(torch.softmax(logits, dim=1), dim=1).eq(y).sum().item()\n",
    "      total += x.size(0)\n",
    "      total_acc += num_correct\n",
    "      print('{} epoch [{}/{}] | '\n",
    "           'Loss: {:.3f} | xent: {:.3f} | center: {:.3f} | pc: {:.3f} | '\n",
    "           'Acc: {:.3f} (avg: {:.3f})'.format(epoch, index, len(dataloader), loss, xent_loss, center_loss, pc_loss, num_correct / x.size(0), total_acc / total))\n",
    "\n",
    "\n",
    "def evaluation(model, dataloader, xent):\n",
    "  model.eval()\n",
    "  total = 0\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "  for index, (x, y) in enumerate(dataloader):\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      logits, _ = model(x)\n",
    "      loss = xent(logits, y)\n",
    "\n",
    "    total += x.size(1)\n",
    "    num_correct = torch.argmax(torch.softmax(logits, dim=1), dim=1).eq(y).sum().item()\n",
    "    total_correct += num_correct\n",
    "    total_loss += loss.item()\n",
    "    # if index % 10 == 0:\n",
    "    #   print('Test [{}/{}] | Loss: {:.4f} (avg: {:.4f}) | Acc: {:.4f} (avg: {:.4f})'.format(index, len(dataloader.dataset), loss.item(), total_loss/total, 100*(num_correct/x.size(0)), 100*(total_correct/total)))\n",
    "  return total_loss/total, 100*(total_correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共通するパラメータの定義\n",
    "\n",
    "実際の学習を行う前に，用いる誤差関数にかかわらない同じ設定のパラメータを事前に設定します．\n",
    "このとき，ネットワークの1つ目の全結合層の出力ユニット数`hidden_dim`は`2`に設定することに注意してください．\n",
    "この理由と使用方法については，「[可視化](#visualize)」の部分で説明を行います．\n",
    "\n",
    "また，学習と評価に使用するデータ（DataLoader）を準備します．\n",
    "今回はMNISTデータセットを使用して実験を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "hidden_dim = 2\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "train_dataset = datasets.MNIST('./root', download=True, train=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "kwargs = {\"num_workers\": 2, \"pin_memory\": False}\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, **kwargs)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=True, drop_last=False, batch_size=1, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijVjOGVhb6vs"
   },
   "source": [
    "## 1. CrossEntropy誤差のみで学習した場合\n",
    "\n",
    "\n",
    "1つ目に，通常の分類タスクの学習で用いられるCross Entropy誤差のみを使用してネットワークの学習を行います．\n",
    "\n",
    "ここでは，`model_xent`という名前のネットワークモデルを作成し，学習を行います．\n",
    "\n",
    "そして，先ほど上で定義した`training()`と`evaluation()`を用いて，学習と評価を行います．\n",
    "この時，`training()`の引数として，center lossおよびPC lossに関する引数は入力しないものとします．\n",
    "このようにすることで，center loss, PC lossに関する引数がNoneとして実行されるため，Cross Entropy誤差のみを用いた学習が可能となります．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "model_xent = CNN(in_channels=1, hidden_dim=hidden_dim, num_classes=10).cuda()\n",
    "xent = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "model_optimizer = optim.SGD(model_xent.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "# 学習\n",
    "for epoch in range(epochs):\n",
    "    training(epoch, model_xent, train_dataloader, xent, model_optimizer)\n",
    "\n",
    "# 評価\n",
    "test_loss, test_acc = evaluation(model_xent, test_dataloader, xent)\n",
    "print('Evaluation result: Loss {:.4f}, Acc {:.4f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualize'></a>\n",
    "\n",
    "### 特徴の分布の可視化\n",
    "\n",
    "Cross Entropy誤差のみで学習したネットワークで獲得された特徴量の分布を可視化して確認します．\n",
    "\n",
    "まず，各サンプルの特徴量とクラスラベルを保存する配列`coords`と`labels`を定義します．\n",
    "この時，`coords`は`[サンプル数, 特徴次元数]`となる２次元配列，`labels`は`[サンプル数]`となる1次元配列を定義します．\n",
    "\n",
    "また，処理したデータの数をカウントする`cnt`を0で初期化して準備します．\n",
    "\n",
    "次に学習したネットワークモデルを用いて，テストデータを認識します．\n",
    "この時，モデルからクラススコアと1層目の全結合層の特徴量が獲得されますが，\n",
    "1層目の全結合層の特徴量を`features`に格納し，先ほど用意した配列`coords`に保存します．\n",
    "同時に，テストデータの正解クラスラベルを`labels`に格納します．\n",
    "\n",
    "全てのテストデータの特徴量と正解ラベルの格納が完了したら，\n",
    "獲得した特徴量を2次元の散布図でプロットを行います．\n",
    "この時．各サンプルのクラスラベル（0 ~ 9）に応じて，プロットの色を変えて表示することで，各クラスのデータがどのあたりに分布しているかを確認することが可能となります．\n",
    "\n",
    "**考察（追加予定）**\n",
    "\n",
    "#### Note: 1層目の全結合層のユニット数を2に設定したねらい\n",
    "\n",
    "上のパラメータ設定で`hidden_dim = 2`と定義したのは，2次元の特徴量にすることで獲得した特徴の分布をプロットし，用意に可視化することが可能なためです．\n",
    "より高次元な特徴量（23, 64, 128など）では，単純に特徴の分布を可視化することが難しく，次元削減などの処理を行ったのちに可視化をする必要があります．\n",
    "今回は，1つ目の全結合層のユニット数を2と設定することで，獲得した特徴量を2次元プロットするように設定しています．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SyfYfpXvb62g"
   },
   "outputs": [],
   "source": [
    "coords = np.zeros((len(test_dataloader.dataset), 2))\n",
    "labels = np.zeros((len(test_dataloader.dataset)))\n",
    "cnt = 0\n",
    "\n",
    "model_xent.eval()\n",
    "for (x, y) in test_dataloader:\n",
    "    x = x.cuda()\n",
    "    batch = x.size(0)\n",
    "    _, features = model_xent(x)\n",
    "    coords[cnt:cnt+batch] = features.squeeze().data.cpu().numpy()\n",
    "    labels[cnt:cnt+batch] = y.data.numpy()\n",
    "    cnt += batch\n",
    "\n",
    "mpl_colorlist = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "colorlist = [mpl_colorlist[int(idx)] for idx in labels]\n",
    "xcoords, ycoords = coords[:, 0], coords[:, 1]\n",
    "xmax, xmin = xcoords.max(), xcoords.min()\n",
    "ymax, ymin = ycoords.max(), ycoords.min()\n",
    "xcoords = (xcoords - xmin) / (xmax - xmin)\n",
    "ycoords = (ycoords - ymin) / (ymax - ymin)\n",
    "\n",
    "plt.scatter(xcoords, ycoords, color=colorlist, s=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross Entropy Loss + Center Lossを用いて学習した場合\n",
    "\n",
    "次にCross Entropy誤差に加えてCenter Lossを用いてネットワークの学習を行います．\n",
    "\n",
    "ここでは，上のCross Entropy誤差のみで学習したネットワークとは別のネットワークを用意するため．`model_xent_center`という名前でネットワークを作成します．\n",
    "\n",
    "また，学習に用いる誤差関数として，Center Loss (`center`) を定義します．\n",
    "さらに，ネットワークを学習するためのoptimzer (`model_optimizer`) に加えて，Center Lossの中心座標を更新するための，optimizer (`center_optimzer`) を定義します．\n",
    "\n",
    "これらを`training()`の引数として追加することで，Cross Entropy誤差とCenter Lossを組み合わせてネットワークの学習を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "# Center Lossのパラメータ\n",
    "center_lr = 0.5\n",
    "lambda_center = 1\n",
    "\n",
    "model_xent_center = CNN(in_channels=1, hidden_dim=hidden_dim, num_classes=10).cuda()\n",
    "xent = nn.CrossEntropyLoss().cuda()\n",
    "center = CenterLoss(num_classes=num_classes, num_features=hidden_dim, use_gpu=True)\n",
    "\n",
    "model_optimizer = optim.SGD(model_xent_center.parameters(), lr=lr, momentum=momentum)\n",
    "center_optimizer = optim.SGD(center.parameters(), lr=center_lr)\n",
    "\n",
    "# 学習\n",
    "for epoch in range(epochs):\n",
    "    training(epoch, model_xent_center, train_dataloader, xent, model_optimizer,\n",
    "             center=center, center_optimizer=center_optimizer, lambda_center=lambda_center)\n",
    "\n",
    "# 評価\n",
    "test_loss, test_acc = evaluation(model_xent_center, test_dataloader, xent)\n",
    "print('Evaluation result: Loss {:.4f}, Acc {:.4f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴の分布の可視化\n",
    "\n",
    "Cross Entropy誤差とCenter Lossで学習したネットワークで獲得された特徴量の分布を可視化して確認します．\n",
    "可視化の方法については，上記で説明していますので割愛します．\n",
    "\n",
    "**考察（追加予定）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.zeros((len(test_dataloader.dataset), 2))\n",
    "labels = np.zeros((len(test_dataloader.dataset)))\n",
    "cnt = 0\n",
    "\n",
    "model_xent_center.eval()\n",
    "for (x, y) in test_dataloader:\n",
    "    x = x.cuda()\n",
    "    batch = x.size(0)\n",
    "    _, features = model_xent_center(x)\n",
    "    coords[cnt:cnt+batch] = features.squeeze().data.cpu().numpy()\n",
    "    labels[cnt:cnt+batch] = y.data.numpy()\n",
    "    cnt += batch\n",
    "\n",
    "mpl_colorlist = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "colorlist = [mpl_colorlist[int(idx)] for idx in labels]\n",
    "xcoords, ycoords = coords[:, 0], coords[:, 1]\n",
    "xmax, xmin = xcoords.max(), xcoords.min()\n",
    "ymax, ymin = ycoords.max(), ycoords.min()\n",
    "xcoords = (xcoords - xmin) / (xmax - xmin)\n",
    "ycoords = (ycoords - ymin) / (ymax - ymin)\n",
    "\n",
    "plt.scatter(xcoords, ycoords, color=colorlist, s=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross Entropy Loss + Center Loss + PC Lossを用いて学習した場合\n",
    "\n",
    "最後に，Cross Entropy誤差，Center Lossに加えてPC Lossを用いてネットワークの学習を行います．\n",
    "\n",
    "ここでは`model_xent_center_pc`という名前でネットワークを作成します．\n",
    "\n",
    "また，学習に用いる誤差関数として，新たにPC Loss (`pc`) とPC Lossの中心座標を更新するための，optimizer (`pc_optimizer`) を定義します．\n",
    "\n",
    "これらを`training()`の引数として追加することで，Cross Entropy誤差とCenter Loss，PC Lossを組み合わせてネットワークの学習を行います．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "# Center Lossのパラメータ\n",
    "center_lr = 0.5\n",
    "lambda_center = 1\n",
    "# PC Lossのパラメータ\n",
    "pc_lr = 0.01\n",
    "lambda_pc = 0.0001\n",
    "\n",
    "model_xent_center_pc = CNN(in_channels=1, hidden_dim=hidden_dim, num_classes=10).cuda()\n",
    "xent = nn.CrossEntropyLoss().cuda()\n",
    "center = CenterLoss(num_classes=num_classes, num_features=hidden_dim, use_gpu=True)\n",
    "pc = PCLoss(num_classes=num_classes, num_features=hidden_dim, use_gpu=True)\n",
    "\n",
    "model_optimizer = optim.SGD(model_xent_center_pc.parameters(), lr=lr, momentum=momentum)\n",
    "center_optimizer = optim.SGD(center.parameters(), lr=center_lr)\n",
    "pc_optimizer = optim.SGD(pc.parameters(), lr=pc_lr)\n",
    "\n",
    "# 学習\n",
    "for epoch in range(epochs):\n",
    "    training(epoch, model_xent_center_pc, train_dataloader, xent, model_optimizer,\n",
    "             center=center, pc=pc, center_optimizer=center_optimizer, pc_optimizer=pc_optimizer,\n",
    "             lambda_center=lambda_center, lambda_pc=lambda_pc)\n",
    "\n",
    "# 評価\n",
    "test_loss, test_acc = evaluation(model_xent_center_pc, test_dataloader, xent)\n",
    "print('Evaluation result: Loss {:.4f}, Acc {:.4f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴の分布の可視化\n",
    "\n",
    "Cross Entropy誤差とCenter Loss，PC Lossで学習したネットワークで獲得された特徴量の分布を可視化して確認します．\n",
    "可視化の方法については，上記で説明していますので割愛します．\n",
    "\n",
    "**考察（追加予定）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.zeros((len(test_dataloader.dataset), 2))\n",
    "labels = np.zeros((len(test_dataloader.dataset)))\n",
    "cnt = 0\n",
    "\n",
    "model_xent_center_pc.eval()\n",
    "for (x, y) in test_dataloader:\n",
    "    x = x.cuda()\n",
    "    batch = x.size(0)\n",
    "    _, features = model_xent_center_pc(x)\n",
    "    coords[cnt:cnt+batch] = features.squeeze().data.cpu().numpy()\n",
    "    labels[cnt:cnt+batch] = y.data.numpy()\n",
    "    cnt += batch\n",
    "\n",
    "mpl_colorlist = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "colorlist = [mpl_colorlist[int(idx)] for idx in labels]\n",
    "xcoords, ycoords = coords[:, 0], coords[:, 1]\n",
    "xmax, xmin = xcoords.max(), xcoords.min()\n",
    "ymax, ymin = ycoords.max(), ycoords.min()\n",
    "xcoords = (xcoords - xmin) / (xmax - xmin)\n",
    "ycoords = (ycoords - ymin) / (ymax - ymin)\n",
    "\n",
    "plt.scatter(xcoords, ycoords, color=colorlist, s=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題\n",
    "\n",
    "1. Lossや最適化手法のパラメータなどを変更して，学習後の特徴の分布がどのように変化するかを確認しましょう．\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
