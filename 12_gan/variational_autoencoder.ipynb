{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/12_gan/variational_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJFbw-pAbOiE"
      },
      "source": [
        "# Variational autoencoder (VAE)\n",
        "\n",
        "---\n",
        "## 目的\n",
        "Pytorchを用いてVariational autoencoder (VAE)を構築し，画像の再構成を行う．\n",
        "また，潜在空間を可視化することで，VAEで獲得した表現を理解する．\n",
        "\n",
        "※ VAEの理論について，本ノートブックの下部に記載しています．ご興味のある方はご確認ください．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z832d5MXUMl"
      },
      "source": [
        "## モジュールのインポート\n",
        "\n",
        "はじめに必要となるモジュールをインポートします．\n",
        "\n",
        "### GPUの確認\n",
        "GPUを使用した計算が可能かどうかを確認します．\n",
        "\n",
        "`GPU availability: True`と表示されれば，GPUを使用した計算を行うことが可能です．\n",
        "Falseとなっている場合は，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8rszdnIbOiF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from time import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# GPUの確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNGKB-mjbOiI"
      },
      "source": [
        "## ネットワークの構築\n",
        "VAEのネットワークを構築します．VAEは画像データを入力して特徴を抽出する**Encoder**と，任意のサイズのベクトルを入力して画像データを復元する**Decoder**で構築されています．\n",
        "\n",
        "Decoderへ入力する潜在変数は，通常のautoencoderであればEncoderからの出力値を利用します．\n",
        "一方で，VAEではEncodeした特徴量が任意の確率分布に従うように学習をします．\n",
        "最も愚直な方法だとEncoderで抽出した特徴量を正規分布の平均$\\mu$及び分散$\\sigma$として扱うことで，Encoderの出力が正規分布に従うと考えられます．\n",
        "Encoder，Decoderを個別に学習するのであれば，この方法でも問題ないです．\n",
        "しかしながら，VAEはEncoderとDecoderを一貫して学習するため，このままだと中間層での微分が不可能です．\n",
        "この問題点は，ニューラルネットワークを学習する際には非常に深刻な問題になります．\n",
        "なぜなら，ニューラルネットワークは計算した誤差を逆伝播してネットワークのパラメータの更新量を決定することが必要不可欠であるため，ネットワーク全体を通して計算グラフがつながっている必要があるからです．\n",
        "\n",
        "### Reparaterization trick\n",
        "そこで，EncoderとDecoderの間に**Reparameterization trick**と呼ばれる処理を用いることで解決します．これは，Encoderが出力する特徴マップが正規分布に従うように学習するテクニックです．\\\n",
        "Reparameterization trickでは以下に示す式によって潜在変数$\\bf{v}\\in \\mathbb{R}^{N}$をサンプリングします．\n",
        "$$\n",
        "\\mathbf{v} = \\mu + \\exp\\left(\\frac{1}{2}\\log\\sigma\\right)\\odot\\epsilon\n",
        "$$\n",
        "$$\n",
        "s.t. \\epsilon\\leftarrow N(0, 1)\n",
        "$$\n",
        "\n",
        "### VAEのネットワーク構造\n",
        "VAEの全体像のイメージを以下の図に示します．\n",
        "Encoder及びDecoderは，全結合層とReLUを用いて構築します．平均$\\mu$，分散$\\sigma$はそれぞれ別の全結合層によって獲得します．ここで注意すべき点は，$\\sigma$がReparametarization trickの計算過程で対数をとるため，$\\sigma>0$でないと計算不可になることです．これを解決するために，厳密には全結合層が出力したベクトルを$\\log\\sigma$であると仮定して計算をしています．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixvrjeNL4h3W"
      },
      "source": [
        "<img src=\"https://dl.dropboxusercontent.com/s/efatumuniv5zdeq/vae.png\" width=50%>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k0lLZAAbOiI"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=10):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "                nn.Linear(28*28, 256),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(256, 100),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        self.l_mu = nn.Linear(100, latent_dim)\n",
        "        self.l_var = nn.Linear(100, latent_dim)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "                nn.Linear(latent_dim, 100),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(100, 256),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(256, 28*28)\n",
        "            )\n",
        "    \n",
        "    def reparameterization_trick(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        eps = torch.randn_like(std)\n",
        "        latent = eps.mul(std).add_(mu)\n",
        "        return latent\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.l_mu(h)\n",
        "        logvar = self.l_var(h)\n",
        "        latent = self.reparameterization_trick(mu, logvar)\n",
        "        out = self.decoder(latent)\n",
        "        return out, mu, logvar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZSistJSbOiK"
      },
      "source": [
        "## データセット，最適化関数などの設定\n",
        "データセットはMNISTを用いて学習をします．\n",
        "最適化関数にはAdam Optimizerを使用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSVhVBYQbOiK"
      },
      "outputs": [],
      "source": [
        "# データセットの設定\n",
        "transform_train = transforms.Compose([transforms.ToTensor()])\n",
        "mnist_data = datasets.MNIST(root='./data', train=True, transform=transform_train, download=True)\n",
        "train_loader = DataLoader(dataset=mnist_data, batch_size=100, shuffle=True)\n",
        "\n",
        "# ネットワークモデル・最適化手法の設定\n",
        "model = VAE(latent_dim=2)\n",
        "if use_cuda:\n",
        "    model = model.cuda()    \n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZDzexeqbOiM"
      },
      "source": [
        "## 誤差関数の設定\n",
        "VAEの誤差関数を定義します．\n",
        "\n",
        "\n",
        "VAEの誤差関数は以下のようになります（詳細はノートブック下部の「VAEの理論」参照）．\n",
        "\\begin{eqnarray}\n",
        "\\mathcal{L}(x,z) &=& \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}[q(z|x)\\|p(z)]\n",
        "\\end{eqnarray}\n",
        "ここで，第1項は再構成誤差 (Reconstruction error)，第2項は正則化項 (Regularization error)と呼ばれます．\n",
        "\n",
        "VAEの論文で再構成誤差は，負の対数尤度をとったベルヌーイ分布を仮定していますが，ここではBinary cross entropy (BCE) とKLダイバージェンスで定義します．\n",
        "BCE lossは，Nをデータ数，xをネットワークの出力（ここでは出力画像），yを教師信号（ここでは入力画像）とすると，以下の式で表されます．\n",
        "$$\n",
        "\\mathcal{L}_{bce} = -\\sum_{i=1}^{N}y_{i}\\log(x_{i}) + (1-y_{i})\\log(1-x_{i})\\\\\n",
        "$$\n",
        "負の対数をとったベルヌーイ分布を展開すると，最終的にはBCEと同様の式になります．\n",
        "\n",
        "また，潜在変数を標準正規分布へ近似するためのKLダイバージェンス$D_{KL}\\left[N(\\mu, \\sigma)\\|N(0, 1)\\right]$は，以下の式で表現されます（詳細な展開は省略）．\n",
        "$$\n",
        "D_{KL}\\left[N(\\mu, \\sigma)\\|N(0, 1)\\right] = \\frac{1}{2}\\sum_{i}(1+2\\log \\sigma_{i}-\\mu_{i}^{2} - \\sigma_{i}^{2})\n",
        "$$\n",
        "KLダイバージェンスは，ネットワーク全体の正則化の役割を果たします．\n",
        "\n",
        "※ KLダイバージェンスは，分布間の距離を図る指標ですが，厳密な距離を表現していないことに注意してください．また，双方向性がないことにも注意してください．\n",
        "\n",
        "この誤差関数の計算をPythonの関数`loss_function`として定義します．\n",
        "`tilde_x`と`x`がそれぞれ，再構成した画像，入力画像のデータの引数，`mu`, `log_ver`がVAEの平均と分散を示しています．\n",
        "`bce`はBCEを計算するためのpytorchのモジュールを入力するための引数です．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS9dNVO0bOiN"
      },
      "outputs": [],
      "source": [
        "def loss_function(tilde_x, x, mu, log_var, bce):\n",
        "    reconstruction_loss= bce(tilde_x.view(-1, 784), x.view(-1, 784))\n",
        "    kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return reconstruction_loss + kl_div"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0f83-lrbOiP"
      },
      "source": [
        "## ネットワークの学習\n",
        "\n",
        "ネットワークの学習を行います．\n",
        "\n",
        "まず，誤差計算に使用するBCE lossのPyTorchモジュール`BCEWithLogitsLoss`を定義します．\n",
        "BCE lossを計算するためには，用いる値が[0, 1]の間で収まっている必要があります．\n",
        "`BCEWithLogitsLoss`では，内部で入力された数値に対してSigmoid関数を適用して誤差計算を行なっているため，便宜上Decoderの出力値にはSigmoid関数を適用しないようなネットワーク構造を定義しています．\n",
        "\n",
        "※ VAEの再構成誤差に対応するBCE lossは全体を平均しない代わりに，要素全てを合計する`reduction=='sum'`を使用しています．合計値を計算する誤差関数では，学習に使用する画像データの解像度と比例して誤差が増加することを覚えておいてください．\n",
        "ここで平均値を計算する方法で誤差計算をおこなうと，KLダイバージェンスが負の値となる場合や，画像の復元ができなくなることがあるため，注意してください（詳細は割愛しますが，KLダイバージェンスが同時確率であることが理由です）．\n",
        "\n",
        "学習回数`n_epoch`は10に設定し，学習を開始します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeMNuhEsbOiP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "n_epoch = 10\n",
        "bce = nn.BCEWithLogitsLoss(reduction='sum')\n",
        "if use_cuda:\n",
        "  bce = bce.cuda()\n",
        "\n",
        "model.train()\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, n_epoch+1):\n",
        "    sum_loss = 0.0\n",
        "    for x, _ in train_loader:\n",
        "        if use_cuda:\n",
        "            x = x.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        tilde_x, mu, log_var = model(x.view(x.size(0), -1))\n",
        "        loss = loss_function(tilde_x, x, mu, log_var, bce)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sum_loss += loss.item()\n",
        "        \n",
        "    print(\"epoch:{}, mean loss: {}, elapsed time: {}\".format(epoch,\n",
        "                                                             sum_loss / len(train_loader),\n",
        "                                                             time() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiBN_Q6ybOiS"
      },
      "source": [
        "## 学習済みモデルを用いて画像の復元\n",
        "先ほど学習した重みパラメータを用いて，画像の復元をします．\n",
        "まず，mnistのテストデータからランダムにサンプルした画像を入力した結果を確認します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdIIlNzibOiT"
      },
      "outputs": [],
      "source": [
        "test_transform = transforms.Compose([transforms.ToTensor()])\n",
        "mnist_testdata = datasets.MNIST(root='./data', train=False, transform=test_transform)\n",
        "test_loader =DataLoader(dataset=mnist_testdata, batch_size=10, shuffle=True)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx, (x, y) in enumerate(test_loader):\n",
        "        if use_cuda:\n",
        "            x_in = x.cuda()\n",
        "        y_in = y\n",
        "        x_out, mu, logvar = model(x_in.view(x_in.size(0), -1))\n",
        "        break\n",
        "\n",
        "if use_cuda:\n",
        "    x_in = x_in.cpu()\n",
        "    x_out = x_out.cpu()\n",
        "\n",
        "# 画像データを[0, 1] --> [0, 255]の値に変更，配列データの変換（numpy arrayへの変換）\n",
        "output_img = (x_out*256.).clamp(min=0., max=255.).view(-1, 1, 28, 28).data.squeeze().numpy().astype(np.uint8)\n",
        "input_img = (x_in * 256.).clamp(min=0., max=255.).data.squeeze().numpy()\n",
        "\n",
        "fig = plt.figure(figsize=(10, 3))\n",
        "# MNISTのテストデータ (上)\n",
        "for i, im in enumerate(input_img):\n",
        "    ax = fig.add_subplot(2, 10, i+1, xticks=[], yticks=[])\n",
        "    ax.imshow(im, 'gray')\n",
        "\n",
        "# VAEから出力された画像データ　（下）\n",
        "for i, im in enumerate(output_img):\n",
        "    ax = fig.add_subplot(2, 10, i+11, xticks=[], yticks=[])\n",
        "    ax.imshow(im, 'gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Eq5QYg-0HQF"
      },
      "source": [
        "## 潜在空間の可視化\n",
        "\n",
        "学習で獲得した潜在空間の表現を可視化して確認します． 潜在空間の2次元ベクトルを擬似的に作成し，その値をDecoderへと入力し画像を生成することで，潜在空間の値を確認します．\n",
        "\n",
        "※ この演算はhidden_num = 2の場合のAuto Encoderでのみ動作します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QoKBc770HQF"
      },
      "outputs": [],
      "source": [
        "# 各次元のサンプリング点の生成\n",
        "nv = 25\n",
        "value1 = np.linspace(-2, 2, nv)\n",
        "value2 = np.linspace(-2, 2, nv)\n",
        "\n",
        "# 結果表示用のNumpy arrayの作成\n",
        "plot_array = np.zeros([28 * 25, 28 * 25], dtype=np.float32)\n",
        "\n",
        "# 潜在変数をDecoderへ入力し，画像を生成する\n",
        "for i, yi in enumerate(value1):\n",
        "    for j, xj in enumerate(value2):\n",
        "        xx = torch.tensor([[yi, xj]], dtype=torch.float32)\n",
        "        xx = xx.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output = model.decoder(xx)\n",
        "            \n",
        "        output = output.view(-1, 28, 28)\n",
        "        output = output.cpu().numpy()\n",
        "        plot_array[(25-i-1)*28:(25-i)*28, j*28:(j+1)*28] = (output*256.).clip(min=0., max=255.)\n",
        "\n",
        "# 結果の表示\n",
        "plt.figure(figsize=(10, 15))        \n",
        "Xi, Yi = np.meshgrid(value1, value2)\n",
        "plt.imshow(plot_array, origin=\"upper\", cmap=\"gray\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_1IHHn78C-Y"
      },
      "source": [
        "# 課題\n",
        "\n",
        "1. VAEの中間層のユニット数を変更して学習した際にどのような傾向が現れるか確認してみましょう．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3_G1PaFn05t"
      },
      "source": [
        "# 参考文献\n",
        "[1] Diederik P. Kingma and Max Welling, Auto-Encoding Variational Bayes, ICLR, 2014.\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbUpp3LsJR68"
      },
      "source": [
        "## 画像分類と画像生成\n",
        "機械学習をはじめとした画像分類モデルは，任意のデータ$x$をモデルに与えるとクラス確率を返すネットワークです．画像分類は任意のデータセットを用いて経験損失を最小化するようなモデルの重みパラメータを発見することが大きな目的です．\\\n",
        "一方，データ（画像や音声等）生成をするニューラルネットワークは，任意の潜在変数や過去の系列情報からデータ$p_{\\theta}(x|z)$を生成することを目的としています．ただ，闇雲に潜在変数を与えているだけでは，求めたデータの生成が困難です．\\\n",
        "そのため，潜在変数から画像を生成するVAEでは，真のデータ分布$p(x)$を仮定した潜在空間から変数をサンプリングする戦略立てがされています．\n",
        "\n",
        "## VAEの理論（なぜ変分推論？）\n",
        "先に述べたように，VAEでは画像生成をするために真のデータ分布$p(x)$を利用する必要があります．しかしながら，我々がよく目にするデータセット達の分布は，複雑なので物理的に求めることが不可能です．\\\n",
        "では，ある潜在変数からデータを生成する$p_{\\theta}(x|z)$を逆向きに$p_{\\theta}(z|x)$として利用することで，実データの分布を探れそうですが，ニューラルネットワークの逆変換になるため計算が困難です．\\\n",
        "そこで，VAEでは実データの次元数を圧縮する$q_{\\phi}(z|x)$を利用します．これによって，ニューラルネットワークを逆向きに利用する必要がなくなり計算が可能です．\n",
        "\n",
        "### 変分下限\n",
        "Encoder$q_{\\phi}(z|x)$とDeocder$p_{\\theta}(x|z)$を含むモデルを単に$p(x)$として，最尤推定により$x$にフィットするような最適なパラメータを発見することを考えます．\\\n",
        "しかしながら，積分の最大化問題を扱う必要があり，少々扱いづらいので変分下限を最大化することで下から抑えます．\\\n",
        "変分下限は以下の式変形によって得られます．\n",
        "\\begin{eqnarray}\n",
        "\\log p(x) &=& \\log\\int p(x,z)dz\\\\\n",
        "&=& \\log\\int q(z|x)\\frac{p(x, z)}{q(z|x)}\\\\\n",
        "&\\geq& \\int q(z|x)\\log\\frac{p(x, z)}{q(z|x)}\\\\\n",
        "&=& \\mathcal{L}(x,z)\n",
        "\\end{eqnarray}\n",
        "上の式変形の最後に出てきた$\\mathcal{L}(x,z)$が変分下限となり，これを最大化すれば良いことになります．\\\n",
        "しかしながら，連続値に不等号が絡んでいるので厳密な最適値を導き出すことが困難で，僅かながらもギャップが生まれてしまいます．\n",
        "従って，$p(x) - \\mathcal{L}(x,z)$によってギャップを効率的に埋める術を知る必要があります．\n",
        "\n",
        "### 変分下限のギャップ\n",
        "$p(x) - \\mathcal{L}(x,z)$のままでは先に進まないので，以下に式変形を示します．\n",
        "\\begin{eqnarray}\n",
        "\\log p(x) - \\mathcal{L}(x,z) &=& \\log p(x) - \\int q(z|x)\\log\\frac{p(x,z)}{q(z|x)}dz\\\\\n",
        "&=& \\log p(x)\\int q(z|x)dz - \\int q(z|x)\\log\\frac{p(x,z)}{q(z|x)}dz\\\\\n",
        "&=& \\int q(z|x)\\log p(x) dz - \\int q(z|x)\\log\\frac{p(z|x)p(x)}{q(z|x)}dz\\\\\n",
        "&=& \\int q(z|x)\\log p(x) dz - \\int q(z|x)\\left(\\log p(z|x) + \\log p(x) - \\log q(z|x)\\right)dz\\\\\n",
        "&=& \\int q(z|x)\\left(\\log p(x) - \\log p(z|x) - \\log p(x) \\log q(z|x)\\right)dz\\\\\n",
        "&=& \\int q(z|x)\\log \\frac{q(z|x)}{p(z|x)} dz\\\\ \n",
        "&=& D_{KL}[q(z|x)\\|p(z|x)]\n",
        "\\end{eqnarray}\n",
        "式変形の結果から，変分下限の素性が$\\mathcal{L}(x,z)=\\log p(x) - D_{KL}[q(z|x)\\|p(z|x)]$だとわかりました．\\\n",
        "$\\log p(x)$はされるため，非負のKLダイバージェンスを最小化すれば，変分下限を最大化することと同値です．\n",
        "\n",
        "EncoderとDecoderそれぞれが出力する分布を近似すれば，真の分布を知ることができそうですが，\bDecoderを向きに利用している$p(z|x)$が含まれているので，計算が困難です．$D_{KL}[q(z|x)\\|p(z|x)]$は綺麗な形にまとめれそうなので式変形をします．\n",
        "\n",
        "### $D_{KL}[q(z|x)\\|p(z|x)]$の式変形\n",
        "ニューラルネットワークの逆変換が含まれている$D_{KL}[q(z|x)\\|p(z|x)]$を式変形して美しい形にします．\n",
        "\\begin{eqnarray}\n",
        "D_{KL}[q(z|x)\\|p(z|x)] &=& \\int q(z|x)\\log\\frac{q(z|x)}{p(z|x)}dz\n",
        "\\end{eqnarray}\n",
        "ニューラルネットワークの逆変換である$p(z|x)$をなくすためにベイズの定理$p(z|x) = \\frac{p(x|z)p(z)}{p(x)}$を利用して式変形を進めます．\n",
        "\\begin{eqnarray}\n",
        "\\int q(z|x)\\log\\frac{q(z|x)}{p(z|x)}dz &=& \\int q(z|x)\\left(\\log q(z|x) - \\log \\frac{p(x|z)p(z)}{p(x)}\\right)dz\\\\\n",
        "&=& \\int q(z|x)\\left(\\log q(z|x) - \\log p(x|z) - \\log p(z) + \\log p(x) \\right)dz\\\\\n",
        "&=& \\int q(z|x)\\left(\\log q(z|x) - \\log p(x|z) - \\log p(z)\\right)dz + \\log p(x)\\\\\n",
        "&=& \\int q(z|x)\\left(\\log \\frac{q(z|x)}{p(z)}-\\log p(x|z)\\right)dz + \\log p(x)\\\\\n",
        "&=& \\int q(z|x)\\log \\frac{q(z|x)}{p(z)}dz - \\int q(z|x)\\log p(x|z)dz + \\log p(x)\\\\\n",
        "&=& D_{KL}[q(z|x)\\|p(z)] - \\mathbb{E}_{q(z|x)}[\\log p(x|z)] + \\log p(x)\n",
        "\\end{eqnarray}\n",
        "美しい形になったので，式変形をした結果を変分下限の式$\\mathcal{L}(x,z)=\\log p(x) - D_{KL}[q(z|x)\\|p(z|x)]$に代入すると以下のように表すことができます．\n",
        "\\begin{eqnarray}\n",
        "\\mathcal{L}(x,z) &=& \\log p(x) - D_{KL}[q(z|x)\\|p(z)] + \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\log p(x)\\\\\n",
        "&=& \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}[q(z|x)\\|p(z)]\n",
        "\\end{eqnarray}\n",
        "\n",
        "これまでの式変形を踏まえると，変分下限はEncdoerに関するDecoderの期待値，つまり生成した画像と真のデータのエントロピーと，Encoderの出力する事後確率$q(z|x)$とユーザが任意に設定する事前確率$p(z)$を計算すれば良いことになります．\n",
        "ここで，事前確率には確率分布の中でも比較的扱いが容易な標準正規分布が利用されることが多いです．"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "01_Variational_autoencoder.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}