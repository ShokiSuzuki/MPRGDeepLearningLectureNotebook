{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShokiSuzuki/MPRGDeepLearningLectureNotebook/blob/dev/16_vit/01_vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWHD62ufU17k"
      },
      "source": [
        "# Vision Transformer\n",
        "\n",
        "---\n",
        "Vision Transformer (ViT) [1] はTransformerをコンピュータビジョンに応用した画像分類手法です．ViTは入力画像を固定領域のパッチに分割して埋め込み，Transformer Encoderに入力します．Transformer Encoder内のSelf Attentionでパッチの関係を学習することで，畳み込みニューラルネットワーク (CNN: Convolutional Neural Network) とは異なり，浅い層から画像全体の特徴を捉えられます．これにより，ImageNetなどのクラス分類タスクでCNNの性能を上回りました．また，ViTはセマンティックセグメンテーションや動画像認識などのタスクに応用され，CNNベースの性能を上回りました．\n",
        "\n",
        "<img src=\"https://github.com/ShokiSuzuki/MPRGDeepLearningLectureNotebook/blob/dev/16_vit/model_scheme.png?raw=true\" width=60%>\n",
        "\n",
        "\n",
        "## Patch Embedding\n",
        "\n",
        "Patch Embeddingは，入力画像を固定領域のパッチに分割して埋め込む処理を行います．例えば，$224 \\times 224$ピクセルの画像を入力として各パッチのサイズを$16 \\times 16$ピクセルとした場合，重なり合わないように$14 \\times 14$の領域に分割します．分割されたパッチは，それぞれflatにして全結合に入力することで埋め込みます．このとき，学習可能なパラメータであるクラストークンを結合し，Transformer Encoderを通した後にクラス分類に使用します．\n",
        "\n",
        "## Position Embedding\n",
        "\n",
        "Position Embeddingは，パッチの位置情報を学習するパラメータです．このパラメータは，Patch Embeddingのあとにそれぞれのパッチに足されます．ネットワークが学習する過程で位置情報を獲得するため，学習条件で値が変化します．\n",
        "\n",
        "Patch EmbeddingとPosition Embeddingを定式化すると以下のようになります．\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_0 &= [ \\mathbf{x}_\\text{class}; \\, \\mathbf{x}^1_p \\mathbf{E}; \\, \\mathbf{x}^2_p \\mathbf{E}; \\cdots; \\, \\mathbf{x}^{N}_p \\mathbf{E} ] + \\mathbf{E}_{pos},\n",
        "&& \\mathbf{E} \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D},\\, \\mathbf{E}_{pos}  \\in \\mathbb{R}^{(N + 1) \\times D}\n",
        "\\end{aligned}\n",
        "\n",
        "ここで，$\\mathbf{x}_\\text{class}$はクラストークン，$\\mathbf{x}_p$はパッチ，$N$はパッチ数，$P$はパッチサイズ，$C$はチャンネル数，$D$は埋め込み次元数，$\\mathbf{E}$は全結合，$\\mathbf{E}_{pos}$はPosition Embeddingです．\n",
        "\n",
        "## ファインチューニング\n",
        "\n",
        "ViTは，大規模データセットで事前学習して小規模データセットでファインチューニングすることが効果的です．事前学習するときに画像枚数を変更すると，CNNは枚数を多くしても精度に限界がありますが，ViTは枚数が多いほど精度向上が見込めます．ViTは，JFT-300Mという3億枚の画像が含まれているデータセットで事前学習し，様々なデータセットでファインチューニングをすることでSoTAを達成していますが，非公開のデータセットのため再現不可能です．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmzYVTVzU17m"
      },
      "source": [
        "# Vision Transformerの学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3oG9bveU17n"
      },
      "source": [
        "### モジュールの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4iq2dBxU17n",
        "outputId": "f0bb0f72-adec-4703-b836-f0e0d781f103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm==0.5.4\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 23.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm==0.5.4) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm==0.5.4) (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm==0.5.4) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.5.4) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.5.4) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.5.4) (1.21.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.5.4) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.5.4) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.5.4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.5.4) (2022.9.24)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ],
      "source": [
        "!pip install timm==0.5.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYr71AN1U17n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import timm\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models import create_model\n",
        "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
        "from functools import partial\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL0KxX4zU17o"
      },
      "source": [
        "### ネットワークの定義"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqyPcVS9U17o"
      },
      "source": [
        "#### Patch Embedding\n",
        "\n",
        "Patch Embeddingでは，画像をパッチに分割して埋め込みます．埋め込まれたパッチをパッチトークンと呼びます．ViTはパッチをflatにして全結合に入力しますが，実装上は，カーネルサイズ（パッチサイズ）= ストライドとした2次元畳み込みでも可能です．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsPouvAJU17o"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size    = img_size\n",
        "        self.patch_size  = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        \n",
        "        # 埋め込み処理のための重み\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(x).flatten(2).transpose(1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huFa_aa6U17p"
      },
      "source": [
        "#### Multi-Head Attention\n",
        "\n",
        "Self-Attentionはパッチトークンを空間方向に混ぜるような変換を行います．Multi-Head Attentionはパッチトークンをベクトルのdepth方向に$h$個に分割し，それぞれでSelf-Attentionを求めます．これにより，Head毎に注目したパッチが異なる特徴が得られるため，アンサンブル効果による精度向上が見込めます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kwg7aH0GU17p"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv.unbind(0)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnImeUIIU17p"
      },
      "source": [
        "#### Multi-Layer Perceptron\n",
        "\n",
        "Multi-Head Attentionでは空間方向に混ぜるような変換を行うのに対し，Multi-Layer Perceptronではベクトルのdepth方向に混ぜるような変換を行います．活性化関数にはGELUを使用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhYgdnmKU17q"
      },
      "outputs": [],
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.drop1 = nn.Dropout(drop)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop2 = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg3afq0BU17q"
      },
      "source": [
        "#### Transformer Encoder\n",
        "\n",
        "Transformer Encoderは，Multi-Head AttentionとMulti-Layer Perceptronを交互に使用します．また，それぞれResidual Connectionを用います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlMTvDm3U17q"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., \n",
        "                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
        "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path1(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BOm0dA0U17q"
      },
      "source": [
        "#### ネットワーク全体の構築\n",
        "\n",
        "これまで定義したクラスをもとにViTを構築します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uej3haHDU17q"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, \n",
        "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, \n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed, \n",
        "                 norm_layer=None, act_layer=None, block_fn=Block):\n",
        "        super().__init__()\n",
        "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
        "        act_layer = act_layer or nn.GELU\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "\n",
        "        self.patch_embed = embed_layer(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_chans=in_chans,\n",
        "            embed_dim=embed_dim,\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # クラストークン\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        \n",
        "        # Position Embedding\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop  = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth\n",
        "        \n",
        "        # Transformer Encoder\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            block_fn(\n",
        "                dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[i],\n",
        "                norm_layer=norm_layer,\n",
        "                act_layer=act_layer\n",
        "            )\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        # Classifier Head\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        \n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        \n",
        "        # クラストークンの結合\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        \n",
        "        x = self.pos_drop(x + self.pos_embed)\n",
        "        \n",
        "        # Transformer Encoderへの入力\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x[:, 0]) # 0番目にあるクラストークンを取り出して全結合へ入力\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXCXRDZsU17r"
      },
      "outputs": [],
      "source": [
        "def vit_tiny_patch16_224(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "def vit_small_patch16_224(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O-nIbCrU17r"
      },
      "source": [
        "### データの準備\n",
        "今回は，CIFAR-10を用いてフルスクラッチで学習します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "c8b51e29fded4956bd376702ce2d771f",
            "a29e6b6eba3d46089189fa3dada3091b",
            "13a624d012094907bb3f2ec12b5c3153",
            "d2e72722e61a4070b605670c055a22a4",
            "9be0359f4bed4b47a43012693529c2aa",
            "f510cae471ac474a94cf2993a4a82bfa",
            "f35c86699202424ba9f24b21146ab44b",
            "724d43f997114942bba466e602196da6",
            "f2c9c2358e4f486caa18a425699f16e1",
            "4659274cbf7a4b3ba80451d7ef779775",
            "89c8fcdae4e44a42ae382c4c533c851d"
          ]
        },
        "id": "N1he5n-VU17r",
        "outputId": "9c4c8576-6f9b-486a-d589-8fa2c43ad540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8b51e29fded4956bd376702ce2d771f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "img_size = 32\n",
        "\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(img_size, padding=4),\n",
        "                                      transforms.Resize(img_size),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                     ])\n",
        "\n",
        "test_transform  = transforms.Compose([transforms.Resize(img_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                     ])\n",
        "\n",
        "dataset_train = torchvision.datasets.CIFAR10(\"./\", train=True, transform=train_transform, download=True)\n",
        "dataset_test  = torchvision.datasets.CIFAR10(\"./\", train=False, transform=test_transform, download=False)\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128, num_workers=2, pin_memory=True, drop_last=True)\n",
        "dataloader_test  = torch.utils.data.DataLoader(dataset_test, batch_size=64, num_workers=2, pin_memory=True, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy9udJN4U17r"
      },
      "source": [
        "### 学習条件の設定\n",
        "\n",
        "ViTの性能をCNNと比較するために，ViTのSmallモデルとパラメータ数が同等のResNet-50を用います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu3I1unkU17r"
      },
      "outputs": [],
      "source": [
        "# クラス数の設定\n",
        "num_classes = 10\n",
        "\n",
        "# ViTの定義\n",
        "vit = vit_small_patch16_224(pretrained=False, num_classes=num_classes, img_size=img_size, patch_size=4)\n",
        "# CNNの定義\n",
        "cnn = create_model(\"resnet50\", pretrained=False, num_classes=num_classes)\n",
        "\n",
        "\n",
        "lr  = 0.0005\n",
        "weight_decay = 0.05\n",
        "epochs = 10   # エポック数の設定\n",
        "warmup_t = 3\n",
        "\n",
        "optimizer_vit     = torch.optim.AdamW(vit.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "lr_scheduler_vit  = CosineLRScheduler(optimizer=optimizer_vit, t_initial=epochs, warmup_t=warmup_t)\n",
        "optimizer_cnn     = torch.optim.AdamW(cnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "lr_scheduler_cnn  = CosineLRScheduler(optimizer=optimizer_cnn, t_initial=epochs, warmup_t=warmup_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jnTcfijU17s"
      },
      "source": [
        "パラメータ数の確認"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-aK4ttgU17s",
        "outputId": "e179552c-68c1-472d-cba5-cd64d67e8994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViT parameters:  21342346\n",
            "CNN parameters:  23528522\n"
          ]
        }
      ],
      "source": [
        "def num_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"ViT parameters: \", num_parameters(vit))\n",
        "print(\"CNN parameters: \", num_parameters(cnn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHIom2qtU17s"
      },
      "source": [
        "### CNNの学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svauzN4eU17s",
        "outputId": "53d2fe2a-47bd-48b0-87c3-de4634d313b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1,            mean loss: 2.35,            mean accuracy: 0.11,            elapsed_time : 44.7\n",
            "test accuracy: 0.1253\n",
            "epoch: 2,            mean loss: 2.352,            mean accuracy: 0.11,            elapsed_time : 85.9\n",
            "test accuracy: 0.1271\n",
            "epoch: 3,            mean loss: 1.713,            mean accuracy: 0.36,            elapsed_time : 128.19\n",
            "test accuracy: 0.4976\n",
            "epoch: 4,            mean loss: 1.371,            mean accuracy: 0.5,            elapsed_time : 169.86\n",
            "test accuracy: 0.5753\n",
            "epoch: 5,            mean loss: 1.174,            mean accuracy: 0.58,            elapsed_time : 211.54\n",
            "test accuracy: 0.6257\n",
            "epoch: 6,            mean loss: 1.017,            mean accuracy: 0.63,            elapsed_time : 253.89\n",
            "test accuracy: 0.6742\n",
            "epoch: 7,            mean loss: 0.906,            mean accuracy: 0.68,            elapsed_time : 299.72\n",
            "test accuracy: 0.7056\n",
            "epoch: 8,            mean loss: 0.824,            mean accuracy: 0.7,            elapsed_time : 344.07\n",
            "test accuracy: 0.7296\n",
            "epoch: 9,            mean loss: 0.75,            mean accuracy: 0.73,            elapsed_time : 387.08\n",
            "test accuracy: 0.7415\n",
            "epoch: 10,            mean loss: 0.701,            mean accuracy: 0.75,            elapsed_time : 429.78\n",
            "test accuracy: 0.7477\n"
          ]
        }
      ],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "cnn.to(device)\n",
        "use_amp = True\n",
        "scaler_cnn = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "start = time()\n",
        "for epoch in range(epochs):\n",
        "    cnn.train()\n",
        "    \n",
        "    sum_loss = 0.0\n",
        "    count    = 0\n",
        "    for img, cls in dataloader_train:\n",
        "        \n",
        "        img = img.to(device, non_blocking=True)\n",
        "        cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "        # CNNに画像を入力 & 損失を計算\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            logit_cnn = cnn(img)\n",
        "            loss_cnn  = criterion(logit_cnn, cls)\n",
        "            \n",
        "        # CNNの更新\n",
        "        optimizer_cnn.zero_grad()\n",
        "        scaler_cnn.scale(loss_cnn).backward()\n",
        "        scaler_cnn.step(optimizer_cnn)\n",
        "        scaler_cnn.update()\n",
        "        \n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss_cnn.item()\n",
        "        count    += torch.sum(logit_cnn.argmax(dim=1) == cls).item()\n",
        "        \n",
        "    lr_scheduler_cnn.step(epoch)\n",
        "    \n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch+1},\\\n",
        "            mean loss: {round(sum_loss/len(dataloader_train), 3)},\\\n",
        "            mean accuracy: {round(count/len(dataloader_train.dataset), 2)},\\\n",
        "            elapsed_time : {round(time()-start, 2)}\")\n",
        "    \n",
        "    # 評価\n",
        "    cnn.eval()\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for img, cls in dataloader_test:\n",
        "            img = img.to(device, non_blocking=True)\n",
        "            cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "            logit_cnn = cnn(img)\n",
        "            count += torch.sum(logit_cnn.argmax(dim=1) == cls).item()\n",
        "            \n",
        "        print(f\"test accuracy: {count/len(dataloader_test.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAS4HmSQU17s"
      },
      "source": [
        "### ViTの学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UuPlFv3U17s",
        "outputId": "5c38da66-9f31-412a-f097-d5b1c1e738db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1,            mean loss: 2.388,            mean accuracy: 0.1,            elapsed_time : 68.22\n",
            "test accuracy: 0.0993\n",
            "epoch: 2,            mean loss: 2.389,            mean accuracy: 0.1,            elapsed_time : 147.16\n",
            "test accuracy: 0.0993\n",
            "epoch: 3,            mean loss: 1.826,            mean accuracy: 0.32,            elapsed_time : 225.19\n",
            "test accuracy: 0.4146\n",
            "epoch: 4,            mean loss: 1.531,            mean accuracy: 0.43,            elapsed_time : 303.73\n",
            "test accuracy: 0.4897\n",
            "epoch: 5,            mean loss: 1.361,            mean accuracy: 0.5,            elapsed_time : 382.24\n",
            "test accuracy: 0.5143\n",
            "epoch: 6,            mean loss: 1.233,            mean accuracy: 0.56,            elapsed_time : 460.59\n",
            "test accuracy: 0.5802\n",
            "epoch: 7,            mean loss: 1.136,            mean accuracy: 0.59,            elapsed_time : 539.01\n",
            "test accuracy: 0.6044\n",
            "epoch: 8,            mean loss: 1.046,            mean accuracy: 0.62,            elapsed_time : 617.51\n",
            "test accuracy: 0.6307\n",
            "epoch: 9,            mean loss: 0.961,            mean accuracy: 0.66,            elapsed_time : 696.11\n",
            "test accuracy: 0.6689\n",
            "epoch: 10,            mean loss: 0.888,            mean accuracy: 0.68,            elapsed_time : 774.58\n",
            "test accuracy: 0.6873\n"
          ]
        }
      ],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "vit.to(device)\n",
        "use_amp = True\n",
        "scaler_vit = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "start = time()\n",
        "for epoch in range(epochs):\n",
        "    vit.train()\n",
        "    \n",
        "    sum_loss = 0.0\n",
        "    count    = 0\n",
        "    for img, cls in dataloader_train:\n",
        "        \n",
        "        img = img.to(device, non_blocking=True)\n",
        "        cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "        # ViTに画像を入力 & 損失を計算\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            logit_vit = vit(img)\n",
        "            loss_vit  = criterion(logit_vit, cls)\n",
        "            \n",
        "        # ViTの更新\n",
        "        optimizer_vit.zero_grad()\n",
        "        scaler_vit.scale(loss_vit).backward()\n",
        "        scaler_vit.step(optimizer_vit)\n",
        "        scaler_vit.update()\n",
        "        \n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss_vit.item()\n",
        "        count    += torch.sum(logit_vit.argmax(dim=1) == cls).item()\n",
        "        \n",
        "    lr_scheduler_vit.step(epoch)\n",
        "    \n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch+1},\\\n",
        "            mean loss: {round(sum_loss/len(dataloader_train), 3)},\\\n",
        "            mean accuracy: {round(count/len(dataloader_train.dataset), 2)},\\\n",
        "            elapsed_time : {round(time()-start, 2)}\")\n",
        "    \n",
        "    # 評価\n",
        "    vit.eval()\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for img, cls in dataloader_test:\n",
        "            img = img.to(device, non_blocking=True)\n",
        "            cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "            logit_vit = vit(img)\n",
        "            count += torch.sum(logit_vit.argmax(dim=1) == cls).item()\n",
        "            \n",
        "        print(f\"test accuracy: {count/len(dataloader_test.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "iP3j-qwNU17s"
      },
      "source": [
        "# ImageNetで事前学習したモデルを用いたファインチューニング\n",
        "\n",
        "次に，ImageNetで事前学習したモデルを用いてCIFAR-10でファインチューニングします．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-h09wINU17s"
      },
      "source": [
        "### データの準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey0GcQ2sU17s",
        "outputId": "1dc22e77-978b-4156-8631-f75672091578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.Resize(224),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                     ])\n",
        "\n",
        "test_transform  = transforms.Compose([transforms.Resize(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                     ])\n",
        "\n",
        "dataset_train = torchvision.datasets.CIFAR10(\"./\", train=True, transform=train_transform, download=True)\n",
        "dataset_test  = torchvision.datasets.CIFAR10(\"./\", train=False, transform=test_transform, download=False)\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=64, num_workers=2, pin_memory=True, drop_last=True)\n",
        "dataloader_test  = torch.utils.data.DataLoader(dataset_test, batch_size=64, num_workers=2, pin_memory=True, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uvx2qwkU17s"
      },
      "source": [
        "### 学習条件の設定\n",
        "\n",
        "ファインチューニングでも，ViTのSmallモデルとパラメータ数が同等のResNet-50を用います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0VWq0l2U17s",
        "outputId": "c160f00c-c583-4bfb-d726-eac1ff987b63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\" to /root/.cache/torch/hub/checkpoints/deit_small_patch16_224-cd65a155.pth\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1_0-14fe96d1.pth\" to /root/.cache/torch/hub/checkpoints/resnet50_a1_0-14fe96d1.pth\n"
          ]
        }
      ],
      "source": [
        "# クラス数の設定\n",
        "num_classes = 10\n",
        "\n",
        "# ViTの定義 (timmのcreate_modelを使用)\n",
        "vit_finetune = create_model(\"deit_small_patch16_224\", pretrained=True, num_classes=num_classes)\n",
        "# CNNの定義\n",
        "cnn_finetune = create_model(\"resnet50\", pretrained=True, num_classes=num_classes)\n",
        "\n",
        "lr  = 0.0001\n",
        "weight_decay = 0.05\n",
        "epochs = 5\n",
        "warmup_t = 0\n",
        "\n",
        "optimizer_vit     = torch.optim.AdamW(vit_finetune.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "lr_scheduler_vit  = CosineLRScheduler(optimizer=optimizer_vit, t_initial=epochs, warmup_t=warmup_t)\n",
        "optimizer_cnn     = torch.optim.AdamW(cnn_finetune.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "lr_scheduler_cnn  = CosineLRScheduler(optimizer=optimizer_cnn, t_initial=epochs, warmup_t=warmup_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtYsGiTqU17s"
      },
      "source": [
        "### CNNの学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQvVUsRFU17t",
        "outputId": "2836bec3-25d8-4f7d-a9df-90bd00b2fbba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1,            mean loss: 0.672,            mean accuracy: 0.81,            elapsed_time : 217.19\n",
            "test accuracy: 0.9459\n",
            "epoch: 2,            mean loss: 0.165,            mean accuracy: 0.95,            elapsed_time : 463.87\n",
            "test accuracy: 0.9578\n",
            "epoch: 3,            mean loss: 0.108,            mean accuracy: 0.96,            elapsed_time : 711.87\n",
            "test accuracy: 0.9628\n",
            "epoch: 4,            mean loss: 0.07,            mean accuracy: 0.98,            elapsed_time : 958.31\n",
            "test accuracy: 0.9657\n",
            "epoch: 5,            mean loss: 0.048,            mean accuracy: 0.99,            elapsed_time : 1206.16\n",
            "test accuracy: 0.9672\n"
          ]
        }
      ],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "cnn_finetune.to(device)\n",
        "use_amp = True\n",
        "scaler_cnn = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "start = time()\n",
        "for epoch in range(epochs):\n",
        "    cnn_finetune.train()\n",
        "    \n",
        "    sum_loss = 0.0\n",
        "    count    = 0\n",
        "    for img, cls in dataloader_train:\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "        # CNNに画像を入力 & 損失を計算\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            logit_cnn = cnn_finetune(img)\n",
        "            loss_cnn  = criterion(logit_cnn, cls)\n",
        "            \n",
        "        # CNNの更新\n",
        "        optimizer_cnn.zero_grad()\n",
        "        scaler_cnn.scale(loss_cnn).backward()\n",
        "        scaler_cnn.step(optimizer_cnn)\n",
        "        scaler_cnn.update()\n",
        "        \n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss_cnn.item()\n",
        "        count    += torch.sum(logit_cnn.argmax(dim=1) == cls).item()\n",
        "        \n",
        "    lr_scheduler_cnn.step(epoch)\n",
        "    \n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch+1},\\\n",
        "            mean loss: {round(sum_loss/len(dataloader_train), 3)},\\\n",
        "            mean accuracy: {round(count/len(dataloader_train.dataset), 2)},\\\n",
        "            elapsed_time : {round(time()-start, 2)}\")\n",
        "    \n",
        "    # 評価\n",
        "    cnn_finetune.eval()\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for img, cls in dataloader_test:\n",
        "            img = img.to(device, non_blocking=True)\n",
        "            cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "            logit_cnn = cnn_finetune(img)\n",
        "            count += torch.sum(logit_cnn.argmax(dim=1) == cls).item()\n",
        "            \n",
        "        print(f\"test accuracy: {count/len(dataloader_test.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bV1FrStU17t"
      },
      "source": [
        "### ViTの学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7FZ2vW4U17t",
        "outputId": "facb26a2-b2f2-4120-8e12-b8bb42482948"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1,            mean loss: 0.219,            mean accuracy: 0.93,            elapsed_time : 273.08\n",
            "test accuracy: 0.9547\n",
            "epoch: 2,            mean loss: 0.101,            mean accuracy: 0.97,            elapsed_time : 586.94\n",
            "test accuracy: 0.9631\n",
            "epoch: 3,            mean loss: 0.064,            mean accuracy: 0.98,            elapsed_time : 900.3\n",
            "test accuracy: 0.9594\n",
            "epoch: 4,            mean loss: 0.036,            mean accuracy: 0.99,            elapsed_time : 1213.06\n",
            "test accuracy: 0.9683\n",
            "epoch: 5,            mean loss: 0.015,            mean accuracy: 0.99,            elapsed_time : 1524.83\n",
            "test accuracy: 0.9749\n"
          ]
        }
      ],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "vit_finetune.to(device)\n",
        "use_amp = True\n",
        "scaler_vit = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "start = time()\n",
        "for epoch in range(epochs):\n",
        "    vit_finetune.train()\n",
        "    \n",
        "    sum_loss = 0.0\n",
        "    count    = 0\n",
        "    for img, cls in dataloader_train:\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "        # ViTに画像を入力 & 損失を計算\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            logit_vit = vit_finetune(img)\n",
        "            loss_vit  = criterion(logit_vit, cls)\n",
        "            \n",
        "        # ViTの更新\n",
        "        optimizer_vit.zero_grad()\n",
        "        scaler_vit.scale(loss_vit).backward()\n",
        "        scaler_vit.step(optimizer_vit)\n",
        "        scaler_vit.update()\n",
        "        \n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss_vit.item()\n",
        "        count    += torch.sum(logit_vit.argmax(dim=1) == cls).item()\n",
        "        \n",
        "    lr_scheduler_vit.step(epoch)\n",
        "    \n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch+1},\\\n",
        "            mean loss: {round(sum_loss/len(dataloader_train), 3)},\\\n",
        "            mean accuracy: {round(count/len(dataloader_train.dataset), 2)},\\\n",
        "            elapsed_time : {round(time()-start, 2)}\")\n",
        "    \n",
        "    # 評価\n",
        "    vit_finetune.eval()\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for img, cls in dataloader_test:\n",
        "            img = img.to(device, non_blocking=True)\n",
        "            cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "            logit_vit = vit_finetune(img)\n",
        "            count += torch.sum(logit_vit.argmax(dim=1) == cls).item()\n",
        "            \n",
        "        print(f\"test accuracy: {count/len(dataloader_test.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ0Q91mUU17t"
      },
      "source": [
        "# 課題\n",
        "1. Multi-Head Attentionのhead数を変えてみましょう"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTXWbeohU17t"
      },
      "source": [
        "# 参考文献\n",
        "\n",
        "[1]  Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In *International Conference on Learning Representations*, 2021."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "vscode": {
      "interpreter": {
        "hash": "292b6db79841738ca824380b98b3729806180501b450715886339eec8f0eb9a3"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c8b51e29fded4956bd376702ce2d771f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a29e6b6eba3d46089189fa3dada3091b",
              "IPY_MODEL_13a624d012094907bb3f2ec12b5c3153",
              "IPY_MODEL_d2e72722e61a4070b605670c055a22a4"
            ],
            "layout": "IPY_MODEL_9be0359f4bed4b47a43012693529c2aa"
          }
        },
        "a29e6b6eba3d46089189fa3dada3091b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f510cae471ac474a94cf2993a4a82bfa",
            "placeholder": "​",
            "style": "IPY_MODEL_f35c86699202424ba9f24b21146ab44b",
            "value": "100%"
          }
        },
        "13a624d012094907bb3f2ec12b5c3153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_724d43f997114942bba466e602196da6",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2c9c2358e4f486caa18a425699f16e1",
            "value": 170498071
          }
        },
        "d2e72722e61a4070b605670c055a22a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4659274cbf7a4b3ba80451d7ef779775",
            "placeholder": "​",
            "style": "IPY_MODEL_89c8fcdae4e44a42ae382c4c533c851d",
            "value": " 170498071/170498071 [00:03&lt;00:00, 33961797.93it/s]"
          }
        },
        "9be0359f4bed4b47a43012693529c2aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f510cae471ac474a94cf2993a4a82bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f35c86699202424ba9f24b21146ab44b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "724d43f997114942bba466e602196da6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2c9c2358e4f486caa18a425699f16e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4659274cbf7a4b3ba80451d7ef779775": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89c8fcdae4e44a42ae382c4c533c851d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}