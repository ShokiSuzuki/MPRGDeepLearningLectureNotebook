{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHUKYbWlZeRxXV/xmVsMR/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/13_rnn/06_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzuK1IkM_JLD"
      },
      "source": [
        "# Transformer\n",
        "\n",
        "2017年に発表されたTransformerは，CNNやRNNなどを用いずAttention機構のみを用いたモデルです．翻訳や文章生成などのタスクでRNNとSeq2seqモデルが主流でしたが，これらのモデルは逐次的に単語を処理するため学習時に並列計算できないという問題がありました．また，長文に対してAttentionが使われていましたが，このAttentionはほとんどRNNと一緒に使われていました．一方で，TransformerはAttention機構だけ使うことで，入出力の文章同士の広範囲な依存関係を捉える構造になっています．\n",
        "\n",
        "モデルはSeq2seqと同様にエンコーダ・デコーダモデルです．エンコーダでは，Multi-Head AttentionとFeed Forwardのブロックを$N$回スタックする構造です．デコーダでは，それに加えMasked Multi-Head Attentionのブロックで構成されています．Masked Multi学習時，デコーダは自己回帰を使用せず，全ターゲットを同時に入力し，全ターゲットを同時に予測します．この時，予測すべきターゲットの情報が予測前のデコーダにリークしないようにMaskします．評価時は自己回帰でターゲットを生成します．\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/13_rnn/06_Transformer/trans.jpeg?raw=true\" width = 50%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rLVuCf1rrwh"
      },
      "source": [
        "# Attetionの種類\n",
        "\n",
        "## Self-Attention\n",
        "Self-AttentionはQuery，Key，Valueが全て同じ情報を使うAttentionです．Self-Attentionは言語の文法や照応関係を獲得するのにも使われています．このSelf-Attentionは汎用に使える仕組みで，エンコーダとデコーダどちらにも使われています．\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/13_rnn/06_Transformer/self-attention.jpeg?raw=true\" width = 50%>\n",
        "\n",
        "\n",
        "\n",
        "## Source-Target-Attention\n",
        "Source-Target-AttentionはQueryとMemory(Key, Value)が異なる情報を使うAttenitonです．Source-Target-Attentionは基本的にデコーダで使われます．例えば，図のようにエンコーダに「お腹/が/減った」を入力した場合に，デコーダが「ラーメン/食べ/よう」を出力する時を考えましょう．この時，最初にデコーダは\\<BOS>を入力した時に，エンコーダの入力に着目しながらラーメンを出力します．この例では「減った」に着目しています．次にラーメンを入力し，「減った」に着目しながら「食べ」を出力します．これを繰り返します．つまり，デコーダはある時刻$t$のターゲットを受け取って，エンコーダの入力に着目しながら$t+1$時刻のターゲットを予測します．\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/13_rnn/06_Transformer/source-target-attention.jpeg?raw=true\" width = 50%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3HIVGzh52zM"
      },
      "source": [
        "# Scaled Dot-Product Attention\n",
        "こちらは，所謂**Self-Attentionの中身**です．こちらはTransformerの鍵になっています．\n",
        "数式は以下の通りです．\n",
        "\\begin{equation}\n",
        "{\\rm Attention}(Q, K, V)={\\rm softmax} ( \\frac{QK^{T}}{\\sqrt{d_k} } ) V\n",
        "\\end{equation}\n",
        "ここで，$Q, K, V$はそれぞれQuery，Key，Valueです．また$d_k$はQueryの次元数を表します．この平方根$d_k$は，見てわかるように$Q, K$の特徴量をスケールする役割を持ちます．これは層数，すなわちスタックするブロック数(前述のN)が大きくなると，内積が大きくなり，softmax関数の勾配を計算すると非常に小さい値しか返さないためです．\n",
        "\n",
        "図のように，QueryとKeyが行列乗算で計算された後，dの平方根でスケーリングした後，後述するMaskをかけます．この時，Maskには負の無限大がかけられます．これにより，paddingした領域に対しsoftmax後の値を0に近い出力にすることができます．つまり，padding領域のAttention weightを計算しないようにします．最後にValueとの行列乗算をします．\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/13_rnn/06_Transformer/scale.jpeg?raw=true\" width = 30%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF8XKiK_A1SC"
      },
      "source": [
        "# Multi-Head Attention\n",
        "Multi-Head Attentionは1つのQuery，Key，Valueを持たせるのではなく，小さいQuery，Key，Valueに分割して，分割した特徴表現を計算します．構造はシンプルで，Linear層とScaled Dot-Product Attentionを分割した構造を持ちます．最終的に，分割した出力を1つにまとめてLinear層に渡します．このようにわざわざ分割する理由ですが，モデルが異なる特徴表現の異なる情報についてAttention weightを計算できるためです．\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/13_rnn/06_Transformer/multi_head.jpeg?raw=true\" width = 30%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG_ZACcSnqJa"
      },
      "source": [
        "# Mask\n",
        "冒頭でも述べたように，デコーダは未来の情報を伝播しないようにMaskをかけます．例をあげます．下図のように，Maskは同じ情報から作成されます．黒丸はマスクされた領域を表します．Maskは例えば，「好き」という情報を入力した場合に，残りの「な/動物/は」を参照できません．これは推論時未来の情報が与えられないためです．そのため，Queryでは，入力の時刻より先のMemoryの情報に対してMaskをすることで，未来の情報を伝播させないようにします．このマスクはデコーダのMasked Multi-Head Attentionで使われます．\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/13_rnn/06_Transformer/dec_pad.jpeg?raw=true\" width = 50%>\n",
        "\n",
        "\n",
        "次に，エンコーダ・デコーダに入力されるソースとターゲットの長さはバッチによって異なります．例えば，下図のように「おはよう」は１系列，「インコ/が/好き」は3系列，そして「お腹/減った」は2系列となっています．学習・推論時を一番長い3系列に合わせたい時，残りの1，2の系列をpaddingする必要があります．しかし，Attention weightを計算するときにpadding領域も計算されてしまうため，その領域がノイズとなり正確なAttentionを計算するのに邪魔になります．そのため，Attention weightを計算する際は，padding領域に対してMaskを適用します．基本的にこちらのマスクはMulti-Head Attention内で用います．\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/13_rnn/06_Transformer/enc_pad.jpeg?raw=true\" width = 50%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7vEns5KA851"
      },
      "source": [
        "# ここからTransformerの実装に移ります"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBi4hC_uAJNM"
      },
      "source": [
        "###データローダの作成\n",
        "まず，データローダを用意します．データは0から9までの数字と加算記号，開始，終了のフラグです．また，３桁の数字の足し算を行うため，各桁の値を１つずつランダムに生成して連結しています．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOgzz6W1CYU3"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import math \n",
        "import copy\n",
        "from time import time\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "word2id = {str(i): i for i in range(10)}\n",
        "word2id.update({\"<pad>\": 10, \"+\": 11, \"<eos>\": 12})\n",
        "id2word = {v: k for k, v in word2id.items()}\n",
        "\n",
        "class CalcDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def transform(self, string, seq_len=7):\n",
        "        tmp = []\n",
        "        for i, c in enumerate(string):\n",
        "            try:\n",
        "                tmp.append(word2id[c])\n",
        "            except:\n",
        "                tmp += [word2id[\"<pad>\"]] * (seq_len - i)\n",
        "                break\n",
        "        return tmp\n",
        "\n",
        "    def __init__(self, data_num, train=True):\n",
        "        super().__init__()\n",
        "        self.data_num = data_num\n",
        "        self.train = train\n",
        "        self.data = []\n",
        "        self.label = []\n",
        "\n",
        "        for _ in range(data_num):\n",
        "            x = int(\"\".join([random.choice(list(\"0123456789\")) for _ in range(random.randint(1, 3))] ))\n",
        "            y = int(\"\".join([random.choice(list(\"0123456789\")) for _ in range(random.randint(1, 3))] ))\n",
        "            left = (\"{:*<7s}\".format(str(x) + \"+\" + str(y))).replace(\"*\", \"<pad>\")\n",
        "            self.data.append(self.transform(left))\n",
        "\n",
        "            z = x + y\n",
        "            right = (\"{:*<6s}\".format(str(z))).replace(\"*\", \"<pad>\")\n",
        "            right = self.transform(right, seq_len=5)\n",
        "            right = [12] + right\n",
        "            right[right.index(10)] = 12\n",
        "            self.label.append(right)\n",
        "        \n",
        "\n",
        "\n",
        "        self.data = np.asarray(self.data)\n",
        "        self.label = np.asarray(self.label)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        d = self.data[item]\n",
        "        l = self.label[item]\n",
        "        return d, l\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "batch_size = 100\n",
        "epoch_num = 200\n",
        "train_data = CalcDataset(data_num = 20000)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38oA_QkFHMrg"
      },
      "source": [
        "### Maskを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA3HuhR1Gfex"
      },
      "source": [
        "def enc_mask(batch_size, src, size):\n",
        "    mask = src == word2id[\"<pad>\"]\n",
        "    mask = mask.float().masked_fill(mask == 1, float(0.0)).masked_fill(mask == 0, float(1.0))\n",
        "    return mask.view(mask.size(0), 1, mask.size(1))\n",
        "\n",
        "def dec_mask(batch_size, size):\n",
        "    mask = torch.triu(torch.ones(size, size), 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float(1.0)).masked_fill(mask == 1, float(0.0))\n",
        "    mask = mask.view(1, *mask.shape)\n",
        "    mask = mask.expand(batch_size, *mask.shape[1:])\n",
        "    return mask\n",
        "\n",
        "def create_masks(batch_size, src, trg):\n",
        "    src_mask = enc_mask(batch_size, src, src.size(1))\n",
        "\n",
        "    if trg is not None:\n",
        "        size = trg.size(1)\n",
        "        np_mask = dec_mask(batch_size, size)\n",
        "        trg_mask = np_mask\n",
        "        \n",
        "    else:\n",
        "        trg_mask = None\n",
        "    return src_mask, trg_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_XgLB7GehLv"
      },
      "source": [
        "特徴ベクトルが$embedding \\_ dim$，Multi-head Attentionのhead数が$heads$，layer数が$n \\_ layers$です．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEKR35-WV1hZ"
      },
      "source": [
        "embedding_dim = 512\n",
        "n_layers = 6\n",
        "heads = 8\n",
        "vocab_size = len(word2id)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuoQk5c-II-"
      },
      "source": [
        "### Multi-Head AttentionとSelf-Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1tFy3Wq97wu"
      },
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, embedding_dim, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = embedding_dim\n",
        "        \n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        \n",
        "        self.eps = eps\n",
        "    \n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm\n",
        "\n",
        "def attention(q, k, v, d_k, mask=None, dec_mask=False):\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        if dec_mask:\n",
        "            mask = mask.view(mask.size(0), 1, mask.size(1), mask.size(2))\n",
        "        else:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "\n",
        "    output = torch.matmul(scores, v)\n",
        "    return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, embedding_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.d_k = embedding_dim // heads\n",
        "        self.h = heads\n",
        "        \n",
        "        self.q_linear = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.v_linear = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.k_linear = nn.Linear(embedding_dim, embedding_dim)\n",
        "        \n",
        "        self.out = nn.Linear(embedding_dim, embedding_dim)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None, dec_mask=False):\n",
        "        \n",
        "        bs = q.size(0)\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        \n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "        \n",
        "        scores = attention(q, k, v, self.d_k, mask, dec_mask)\n",
        "\n",
        "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.embedding_dim)\n",
        "        output = self.out(concat)\n",
        "    \n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embedding_dim, d_ff=2048):\n",
        "        super().__init__() \n",
        "    \n",
        "        self.linear_1 = nn.Linear(embedding_dim, d_ff)\n",
        "        self.linear_2 = nn.Linear(d_ff, embedding_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear_1(x))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR1HDs1o-J0M"
      },
      "source": [
        "### Encoder-DecoderのLinear処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwPn5pMO93Zm"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim, heads):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(embedding_dim)\n",
        "        self.norm_2 = Norm(embedding_dim)\n",
        "        self.attn = MultiHeadAttention(heads, embedding_dim)\n",
        "        self.ff = FeedForward(embedding_dim)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.attn(x2,x2,x2,mask, dec_mask=False)\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.ff(x2)\n",
        "        return x\n",
        "    \n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim, heads):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(embedding_dim)\n",
        "        self.norm_2 = Norm(embedding_dim)\n",
        "        self.norm_3 = Norm(embedding_dim)\n",
        "        \n",
        "        self.attn_1 = MultiHeadAttention(heads, embedding_dim)\n",
        "        self.attn_2 = MultiHeadAttention(heads, embedding_dim)\n",
        "        self.ff = FeedForward(embedding_dim)\n",
        "\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.attn_1(x2, x2, x2, trg_mask, dec_mask=True)\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.attn_2(x2, e_outputs, e_outputs, src_mask, dec_mask=False)\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.ff(x2)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRzgV_uy-MNP"
      },
      "source": [
        "### Encoder-DecoderのEmbedding処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqF94jyr9t8Y"
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        # self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=word2id[\"<pad>\"]) なぜかpadding_indが機能しない． Why?\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "    def forward(self, x):\n",
        "        # 仕方なく，こちらで0埋めする\n",
        "        pad_index = x != word2id[\"<pad>\"]\n",
        "        pad_index = pad_index.view(*pad_index.shape, 1)\n",
        "        return self.embed(x)*pad_index + 0.0\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_seq_len = 200):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        pe = torch.zeros(max_seq_len, embedding_dim)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, embedding_dim, 2):\n",
        "                pe[pos, i] = \\\n",
        "                math.sin(pos / (10000 ** ((2 * i)/embedding_dim)))\n",
        "                pe[pos, i + 1] = \\\n",
        "                math.cos(pos / (10000 ** ((2 * (i + 1))/embedding_dim)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x * math.sqrt(self.embedding_dim)\n",
        "        seq_len = x.size(1)\n",
        "        pe = Variable(self.pe[:,:seq_len], requires_grad=False)\n",
        "        if x.is_cuda:\n",
        "            pe.cuda()\n",
        "        x = x + pe\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S5Iddtmqjgv"
      },
      "source": [
        "### Transformerモデル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF8zZpaR7fs4"
      },
      "source": [
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, embedding_dim)\n",
        "        self.pe = PositionalEncoder(embedding_dim)\n",
        "        self.layers = get_clones(EncoderLayer(embedding_dim, heads), N)\n",
        "        self.norm = Norm(embedding_dim)\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, embedding_dim)\n",
        "        self.pe = PositionalEncoder(embedding_dim)\n",
        "        self.layers = get_clones(DecoderLayer(embedding_dim, heads), N)\n",
        "        self.norm = Norm(embedding_dim)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, N, heads):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size, embedding_dim, N, heads)\n",
        "        self.decoder = Decoder(vocab_size, embedding_dim, N, heads)\n",
        "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output\n",
        "\n",
        "def get_model(embedding_dim, heads, n_layers, vocab_size):\n",
        "    \n",
        "    assert embedding_dim % heads == 0\n",
        "\n",
        "    model = Transformer(vocab_size, embedding_dim, n_layers, heads)\n",
        "       \n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p) \n",
        "    \n",
        "    return model\n",
        "\n",
        "model = get_model(embedding_dim, heads, n_layers, vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2id[\"<pad>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGsf9vhsqlIC"
      },
      "source": [
        "### 学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w3J7m235ezV"
      },
      "source": [
        "# GPUの確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)\n",
        "\n",
        "all_losses = []\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    epoch_loss = 0\n",
        "    for src, trg in train_loader:\n",
        "        model.zero_grad()\n",
        "\n",
        "        if use_cuda:\n",
        "            src = src.cuda()\n",
        "            trg = trg.cuda()\n",
        "\n",
        "        trg_input = trg[:, :-1]\n",
        "        src_mask, trg_mask = create_masks(batch_size, src, trg_input)\n",
        "        if use_cuda:\n",
        "          src_mask = src_mask.cuda()\n",
        "          trg_mask = trg_mask.cuda()\n",
        "        import sys\n",
        "        print(src.shape, src_mask.shape)\n",
        "        sys.exit()\n",
        "        preds = model(src, trg_input, src_mask, trg_mask)\n",
        "        loss = criterion(preds.view(-1, preds.size(-1)), trg[:, 1:].contiguous().view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    elapsed_time = time() - start\n",
        "    all_losses.append(epoch_loss)\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"epoch: {}, mean loss: {}, elapsed_time: {}\".format(epoch, loss.item(), elapsed_time))\n",
        "        \n",
        "model_name = \"seq2seq_calculator_v{}.pt\".format(epoch)\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "}, model_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRVagaexqmwj"
      },
      "source": [
        "### 評価\n",
        "今回は貪欲法で実装．本来のTransformerはビームサーチで行っているらしいが，割愛．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzFgv-nQqocW"
      },
      "source": [
        "batch_size = 1\n",
        "test_data = CalcDataset(data_num = 2000)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "model = get_model(embedding_dim, heads, n_layers, vocab_size).to(device)\n",
        "model_name = \"seq2seq_calculator_v{}.pt\".format(200)\n",
        "checkpoint = torch.load(model_name)\n",
        "model.load_state_dict(checkpoint[\"model\"])\n",
        "\n",
        "accuracy = 0\n",
        "        \n",
        "# 評価の実行   \n",
        "with torch.no_grad():\n",
        "    for src, trg in test_loader:\n",
        "        if use_cuda:\n",
        "            src = src.cuda()\n",
        "            trg = trg.cuda()\n",
        "\n",
        "        trg_input = trg[:, :-1].clone()\n",
        "        src_mask, trg_mask = create_masks(batch_size, src, trg_input)\n",
        "        if use_cuda:\n",
        "          src_mask = src_mask.cuda()\n",
        "          trg_mask = trg_mask.cuda()\n",
        "\n",
        "        # encoder\n",
        "        e_output = model.encoder(src, src_mask)\n",
        "\n",
        "        # decoder\n",
        "        right = []\n",
        "        for s in range(7):\n",
        "            outputs = trg_input[:, :s+1]\n",
        "            trg_mask_ = trg_mask[:, :s+1, :s+1]\n",
        "            out = model.out(model.decoder(outputs, e_output, src_mask, trg_mask_))\n",
        "            out = F.softmax(out, dim=2)\n",
        "\n",
        "            if s == 0:\n",
        "              index = torch.argmax(out.cpu().detach()).item()\n",
        "            else:\n",
        "              index = torch.argmax(out, dim=2)[0, -1].cpu().detach().item()\n",
        "            token = id2word[index]\n",
        "\n",
        "            if token == \"<eos>\":\n",
        "                break\n",
        "            right.append(token)\n",
        "\n",
        "            if use_cuda:\n",
        "              trg_input[:, s+1] = torch.LongTensor([word2id[token]]).cuda()\n",
        "            else:\n",
        "              trg_input[:, s+1] = torch.LongTensor([word2id[token]])\n",
        "        right = \"\".join(right)\n",
        "\n",
        "        if \"+\" in right or \"<pad>\" in right:\n",
        "          accuracy += 0\n",
        "          continue\n",
        "\n",
        "        x = list(src[0].to('cpu').detach().numpy() )\n",
        "        try:\n",
        "            padded_idx_x = x.index(word2id[\"<pad>\"])\n",
        "        except ValueError:\n",
        "            padded_idx_x = len(x)\n",
        "        left = \"\".join(map(lambda c: str(id2word[c]), x[:padded_idx_x]))\n",
        "        flag = [\"F\", \"T\"][eval(left) == int(right)]\n",
        "        print(\"{:>7s} = {:>4s} :{}\".format(left, right, flag))\n",
        "        if flag == \"T\":\n",
        "            accuracy += 1\n",
        "\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy / len(test_loader)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJeoXisaCnok"
      },
      "source": [
        "# 課題\n",
        "* 四則演算を変えてみよう"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxtXQWzDCuSf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}